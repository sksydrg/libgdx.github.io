<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    

<meta charset="utf-8">


<!-- begin _includes/seo.html --><title>Wiki Search - libGDX</title>

  <meta name="description" content="Ask, and it shall be given to you; seek, and you shall find!">




<meta property="og:type" content="website">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="libGDX">
<meta property="og:title" content="Wiki Search">
<meta property="og:url" content="https://libgdx.com/wiki/search">


  <meta property="og:description" content="Ask, and it shall be given to you; seek, and you shall find!">



  <meta property="og:image" content="https://libgdx.com/assets/images/logo.png">










<link rel="canonical" href="https://libgdx.com/wiki/search">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": null,
      "url": "https://libgdx.com/"
    
  }
</script>


  <meta name="google-site-verification" content="qc0iys9Aweas1z8EhICO-0Ohwh6Iq1GDV6K75e0ToO8" />






<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="libGDX Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>


<link rel="stylesheet" href="/assets/css/main.css">



  <meta name="color-scheme" content="light dark">
  <style>
    @import url("/assets/css/main2.css") (prefers-color-scheme: dark);
  </style>


<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>




    
<link rel="apple-touch-icon" sizes="180x180" href="/assets/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/icons/favicon-16x16.png">
<link rel="manifest" href="/assets/icons/site.webmanifest">
<link rel="mask-icon" href="/assets/icons/safari-pinned-tab.svg" color="#5bbad5">
<link rel="shortcut icon" href="/assets/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="msapplication-config" content="/assets/icons/browserconfig.xml">
<meta name="theme-color" content="#ffffff">

    <link rel="stylesheet" href="/assets/css/wiki.css">
  </head>

  <body class="layout--default_wiki">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    




<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
          <a class="site-logo" href="/wiki/">
            <picture>
              
                
                <source srcset="/assets/brand/logo_dark.svg" media="(prefers-color-scheme: dark)">
              
              <img src="/assets/brand/logo.svg" alt=" ">
            </source></picture>
          </a>
        
        <a class="site-title" href="/">
           
          
        </a>
        <ul class="visible-links">
<li class="masthead__menu-item">
              <a href="/">Back to Main Site</a>
            </li>
<li class="masthead__menu-item">
              <a href="/wiki/">Wiki Home</a>
            </li>
</ul>
        
        <a href="/wiki/search">
          <svg class="icon" width="16" height="16" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 15.99 16">
            <path d="M15.5,13.12L13.19,10.8a1.69,1.69,0,0,0-1.28-.55l-0.06-.06A6.5,6.5,0,0,0,5.77,0,6.5,6.5,0,0,0,2.46,11.59a6.47,6.47,0,0,0,7.74.26l0.05,0.05a1.65,1.65,0,0,0,.5,1.24l2.38,2.38A1.68,1.68,0,0,0,15.5,13.12ZM6.4,2A4.41,4.41,0,1,1,2,6.4,4.43,4.43,0,0,1,6.4,2Z" transform="translate(-.01)"></path>
          </svg>
        </a>
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      <div id="main" role="main">
  <header>
    <h1 id="page-title" class="page__title" itemprop="headline">Wiki Search
</h1>

    <div class="header-meta">
    </div>
  </header>
  <article class="page h-entry" itemscope="" itemtype="https://schema.org/CreativeWork">
    <div class="page__inner-wrap">
      <section class="page__content e-content markdown-body" itemprop="text">
          <aside class="sidebar__right">
            <div class="toc">
              <div class="wiki_toc__menu">
                <input id="wiki_ac-toc" name="accordion-toc" type="checkbox">
                <label for="wiki_ac-toc">Toggle Menu</label>

                
                <div class="wiki_nav__items">
                  <h1 id="table-of-contents">Table of Contents</h1>

<h2 id="getting-started">Getting Started</h2>
<ul>
  <li><a href="/wiki/start/setup">Setting Up a Dev Environment</a></li>
  <li><a href="/wiki/start/project-generation">Creating Your First libGDX Project</a></li>
  <li><a href="/wiki/start/import-and-running">Importing &amp; Running It</a></li>
  <li><a href="/wiki/start/a-simple-game">A Simple Game</a></li>
  <li><a href="/wiki/start/simple-game-extended">Extending the Simple Game</a></li>
  <li><a href="/wiki/start/demos-and-tutorials">Demos &amp; Tutorials</a></li>
</ul>

<h2 id="developers-guide">Developer’s Guide</h2>
<details><summary>The Application Framework</summary>
<div>
    <ul>
      <li><a href="/wiki/app/the-application-framework">The Application Framework</a></li>
      <li><a href="/wiki/app/the-life-cycle">The life-cycle</a></li>
      <li><a href="/wiki/app/modules-overview">Modules overview</a></li>
      <li><a href="/wiki/app/starter-classes-and-configuration">Starter classes and configuration</a></li>
      <li><a href="/wiki/app/querying">Querying</a></li>
      <li><a href="/wiki/app/logging">Logging</a></li>
      <li><a href="/wiki/app/threading">Threading</a></li>
      <li><a href="/wiki/app/interfacing-with-platform-specific-code">Interfacing with platform specific code</a></li>
    </ul>
  </div>
</details>

<details><summary>Audio</summary>
<div>
    <ul>
      <li><a href="/wiki/audio/audio">Audio</a></li>
      <li><a href="/wiki/audio/playing-pcm-audio">Playing PCM audio</a></li>
      <li><a href="/wiki/audio/recording-pcm-audio">Recording PCM audio</a></li>
      <li><a href="/wiki/audio/sound-effects">Sound effects</a></li>
      <li><a href="/wiki/audio/streaming-music">Streaming music</a></li>
    </ul>
  </div>
</details>

<details><summary>Deployment</summary>
<div>
    <ul>
      <li><a href="/wiki/deployment/deploying-your-application">Deploying Your Application</a></li>
      <li><a href="/wiki/deployment/bundling-a-jre">Bundling a JRE</a></li>
    </ul>
  </div>
</details>

<details><summary>Extensions</summary>
<div>
    <ul>
      <li>
<a href="/wiki/extensions/physics/physics">Physics</a>
        <ul>
          <li>
<a href="/wiki/extensions/physics/bullet/bullet-physics">Bullet Physics</a>
            <ul>
              <li><a href="/wiki/extensions/physics/bullet/bullet-wrapper-contact-callbacks">Bullet Wrapper Contact callbacks</a></li>
              <li><a href="/wiki/extensions/physics/bullet/bullet-wrapper-custom-classes">Bullet Wrapper Custom classes</a></li>
              <li><a href="/wiki/extensions/physics/bullet/bullet-wrapper-debugging">Bullet Wrapper Debugging</a></li>
              <li><a href="/wiki/extensions/physics/bullet/bullet-wrapper-setup">Bullet Wrapper Setup</a></li>
              <li><a href="/wiki/extensions/physics/bullet/bullet-wrapper-using-models">Bullet Wrapper Using models</a></li>
              <li><a href="/wiki/extensions/physics/bullet/bullet-wrapper-using-the-wrapper">Bullet Wrapper Using the wrapper</a></li>
            </ul>
          </li>
          <li><a href="/wiki/extensions/physics/box2d">Box2d</a></li>
        </ul>
      </li>
      <li><a href="/wiki/extensions/artificial-intelligence">Artificial Intelligence</a></li>
      <li><a href="/wiki/extensions/gdx-freetype">Gdx freetype</a></li>
      <li><a href="/wiki/extensions/gdx-pay">gdx pay</a></li>
      <li><a href="/wiki/extensions/third-party-extension-support">Third Party Extension Support</a></li>
    </ul>
  </div>
</details>

<p><a href="/wiki/file-handling">File handling</a></p>

<details><summary>Graphics</summary>
<div>
    <ul>
      <li><a href="/wiki/graphics/graphics">Graphics</a></li>
      <li>2D Graphics
        <ul>
          <li>Bitmap Fonts
            <ul>
              <li><a href="/wiki/graphics/2d/fonts/bitmap-fonts">Bitmap fonts</a></li>
              <li><a href="/wiki/graphics/2d/fonts/color-markup-language">Color Markup Language</a></li>
              <li><a href="/wiki/graphics/2d/fonts/distance-field-fonts">Distance field fonts</a></li>
            </ul>
          </li>
          <li>Scene2d
            <ul>
              <li><a href="/wiki/graphics/2d/scene2d/scene2d-ui">Scene2d.ui</a></li>
              <li><a href="/wiki/graphics/2d/scene2d/scene2d">Scene2d</a></li>
              <li><a href="/wiki/graphics/2d/scene2d/skin">Skin</a></li>
              <li><a href="/wiki/graphics/2d/scene2d/table">Table</a></li>
            </ul>
          </li>
          <li><a href="/wiki/graphics/2d/2d-animation">2D Animation</a></li>
          <li><a href="/wiki/graphics/2d/2d-particleeffects">2D ParticleEffects</a></li>
          <li><a href="/wiki/graphics/2d/clipping-with-the-use-of-scissorstack">Clipping, with the use of scissorstack</a></li>
          <li><a href="/wiki/graphics/2d/imgui">ImGui</a></li>
          <li><a href="/wiki/graphics/2d/masking">Masking</a></li>
          <li><a href="/wiki/graphics/2d/ninepatches">Ninepatches</a></li>
          <li><a href="/wiki/graphics/2d/orthographic-camera">Orthographic camera</a></li>
          <li><a href="/wiki/graphics/2d/packing-atlases-at-runtime">Packing atlases at runtime</a></li>
          <li><a href="/wiki/graphics/2d/packing-atlases-offline">Packing atlases offline</a></li>
          <li><a href="/wiki/graphics/2d/pixmaps">Pixmaps</a></li>
          <li><a href="/wiki/graphics/2d/spritebatch-textureregions-and-sprites">Spritebatch, Textureregions, and Sprites</a></li>
          <li><a href="/wiki/graphics/2d/texture-compression">Texture Compression</a></li>
          <li><a href="/wiki/graphics/2d/tile-maps">Tile maps</a></li>
          <li><a href="/wiki/graphics/2d/using-textureatlases">Using textureatlases</a></li>
        </ul>
      </li>
      <li>
<a href="/wiki/graphics/3d/3d-graphics">3D Graphics</a>
        <ul>
          <li><a href="/wiki/graphics/3d/3d-animations-and-skinning">3D animations and skinning</a></li>
          <li><a href="/wiki/graphics/3d/3d-particle-effects">3D Particle Effects</a></li>
          <li><a href="/wiki/graphics/3d/3d-picking">3D Picking</a></li>
          <li><a href="/wiki/graphics/3d/decals">Decals</a></li>
          <li><a href="/wiki/graphics/3d/importing-blender-models-in-libgdx">Importing Blender models in LibGDX</a></li>
          <li><a href="/wiki/graphics/3d/material-and-environment">Material and environment</a></li>
          <li><a href="/wiki/graphics/3d/modelbatch">ModelBatch</a></li>
          <li><a href="/wiki/graphics/3d/modelbuilder-meshbuilder-and-meshpartbuilder">ModelBuilder, MeshBuilder and MeshPartBuilder</a></li>
          <li><a href="/wiki/graphics/3d/modelcache">ModelCache</a></li>
          <li><a href="/wiki/graphics/3d/models">Models</a></li>
          <li><a href="/wiki/graphics/3d/quick-start">Quick start</a></li>
          <li><a href="/wiki/graphics/3d/virtual-reality">Virtual Reality (VR)</a></li>
        </ul>
      </li>
      <li>OpenGL Utility Classes
        <ul>
          <li><a href="/wiki/graphics/opengl-utils/frame-buffer-objects">Frame buffer objects</a></li>
          <li><a href="/wiki/graphics/opengl-utils/meshes">Meshes</a></li>
          <li><a href="/wiki/graphics/opengl-utils/rendering-shapes">Rendering shapes</a></li>
          <li><a href="/wiki/graphics/opengl-utils/shaders">Shaders</a></li>
        </ul>
      </li>
      <li><a href="/wiki/graphics/clearing-the-screen">Clearing the screen</a></li>
      <li><a href="/wiki/graphics/continuous-and-non-continuous-rendering">Continuous and Non Continuous Rendering</a></li>
      <li><a href="/wiki/graphics/integrating-libgdx-and-the-device-camera">Integrating libgdx and the device camera</a></li>
      <li><a href="/wiki/graphics/opengl-es-support">OpenGL (ES) Support</a></li>
      <li><a href="/wiki/graphics/profiling">Profiling</a></li>
      <li><a href="/wiki/graphics/querying-and-configuring-graphics">Querying and Configuring Graphics (monitors, display modes, vsync, display cutouts)</a></li>
      <li><a href="/wiki/graphics/taking-a-screenshot">Taking a Screenshot</a></li>
      <li><a href="/wiki/graphics/viewports">Viewports</a></li>
    </ul>
  </div>
</details>

<p><a href="/wiki/html5-backend-and-gwt-specifics">HTML5 Backend and GWT Specifics</a></p>

<details><summary>Input Handling</summary>
<div>
    <ul>
      <li><a href="/wiki/input/input-handling">Input Handling</a></li>
      <li><a href="/wiki/input/accelerometer">Accelerometer</a></li>
      <li><a href="/wiki/input/back-and-menu-key-catching">Back and menu key catching</a></li>
      <li><a href="/wiki/input/compass">Compass</a></li>
      <li><a href="/wiki/input/configuration-and-querying">Configuration and Querying</a></li>
      <li><a href="/wiki/input/controllers">Controllers</a></li>
      <li><a href="/wiki/input/cursor-visibility-and-catching">Cursor Visibility and Catching</a></li>
      <li><a href="/wiki/input/event-handling">Event handling</a></li>
      <li><a href="/wiki/input/gesture-detection">Gesture detection</a></li>
      <li><a href="/wiki/input/gyroscope">Gyroscope</a></li>
      <li><a href="/wiki/input/mouse-touch-and-keyboard">Mouse, Touch and Keyboard</a></li>
      <li><a href="/wiki/input/on-screen-keyboard">On screen keyboard</a></li>
      <li><a href="/wiki/input/polling">Polling</a></li>
      <li><a href="/wiki/input/simple-text-input">Simple text input</a></li>
      <li><a href="/wiki/input/vibrator">Vibrator</a></li>
    </ul>
  </div>
</details>

<p><a href="/wiki/internationalization-and-localization">Internationalization and Localization</a></p>

<details><summary>Using libGDX With Other JVM Languages</summary>
<div>
    <ul>
      <li><a href="/wiki/jvm-langs/using-libgdx-with-other-jvm-languages">Using libGDX With Other JVM Languages</a></li>
      <li><a href="/wiki/jvm-langs/using-libgdx-with-clojure">Using libgdx with Clojure</a></li>
      <li><a href="/wiki/jvm-langs/using-libgdx-with-kotlin">Using libGDX with Kotlin</a></li>
      <li><a href="/wiki/jvm-langs/using-libgdx-with-python">Using libgdx with Python</a></li>
      <li><a href="/wiki/jvm-langs/using-libgdx-with-scala">Using libgdx with Scala</a></li>
    </ul>
  </div>
</details>

<p><a href="/wiki/managing-your-assets">Managing your assets</a></p>

<details><summary>Math Utilities</summary>
<div>
    <ul>
      <li><a href="/wiki/math-utils/math-utilities">Math Utilities</a></li>
      <li><a href="/wiki/math-utils/circles-planes-rays-etc">Circles, planes, rays, etc.</a></li>
      <li><a href="/wiki/math-utils/interpolation">Interpolation</a></li>
      <li><a href="/wiki/math-utils/path-interface-and-splines">Path interface and Splines</a></li>
      <li><a href="/wiki/math-utils/vectors-matrices-quaternions">Vectors, matrices, quaternions</a></li>
    </ul>
  </div>
</details>

<p><a href="/wiki/networking">Networking</a></p>

<p><a href="/wiki/preferences">Preferences</a></p>

<details><summary>Third Party Services</summary>
<div>
    <ul>
      <li><a href="/wiki/third-party/admob-in-libgdx">Admob in libgdx</a></li>
      <li><a href="/wiki/third-party/firebase-in-libgdx">Firebase in libGDX</a></li>
      <li><a href="/wiki/third-party/google-mobile-ads-in-libgdx">Google Mobile Ads in Libgdx (replaces deprecated AdMob)</a></li>
      <li><a href="/wiki/third-party/google-play-games-services-in-libgdx">Google Play Games Services in LibGDX</a></li>
      <li><a href="/wiki/third-party/pollfish-in-libgdx">Pollfish in libgdx</a></li>
      <li><a href="/wiki/third-party/proguard-dexguard-and-libgdx">ProGuard DexGuard and libGDX</a></li>
      <li><a href="/wiki/third-party/smaato-in-libgdx">Smaato in libGDX</a></li>
    </ul>
  </div>
</details>

<details><summary>Tools</summary>
<div>
    <ul>
      <li><a href="/wiki/tools/2d-particle-editor">2D Particle Editor</a></li>
      <li><a href="/wiki/graphics/3d/3d-particle-effects">Flame</a></li>
      <li><a href="/wiki/tools/hiero">Hiero</a></li>
      <li><a href="/wiki/tools/overlap2d">Overlap2D</a></li>
      <li><a href="/wiki/tools/skin-composer">Skin Composer</a></li>
      <li><a href="/wiki/tools/texture-packer">Texture packer</a></li>
    </ul>
  </div>
</details>

<details><summary>Utilities</summary>
<div>
    <ul>
      <li><a href="/wiki/utils/collections">Collections</a></li>
      <li><a href="/wiki/utils/jnigen">jnigen</a></li>
      <li><a href="/wiki/utils/reading-and-writing-json">Reading and writing JSON</a></li>
      <li><a href="/wiki/utils/reading-and-writing-xml">Reading and writing XML</a></li>
      <li><a href="/wiki/utils/reflection">Reflection</a></li>
      <li><a href="/wiki/utils/saved-game-serialization">Saved game serialization</a></li>
    </ul>
  </div>
</details>

<h2 id="articles">Articles</h2>
<ul>
  <li><a href="/wiki/articles/getting-help">Getting Help</a></li>
  <li><a href="/wiki/articles/external-tutorials">External Tutorials</a></li>
  <li><a href="/wiki/articles/coordinate-systems">Coordinate Systems</a></li>
  <li><a href="/wiki/articles/memory-management">Memory Management</a></li>
  <li><a href="/wiki/articles/updating-libgdx">Updating Your libGDX Version</a></li>
  <li><a href="/wiki/articles/dependency-management-with-gradle">Dependency Management with Gradle: Adding Extensions and Third-Party Libraries</a></li>
  <li><a href="/wiki/articles/improving-workflow-with-gradle">Improving Your Gradle Workflow</a></li>
  <li><a href="/wiki/articles/maven-integration">Maven Integration</a></li>
  <li><a href="/wiki/articles/creating-a-separate-assets-project-in-eclipse">Creating Asset Project in Eclipse</a></li>
  <li><a href="/wiki/articles/java-development-kit-selection">Java Development Kit - Selection</a></li>
  <li><a href="/wiki/articles/console-support">Console Support?</a></li>
</ul>

                </div>
              </div>
            </div>
          </aside>

          <input type="text" id="search" class="search-input" autofocus="" placeholder="Enter your search term..." style="border: 1px solid #d4d8e3;border-radius: 4px;box-shadow: 0 1px 1px 0 rgba(85,95,110,.2); padding: 14px; font-size: 16px;">
          <div id="results" class="results">
          </div>

      </section>
    </div>

  </article>
</div>

<script src="https://code.jquery.com/jquery-3.6.0.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

<script src="/assets/js/lunr/lunr.min.js"></script>

<pre>
<script>
// STORE
var store = [{
        "title": "2D Animation",
        "excerpt":
        
        "2D Animation is a technique used to create the illusion of movement using static images. This article describes how to create animations with libGDX using its Animation Class.   Background   An animation consists of multiple frames which are shown in a sequence at set intervals. An animation of a running man can be achieved by taking pictures of him while running and playing those images back sequentially in a loop.   The following “sprite sheet” image shows a complete cycle of a man running. Each box contains a frame of animation. When these frames are shown sequentially over a period of time, they appear as an animated image.      The frame rate is how often the frame is changed per second. The example sprite sheet’s complete running cycle has 30 frames (6 columns and 5 rows). If the character is to complete a cycle in one second, 30 frames must be shown per second, so the frame rate is 30 FPS. The time per frame (known as the frame time or interval time) is the reciprocal of the FPS, in this case 0.033 seconds per frame.   An animation is a very simple state machine. The running man has 30 states as per the sprite sheet. The numbered frames represent the states a running man goes through, only one at a time. The current state is determined by the amount of time since the animation began. If less than 0.033 seconds have elapsed, we are in State 1, so the first sprite is drawn. If we are between 0.033 and 0.067 seconds, then we are in State 2, and so on. If the animation is looping, it returns to the first frame after all frames have been shown.      The Animation class   libGDX’s Animation (code) class can be used to easily manage an animation. It is constructed with a list of images and the frame interval time. During playback, its getKeyFrame method takes an elapsed time parameter and returns the appropriate image for that time.   Animation has a generic type parameter for the type of class that represents the image. The type would typically be a TextureRegion or PolygonRegion, but any renderable object can be used. The type is declared by specifying the animation type in the Animation declaration, for example Animation&lt;TextureRegion&gt; myAnimation = new Animation&lt;TextureRegion&gt;(/*...*/). Note that it would usually be inadvisable to use the Sprite class to represent frames of an animation, because the Sprite class contains positional data that would not carry from frame to frame.   TextureAtlas example   libGDX’s TextureAtlas (code) class is typically used for combining many separate TextureRegions into a smaller set of Textures to reduce expensive draw calls. (details here).   TexturePacker and TextureAtlas provide a convenient way to generate animations. All the source images of an animation should be named with an underscore and frame number at the end, such as running_0.png, running_1.png, running_2.png, etc. TexturePacker will automatically use these numbers as frame numbers (so long as the packing parameter useIndexes is left true).   After the TextureAtlas is loaded, a complete array of frames can be acquired at once and passed into the Animation constructor:   public Animation&lt;TextureRegion&gt; runningAnimation;  //...  runningAnimation =     new Animation&lt;TextureRegion&gt;(0.033f, atlas.findRegions(\"running\"), PlayMode.LOOP);   Sprite sheet example   The following code snippet will create an Animation using the animation_sheet.png sprite-sheet and renders the animation to the screen.   public class Animator implements ApplicationListener {  \t// Constant rows and columns of the sprite sheet \tprivate static final int FRAME_COLS = 6, FRAME_ROWS = 5;  \t// Objects used \tAnimation&lt;TextureRegion&gt; walkAnimation; // Must declare frame type (TextureRegion) \tTexture walkSheet; \tSpriteBatch spriteBatch;  \t// A variable for tracking elapsed time for the animation \tfloat stateTime;  \t@Override \tpublic void create() {  \t\t// Load the sprite sheet as a Texture \t\twalkSheet = new Texture(Gdx.files.internal(\"animation_sheet.png\"));  \t\t// Use the split utility method to create a 2D array of TextureRegions. This is \t\t// possible because this sprite sheet contains frames of equal size and they are \t\t// all aligned. \t\tTextureRegion[][] tmp = TextureRegion.split(walkSheet, \t\t\t\twalkSheet.getWidth() / FRAME_COLS, \t\t\t\twalkSheet.getHeight() / FRAME_ROWS);  \t\t// Place the regions into a 1D array in the correct order, starting from the top \t\t// left, going across first. The Animation constructor requires a 1D array. \t\tTextureRegion[] walkFrames = new TextureRegion[FRAME_COLS * FRAME_ROWS]; \t\tint index = 0; \t\tfor (int i = 0; i &lt; FRAME_ROWS; i++) { \t\t\tfor (int j = 0; j &lt; FRAME_COLS; j++) { \t\t\t\twalkFrames[index++] = tmp[i][j]; \t\t\t} \t\t}  \t\t// Initialize the Animation with the frame interval and array of frames \t\twalkAnimation = new Animation&lt;TextureRegion&gt;(0.025f, walkFrames);  \t\t// Instantiate a SpriteBatch for drawing and reset the elapsed animation \t\t// time to 0 \t\tspriteBatch = new SpriteBatch(); \t\tstateTime = 0f; \t}  \t@Override \tpublic void render() { \t\tGdx.gl.glClear(GL20.GL_COLOR_BUFFER_BIT); // Clear screen \t\tstateTime += Gdx.graphics.getDeltaTime(); // Accumulate elapsed animation time  \t\t// Get current frame of animation for the current stateTime \t\tTextureRegion currentFrame = walkAnimation.getKeyFrame(stateTime, true); \t\tspriteBatch.begin(); \t\tspriteBatch.draw(currentFrame, 50, 50); // Draw current frame at (50, 50) \t\tspriteBatch.end(); \t}  \t@Override \tpublic void dispose() { // SpriteBatches and Textures must always be disposed \t\tspriteBatch.dispose(); \t\twalkSheet.dispose(); \t} }      Creating an animation is extremely simple by using the following constructor.                  Method signature       Description                       Animation (float frameDuration, TextureRegion... keyFrames)       The first parameter is the frame time and the second is an array of regions (frames) making up the animation           Best practices     Pack frames into one texture along with other sprites to optimize rendering. This is easily done with TexturePacker.   Settle for a reasonable number of frames depending on the game type. For a retro arcade style, 10 fps may suffice, while more realistic looking movements require more frames.   Assets   Get the sprite sheet here.  ",
        
        "url": "/wiki/graphics/2d/2d-animation" },{
        "title": "2D Particle Editor",
        "excerpt":
        
        "The libGDX 2D Particle Editor is a powerful tool for making particle effects. See the video and documentation below. The Java API works (the editor is built using it) but could use some clean up and definitely some documentation. There is a runnable example, though unfortunately it isn’t the simplest for typical usage. Improvements to particles is planned, but will be some time until we can get to it.      Running the 2D Particle Editor   To run the editor, you can check out libGDX and run it from source. The editor is in the gdx-tools project. Alternatively, download the editor here.   Thirdly, if you are using Gradle and you added “Tools” extension to your project, you can easily run the particle editor from your IDE. For example, in IntelliJ: Open the Navigate menu (Cmd-N on OSX) and type ParticleEditor, and IntelliJ should find the ParticleEditor.java file. It has a .main() method that is used to launch the file. Right-click and select “Run ParticleEditor.main()” and IntelliJ will open the run configuration dialog box. In the “Use classpath of module:” dropdown, select your desktop project, and then click Run. This will create a run configuration that you can use later to launch the particle editor easily.   Using the Particle Editor   Video:      Also see this tutorial on JGO.   Briefly, a particle effect consists of some images that are moved around. The images usually use additive blending and some pretty stunning results can be produced with only a few images. Particle effects are good for fire, explosions, smoke, etc. Each particle has many properties that control how it behaves: life, velocity, rotation, scale, etc. The particle editor allows you to manipulate these properties and observe the result in real time. You can also create effects programmatically, but it is much more difficult and time consuming to create great effects.   The first step to creating an effect is to choose an image. The default image is just a simple round gradient. Experiment with different images to create a wide variety of effects. Images will often combine for some surprising and sometimes very cool looking results.   When you are configuring properties, you are actually configuring the particle emitter that will create and manage the particles. In code, the emitter is represented by the ParticleEmitter class. A particle effect is made up of one or more emitters, which is managed in the lower left of the particle editor. In code, the effect is represented by the ParticleEffect class, which has a list of ParticleEmitters.   Properties Panel Elements   There are some common elements that appear in several of the properties panels.   Active button   Properties with an “Active” toggle button can be turned off, which can minimize some of the work that needs to be done at run-time.   Number / number range   Some of the number fields have a &gt; button beside them. Clicking this button changes the number into a number range, where at runtime, a random value is selected from between the two specified values, every time the number is referenced. For example, if a range of 1-2 is selected for the Life property, each new particle will have some random length life between 1 and 2 seconds.   Chart   A chart is used to control the value of a property over time. The word “Duration” or “Life” in the middle of the chart indicates whether the horizontal timeline of the chart is relative to the duration of the emitter, or the lifetime of each single particle.   The “High” and “Low” number fields indicate the values that correspond with the top and bottom of the chart. Like other number fields, they can be expanded into a range with the &gt; button. The random number in the range is chosen when the effect starts for a “Duration” chart, and when a particle is spawned for a “Life” chart.   Within the chart itself:     To add nodes, click anywhere in the chart.   To move nodes, click and drag an existing node. You can hold CTRL while dragging to drag all nodes at once. You can hold CTRL-SHIFT to drag all nodes proportionally, relative to the bottom, which is especially useful for the Transparency parameter.   To delete a node, double-click it.   The + button expands the chart for fine-tuning.   The “Relative” checkbox. When unchecked, the value at any one point in time for the property will be what the chart shows. When checked, the value shown on the chart is added to the initial value of the property. Why? Imagine you have rotation set to start at 0 and go to 360 degrees over the life of a particle. This is nice, but all the particles start at the same zero rotation, so you change the “Low” value to start between 0 and 360. Now your particles will start between 0 and 360, and rotate to exactly 360 degrees. If a particle spawns at 330 degrees, it will only rotate 30 degrees. Now, if you check “Relative”, a particle that spawns at 330 degrees will rotate to 330 + 360 degrees, which is probably what you want in this case.   Finally, the “Independent” checkbox. Some properties allow switching how the chart controls the property over time from emitter to single/independent particle. Let’s imagine that we have a chart with ranged values defined on the “Life” property. By default the chart affects the emitter as a whole. Each time the emitter generates particles, a random value within the appropriate range for that time will be chosen and all emited particles will have that value set as life. On the other hand, if “Independent” is checked, a new random value will be calculated per emitted particle and set as life to each of them independently.   Properties   Delay: When an effect starts, this emitter will do nothing for this many milliseconds. This can be used to synchronize multiple emitters.   Duration: How long the emitter will emit particles. Note this is not the same as how long particles will live.   Count: Controls the minimum number of particles that must always exist, and the maximum number of particles that can possibly exist. The minimum is nice for making sure particles are always visible, and the maximum lets the emitter know how much memory to allocate.   Emission: How many particles will be emitted per second.   Life: How long a single particle will live.   Life Offset: How much life is used up when a particle spawns. The particle is still moved/rotated/etc for the portion of its life that is used up. This allows particles to spawn, eg, halfway through their life.   X Offset and Y Offset: The amount in pixels to offset where particles spawn.   Spawn: The shape used to spawn particles: point, line, square, or ellipse. Ellipse has additional settings.   Spawn Width and Spawn Height: Controls the size of the spawn shape.   Size: The size of the particle.   Velocity: The speed of the particle.   Angle: The direction the particle travels. Not very useful if velocity is not active.   Rotation: The rotation of the particle.   Wind and Gravity: The x-axis or y-axis force to apply to particles, in pixels per second.   Tint: The particle color. Click the little triangle and then use the sliders to change the color. Click in the bar above the triangle to add more triangles. This allows you to make particles change to any number of colors over their lifetime. Click and drag to move a triangle (if it isn’t at the start or end). Double-click to delete.   Transparency: Controls the alpha of the particle. This chart is different than the others because you cannot modify its vertical range. It is always from 0 to 1.   Options     Additive: For additive blending.   Pre-multiplied alpha: For pre-multiplied alpha blending, which enables a mixture of alpha and additive blending. If this is selected, the Additive option is ignored.   Attached: Means existing particles will move when the emitter moves.   Continuous: Means the emitter restarts as soon as its duration expires. Note that this means an effect will never end, so other emitters in the effect that are not continuous will never restart.   Aligned: The angle of a particle is added to the rotation. This allows you to align the particle image to the direction of travel.   In the upper left of the particle editor, “Count” shows how many particles exist for the currently selected emitter. “Max” shows how many particles exist for all emitters over the past few seconds. Below that is a percentage that represents the duration percent of the currently selected emitter.   Effect settings saved with the particle editor are written to a text file, which can be loaded into a ParticleEffect instance in your game. The ParticleEffect can load images from a directory, or a SpriteSheet. Of course, a SpriteSheet is recommended and can easily be made with the SpriteSheetPacker.   Most effects can be simplified to use just a few images. My most complex effects that use 4 or more emitters typically only need 15 or so total particles alive at once. See ParticleEmitterTest in gdx-tests if you’d like to test how many particles your device can handle. However, the performance varies greatly with the particle image size.  ",
        
        "url": "/wiki/tools/2d-particle-editor" },{
        "title": "2D ParticleEffects",
        "excerpt":
        
        "   Basic ParticleEffect usage   Efficiently using ParticleEffects   Examples            Pooled Effect example       Batched Effect example           Video example   Basic ParticleEffect usage  Using Particle effects is easy, load up your particle that has been generated in the ParticleEditor  TextureAtlas particleAtlas; //&lt;-load some atlas with your particle assets in ParticleEffect effect = new ParticleEffect(); effect.load(Gdx.files.internal(\"myparticle.p\"), particleAtlas); effect.start();  //Setting the position of the ParticleEffect effect.setPosition(x, y);  //Updating and Drawing the particle effect //Delta being the time to progress the particle effect by, usually you pass in Gdx.graphics.getDeltaTime(); effect.draw(batch, delta);    Efficiency  Rendering particles is great, rendering lots of particles is even better, here is how you do it without melting your users devices.   ParticleEffects are no different than Sprites, in fact they ARE sprites. Take everything you know already about efficiently rendering sprites and carry them across.      Use an atlas!   If your particle effect sprite shares a texture with all of your other gameplay assets, or at least the ones that are being batched together, you wont have to switch  textures, which causes the Batch to flush. You don’t want the batch to flush too often as its an expensive operation, and you won’t get the most out of your Batch.      Pool your effects   Creating new ParticleEffects willy nilly? Great, now stop doing that and use a Pool! Unfortunately garbage collection degrades the performance of your game, especially on the mobile platforms, so you want to avoid garbage at all costs. Use of the ParticleEffectPool completely mitigates garbage generation as you will be reusing your ParticleEffect when you are finished with them. No more wasted memory! No more garbage collection   In simple terms, grab a new object from the Pool, use it, when you are finished, return it so you can use it again.   See the example below for how to implement pooling.           Batch your effects Draw all your ParticleEffects that have the same Textures/blend modes together. We don’t want to interrupt the Batch, which means no texture swapping, no blend state changing. This gets the most out of our Batch, and keeps our device happy.       If you have ParticleEffects that have different blending, for example some may have Additive, and others dont, group them together when you draw so you only swap the blend mode once.       This also applies to ParticleEffects that may have a Sprite that belongs to a different Texture than another effect. Draw all your instances that use the same Texture first, then the rest. If you interleave these ParticleEffects with different Textures, you will be causing lots of draw calls. See the example below for how to implement batching.            Clean up the blend modes yourself Particles that have Additive Blending will change the state of the Batch. By default the ParticleEffect returns the Batch’s state to the original state it was in, so the following draws are not effected by the ParticleEffect’s blend mode. This is great, apart from if you want to draw multiple ParticleEffect instances, as this will change the blend mode after EACH instance is drawn, resulting in a draw call.       //original blendstate additiveParticleEffect.draw(batch, delta); //set the blend state to additive, (flushes the batch) //additiveParticleEffect will then reset the batch to the original blend state (flushes the batch) //repeat //original blendstate additiveParticleEffect.draw(batch, delta); //set the blend state to additive, (flushes the batch) //additiveParticleEffect will then reset the batch to the original blend state (flushes the batch) //repeat           To avoid this, we can use ParticleEffect’s setEmittersCleanUpBlendFunction function, and set it to false. This will prevent the ParticleEffect from resetting the blend mode to the original, and avoiding the extra flush of the Batch.       //original blendstate additiveParticleEffect.draw(batch, delta); //set the blend state to additive, (flushes the batch) //blendstate is now additive //repeat //additive blendstate additiveParticleEffect.draw(batch, delta); //already additive, no state change (no flush) //repeat           This is great, efficiently drawing lots of ParticleEffects with the same blend state! Be careful with this function, you are now in charge of returning the Batch’s blend state to the original, so you must do this after you are finished drawing all your particles.       Examples   For a collection of community created particle effects for LibGDX see the Particle Park  A coding example is available on LibGDX.info   Pooled effect example:  ParticleEffectPool bombEffectPool; Array&lt;PooledEffect&gt; effects = new Array();  //...  //Set up the particle effect that will act as the pool's template ParticleEffect bombEffect = new ParticleEffect(); bombEffect.load(Gdx.files.internal(\"particles/bomb.p\"), atlas);  //If your particle effect includes additive or pre-multiplied particle emitters //you can turn off blend function clean-up to save a lot of draw calls, but //remember to switch the Batch back to GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA //before drawing \"regular\" sprites or your Stage. bombEffect.setEmittersCleanUpBlendFunction(false);  bombEffectPool = new ParticleEffectPool(bombEffect, 1, 2);  // Create effect: PooledEffect effect = bombEffectPool.obtain(); effect.setPosition(x, y); effects.add(effect);   // Update and draw effects: for (int i = effects.size - 1; i &gt;= 0; i--) { \tPooledEffect effect = effects.get(i); \teffect.draw(batch, delta); \tif (effect.isComplete()) { \t\teffect.free(); \t\teffects.removeIndex(i); \t} }  //...  // Reset all effects: for (int i = effects.size - 1; i &gt;= 0; i--)     effects.get(i).free(); //free all the effects back to the pool effects.clear(); //clear the current effects array   Batched effect example:  Array&lt;PooledEffect&gt; additiveEffects; Array&lt;PooledEffect&gt; normalEffects;  ParticleEffect additiveEffect = new ParticleEffect(); additiveEffect.load(//...); additiveEffect.setEmittersCleanUpBlendFunction(false); //Stop the additive effect restting the blend state  batch.begin(); //draw all additive blended effects for (PooledEffect additiveEffect : additiveEffects) {   additiveEffect.draw(batch, delta); }  //We need to reset the batch to the original blend state as we have setEmittersCleanUpBlendFunction as false in additiveEffect batch.setBlendFunction(GL20.GL_SRC_ALPHA, GL20.GL_ONE_MINUS_SRC_ALPHA); //draw all 'normal alpha' blended effects for (PooledEffect normalEffect : normalEffects) {   normalEffect.draw(batch, delta); }  batch.end();   video-example      Particle Effect Example on LibGDX.info   source of the video   source of the video using pooling     ",
        
        "url": "/wiki/graphics/2d/2d-particleeffects" },{
        "title": "3D animations and skinning",
        "excerpt":
        
        "A Model (and ModelInstance) can contain one or more animations. An animation is a sequence (keyframes) of transformations which are applied to one or more nodes of the model. Each animation is identified by it’s name (id), which must be unique within the model. E.g. you might have character with an animation called “walk” and “attack”. This page describes how to load and use animations within your application.   Skinning can be used to deform the model (mesh) based on the transformation of one or more nodes. This is commonly used conjunction with animations. The page also describes how to use skinning within your application.   Loading animations  When using fbx-conv to convert your model from FBX to G3DB/G3DJ, animations are automatically converted along with it. Just like FBX, G3DB/G3DJ files can contain multiple animations in a single file along with the other model data. Animations applied to nodes which are not present in the source FBX file, will not be converted. So make sure to select all the nodes and animations when exporting to FBX.   You can check the converted animations by converting to the G3DJ file format and opening the resulting file in your favorite text editor. The animations are located at the bottom of the file. Be aware that animations (keyframes) can bloat the file, so it’s advised to use the smaller G3DB file format on production.   libGDX does not support custom interpolation between keyframes. If you use an interpolation other than linear, fbx-conv will generate additional keyframes to compensate this. This compensation is done according to the target FPS. While the target FPS is customizable in FBX, not all modeling applications allow you to change this value (the default is 30 FPS). To avoid bloating the file with additional keyframes, make sure to use linear interpolation whenever possible.   When you load the G3DB/G3DJ file in your application using G3dModelLoader or AssetManager, the animations are also loaded. When you create a ModelInstance from the Model, the Animations are also copied to the ModelInstance. If you specify specific nodes while creating the ModelInstance, only the animations that are applied to those nodes (and child nodes) are copied to the new ModelInstance.   Using animations  Applying a single animation  To apply an animation to a ModelInstance, you can create an AnimationController. An AnimationController is dedicated to a single ModelInstance and the ModelInstance must outlive the AnimationController. The AnimationController can be used to apply a single animation to the ModelInstance at a time, although it support blending (=transitioning) between multiple animations. If you want to apply multiple animations to the same ModelInstance, you can use multiple AnimationControllers, as long as they don’t interfere with each other (don’t affect the same nodes).   When an animation is set (described below), the AnimationController needs to be updated every frame. This can be done using the AnimationController.update(float delta) method. The delta argument is used to inform the AnimationController how much time is past since the last call to update. Most commonly Gdx.graphics.getDeltaTime() is used for that parameter.   To set the current animation, use the setAnimation(String id) method. This will cause the animation to be played immediately (removing the current animation, if any). The id argument must be the name (case sensitive) of an existing animation within the ModelInstance.  ModelInstance instance; AnimationController controller; public void create() {     ...     instance = new ModelInstance(model);     controller = new AnimationController(instance);     controller.setAnimation(\"walk\"); } public void render() {     ...     controller.update(Gdx.graphics.getDeltaTime());     ... }   The setAnimation(String id) method will play the animation once at normal speed and then stop (freezing the animation at the last frame of the animation). To remove the animation (so the nodes are at their original transformation), call setAnimation(null).   Looping  If you want to execute an animation more than once, you can use the setAnimation(String id, int loopCount) method. The loopCount argument can be used to specify how many times the animation should be played (which must not be zero). To continuous loop the animation use the value -1.   AnimationDesc  Every animation method (like setAnimation(...) returns an AnimationDesc instance, which is valid while the animation is being played (or queued to be played, see below). The AnimationDesc contains all information about the animation, like the current animation time and the number of loops remaining for that animation. You can alter those values to control the animation during execution. When the animation is ended, you must remove all references to the AnimationDesc, as it might be reused (pooled) for a next animation.   AnimationListener  To get notified when an animation is looped or ended you can implement the AnimationListener interface. It will be notified when an animation is ended or looped. It receives the AnimationDesc of the animation that ended or looped. You can supply your AnimationListener with every animation method, e.g. setAnimation(String id, int loopCount, AnimationListener listener).   Blending animations  When switching animation using the setAnimation, the previous animation is stopped abruptly. It is possible to blend the previous animation into the new animation, giving a smooth transition between animations. You can use the animate(...) for that. The animate method takes the same parameters as the setAnimation method along with an additional transitionTime argument. During this time the new animation will be linear interpolated on top of the old animation, resulting in a smooth transition. If there is no previous animation, then the new animation will start immediately.   Queuing animations  To start an animation when the current animation is completed, you can use the queue(...) method. If the current animation is continuously looping, the queued animation will be executed when the current loop completed. Otherwise the queued animation will be executed when the current animation is ended (including all loops remaining). If there is no current animation, the queued animation will be executed immediately. Queued animations will be blended if a transition time greater than zero is specified.   Action animation  An action animation is a (commonly short) animation that is executed on top of the current the animation. In practice, it is the same calling animate(...) with the action animation and calling queue(...) with the previous animation.   Animation speed  Every animate method (like setAnimation, animate, queue and action) allows you to set the speed of that particular animation. The speed is defined as “seconds per second”. So a value of 1 will play the animation at the normal speed. A value higher than one will play the animation faster (e.g. a value of 2 will execute the animation at twice the original speed). A value lower than one will play the animation slower (e.g. a value of 0.5 will execute the animation at half the original speed). The speed can be negative, meaning that the animation will be executed in reverse.   While you can control the speed of an individual animation, you can also control the overall speed of every animation by changing the delta value of the update(float delta) method AnimationController. E.g. if you call controller.update(0.5f * Gdx.graphics.getDeltaTime());, the overall speed will be half the original speed. The delta value can also be negative, meaning all animations are played in reverse.   Node transformations  While an animation is executed, all the affected nodes will have the isAnimated member set to true. This will cause the Node’s translation, rotation and scale values not be used. So, while animating, you cannot transform the affected nodes yourself.   Skinning  Skinning is used to transform the model according to the transformation of one or more nodes (also called bones or joints). Most commonly when using skinning, invisible nodes (nodes with no NodeParts attached to them) are used to deform the model. These nodes (which commonly form a hierarchy) are also called a skeleton or armature.   So in practice, when using skinning, your model has an invisible skeleton part (one or more nodes) and a visible part (one or more nodes). While the visible nodes themselves stay untransformed, the skeleton is transformed (e.g. by animation) and therefor affect the position of the vertices of the visible mesh.   This is accomplished by using blend weights (also called bone weights) which are vertex attributes of the visible nodes. Each blend weight has an index (of the specific bone/node) and a weight (how much it is influenced by that bone/node). Each visible node (NodePart) has a reference to it’s bones (nodes of the skeleton).   Loading skinning  libGDX only supports shader skinning, which requires at least Open GL ES 2.0. If you created your skinned model using a modeling application, exported it to FBX and converted it to G3DB/G3DJ, then skinning should seamlessly work. Just keep in mind that you need to export both the visible and invisible (skeleton) nodes along with the animations themselves.   Skinning can require some tweaking to get the best and optimal result. By default fbx-conv will include 4 bone weights per vertex (each vertex can be influenced by at most four nodes). Also by default, fbx-conv will group vertices that share the same bones and split the mesh into multiple parts when the total number of bones influencing the vertices is more than 12.   Splitting the mesh is done because the shader is limited in the amount of variables it can use. However, this will result in multiple render calls which might impact performance. For more information about this, have a look at this post.   You can change the default values of fbx-conv by using the command line option -w and -b. The -w command allows you to specify the amount of bones that can influence one vertex. Which must be in the range between 1 and 8 (the default shader does not support more than 8 bone weights). You typically should try to keep this as low as possible.  fbx-conv -w 3 inputfile.fbx   The -b command allows you to specify the total amount of bones allowed before fbx-conv starts splitting the mesh. This value must be more than zero and has no hard limit. You typically should not set it below three times the number specified with the -w command. You can increase the value to avoid splitting the mesh, but at the cost of shader uniforms.  fbx-conv -b 16 inputfile.fbx   If you do use the -b command line option to change the maximum number of bones, you should also change the shader to use that same amount. This can be done using the Config#numBones member when creating the ShaderProvider for ModelBatch. For example:  DefaultShader.Config config = new DefaultShader.Config(); config.numBones = 16; modelBatch = new ModelBatch(new DefaultShaderProvider(config));  ",
        
        "url": "/wiki/graphics/3d/3d-animations-and-skinning" },{
        "title": "3D Graphics",
        "excerpt":
        
        "The 3D Graphics API     Quick Start   Models   Material and Environment   ModelBatch   ModelCache   ModelBuilder, MeshBuilder and MeshPartBuilder   3D Animations and Skinning   Importing Blender Models in libGDX   3D Particle Effects   Virtual Reality (VR)  ",
        
        "url": "/wiki/graphics/3d/3d-graphics" },{
        "title": "3D Particle Effects",
        "excerpt":
        
        "Because of issues with perspective and depth, the 2D particle effects are not suitable for 3D applications. Additionally, the 3d particle effects take full advantage of movement through 3d space, allowing a wide variety of dynamic graphical effects.      Flame - 3D Particle Editor  Much like their 2D cousins, 3D particle effects can be edited with a GUI editor included in libgdx. The editor is called Flame, and can be downloaded here.   Particle Effect Types  There are 3 different kinds of 3D particle effects:     Billboards   PointSprites   ModelInstance   Billboards are sprites that always face the camera (the Decal class in libGDX is essentially a billboard).   PointSprites draw a sprite to a single 3d point. They are simpler than billboards, but more efficient. More information about point sprites in OpenGL: https://www.informit.com/articles/article.aspx?p=770639&amp;seqNum=7   ModelInstances are familiar to you if you have done any 3D work in libgdx. They are instances of 3D models. Not surprisingly, this is the most taxing type of particle effect in terms of performance.   Due to those differences, each particle effect type has its own dedicated batch renderer: BillboardParticleBatch, PointSpriteParticleBatch, ModelInstanceParticleBatch.     Using 3D Particle Effects  The easiest way to use 3D particle effects is by taking advantage of the ParticleSystem class, abstracting away various details and managing them for you. First we will create the batch of the type(s) we wish to use, then create the ParticleSystem. In this case, we are going to use PointSprites.   For a more in depth look at how to use 3d particles programmatically, take a look at the test class.   IMPORTANT: When you import the ParticleEffect class into your IDE, make sure you do not accidentally import the 2D effect ParticleEffect class. They share the same name, but have different import paths. You are looking for: com.badlogic.gdx.graphics.g3d.particles.ParticleEffect   Step 1: Create Batches and ParticleSystem  ParticleSystem particleSystem = new ParticleSystem(); // Since our particle effects are PointSprites, we create a PointSpriteParticleBatch PointSpriteParticleBatch pointSpriteBatch = new PointSpriteParticleBatch(); pointSpriteBatch.setCamera(cam); particleSystem.add(pointSpriteBatch);   Step 2: Load Effects Using AssetManager  Now we need to load our particle effects that we have created using the Flame GUI editor. First, create a ParticleEffectLoadParameter to pass to the asset manager when loading. Then the assets may be loaded.  AssetManager assets = new AssetManager(); ParticleEffectLoader.ParticleEffectLoadParameter loadParam = new ParticleEffectLoader.ParticleEffectLoadParameter(particleSystem.getBatches()); assets.load(\"particle/effect.pfx\", ParticleEffect.class, loadParam); assets.finishLoading()   Step 3: Add Loaded ParticleEffects to the ParticleSystem  ParticleEffect originalEffect = assets.get(\"particle/effect.pfx\"); // we cannot use the originalEffect, we must make a copy each time we create new particle effect ParticleEffect effect = originalEffect.copy(); effect.init(); effect.start();  // optional: particle will begin playing immediately particleSystem.add(effect);   Your game most likely will have many particle effects, either at once or over time during game play. You really don’t want to make a new copy of the particle effect each time you create an object or graphical effect that needs it. Instead, you should pool the effects to avoid new object creation. You can read more about Pooling in this wiki or the libGDX Pool class documentation.   Here is an example of a Pool:  private static class PFXPool extends Pool&lt;ParticleEffect&gt; { \tprivate ParticleEffect sourceEffect;  \tpublic PFXPool(ParticleEffect sourceEffect) { \t\tthis.sourceEffect = sourceEffect; \t}  \t@Override \tpublic void free(ParticleEffect pfx) { \t\tpfx.reset(); \t\tsuper.free(pfx); \t}  \t@Override \tprotected ParticleEffect newObject() { \t\treturn sourceEffect.copy(); \t} }  Note that we reset the particle when it is freed, not during obtain. This avoids a NullPointerException that occurs because of how the ParticleSystem works.   Step 4: Rendering our 3D Particles Using the ParticleSystem  A ParticleSystem must update and draw its own components, then be passed to a ModelBatch instance to be rendered to the scene.  private void renderParticleEffects() { \tparticleSystem.update(); // technically not necessary for rendering \tparticleSystem.begin(); \tparticleSystem.draw(); \tparticleSystem.end(); \tmodelBatch.render(particleSystem); }   You can also translate and rotate the effect. Depending on how your engine works you might want to use a specific matrix that is reset to identity on changes or only add the delta transformation/rotation.   private void renderParticleEffects() { \ttargetMatrix.idt(); \ttargetMatrix.translate(targetPos); \teffect.setTransform(targetMatrix); \tparticleSystem.update(); // technically not necessary for rendering \tparticleSystem.begin(); \tparticleSystem.draw(); \tparticleSystem.end(); \tmodelBatch.render(particleSystem); }   or   private void renderParticleEffects() { \teffect.translate(deltaPos); \tparticleSystem.update(); // technically not necessary for rendering \tparticleSystem.begin(); \tparticleSystem.draw(); \tparticleSystem.end(); \tmodelBatch.render(particleSystem); }   Stop New Particle Emission, But Let Existing Particles Finish Playing  It is a little bit more complicated to do this in the 3D Particle System:   Emitter emitter = pfx.getControllers().first().emitter; \t\tif (emitter instanceof RegularEmitter) { \t\t\tRegularEmitter reg = (RegularEmitter) emitter; \t\t\treg.setEmissionMode(RegularEmitter.EmissionMode.EnabledUntilCycleEnd); \t\t}   Simple Examples  A simplified example of the above GdxTest.java can be found here.   Custom Textures  In order to create new particle images, you must save them as RGBA PNG with exactly 8 bytes per channel otherwise they will not be rendered properly.   GIMP Settings:      ",
        
        "url": "/wiki/graphics/3d/3d-particle-effects" },{
        "title": "3D Picking",
        "excerpt":
        
        "   Compose a pick ray with and origin and direction:   float viewportX = (2.0f * getMousePosX()) / viewportWidth - 1.0f; float viewportY = (2.0f * (viewportHeight - getMousePosY())) / viewportHeight - 1.0f;  Vector3 gdxOrigin = new Vector3(); gdxOrigin.set(viewportX, viewportY, -1.0f); gdxOrigin.prj(camera.invProjectionView);  Vector3 gdxDirection = new Vector3(); gdxDirection.set(viewportX, viewportY, 1.0f); gdxDirection.prj(camera.invProjectionView); gdxDirection.sub(gdxOrigin).nor();  // collide geometries   One way to do picking is to do CPU based collision math using Euclid, a performant and comprehensive vector math and geometry library.  ",
        
        "url": "/wiki/graphics/3d/3d-picking" },{
        "title": "A Simple Game",
        "excerpt":
        
        "Before diving into the APIs provided by libGDX, let’s create a very simple “game”, that touches each module provided by the framework, to get a feeling for things. We’ll introduce a few different concepts without going into unnecessary detail.                                                                                                   Set Up a Dev Environment                                                                                           Generate a Project                                                                                         Importing &amp; Running                                                                                         A Simple Game                                       In the following, we’ll look at:      Basic file access   Clearing the screen   Drawing images   Using a camera   Basic input processing   Playing sound effects   Project Setup  Follow the steps in the Generating a Project guide. In the following, we will use these settings:      Application name: drop   Package name: com.badlogic.drop   Game class: Drop   Now fill in the destination. If you are interested in Android development, be sure to check that option and provide the Android SDK folder. For the purpose of this tutorial, we will uncheck the iOS sub project (as you would need OS X to run it) and all extensions (extensions are a more advanced topic).   Once imported into your IDE, you should have 5 projects or modules: the main one drop, and the sub projects android (or drop-android under Eclipse), core / drop-core, desktop / drop-desktop, and html / drop-html.   To launch or debug the game, see the page Importing &amp; Running a Project.   If we just run the project, we will get an error: Couldn't load file: badlogic.jpg. Your Run Configuration has to be properly configured first: Select as working directory PATH_TO_YOUR_PROJECT/​drop/assets. In addition, if you are working on macOS, be sure to either add the com.badlogicgames.gdx:gdx-lwjgl3-glfw-awt-macos dependency to your desktop project or set the VM Options to -XstartOnFirstThread. When we run the game now, we will get the default ‘game’ generated by the setup app: a Badlogic Games image on a red background. Not too exciting, but that’s about to change.   The Game  The game idea is very simple:      Catch raindrops with a bucket.   The bucket is located in the lower part of the screen.   Raindrops spawn randomly at the top of the screen every second and accelerate downwards.   Player can drag the bucket horizontally via the mouse/touch or move it via the left and right cursor keys.   The game has no end - think of it as a zen-like experience :)   The Assets  We need a few images and sound effects to make the game look somewhat pretty. For the graphics we need to define a target resolution of 800x480 pixels (landscape mode on Android). If the device the game is run on does not have that resolution, we simply scale everything to fit on the screen.   Note: for high profile games you might want to consider using different assets for different screen densities. This is a big topic on its own and won’t be covered here.   The raindrop and the bucket should take up a small(ish) portion of the screen vertically, so we’ll let them have a size of 64x64 pixels.   The following sources provide some sample assets:      water drop sound by junggle, see here   rain sounds by acclivity, see here   droplet sprite by mvdv, see here   bucket sprite by mvdv, see here   To make the assets available to the game, we have to place them in the assets folder, which is located in the root directory of your game. I named the 4 files: drop.wav, rain.mp3, droplet.png and bucket.png and put them in assets/. We only need to store the assets once, as both the desktop and HTML5 projects are configured to ‘see’ this folder through different means. After that, depending on your IDE you may have to refresh the project tree to make the new files known (in Eclipse, right click -&gt; Refresh), otherwise you may get a ‘file not found’ runtime exception.   Configuring the Starter Classes  Given our requirements we can now configure our different starter classes. We’ll start with the desktop project. Open the DesktopLauncher.java class in desktop/src/… (or drop-desktop under Eclipse). We want a 800x480 window and set the title to “Drop”. The code should look like this:   package com.badlogic.drop.desktop;  import com.badlogic.gdx.backends.lwjgl3.Lwjgl3Application; import com.badlogic.gdx.backends.lwjgl3.Lwjgl3ApplicationConfiguration; import com.badlogic.drop.Drop;  public class DesktopLauncher {    public static void main (String[] arg) {       Lwjgl3ApplicationConfiguration config = new Lwjgl3ApplicationConfiguration();       config.setTitle(\"Drop\");       config.setWindowedMode(800, 480);       config.useVsync(true);       config.setForegroundFPS(60);       new Lwjgl3Application(new Drop(), config);    } }   If you are only interested in desktop development, you can skip the rest of this section.   Moving on to the Android project, we want the application to be run in landscape mode. For this we need to modify AndroidManifest.xml in the android (or drop-android) root directory, which looks like this:   &lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt; &lt;manifest xmlns:android=\"http://schemas.android.com/apk/res/android\"     package=\"com.badlogic.drop.android\"     android:versionCode=\"1\"     android:versionName=\"1.0\" &gt;      &lt;uses-sdk android:minSdkVersion=\"8\" android:targetSdkVersion=\"20\" /&gt;      &lt;application         android:allowBackup=\"true\"         android:icon=\"@drawable/ic_launcher\"         android:label=\"@string/app_name\"         android:theme=\"@style/GdxTheme\" &gt;         &lt;activity             android:name=\"com.badlogic.drop.android.AndroidLauncher\"             android:label=\"@string/app_name\"             android:screenOrientation=\"landscape\"             android:configChanges=\"keyboard|keyboardHidden|orientation|screenSize\"&gt;             &lt;intent-filter&gt;                 &lt;action android:name=\"android.intent.action.MAIN\" /&gt;                 &lt;category android:name=\"android.intent.category.LAUNCHER\" /&gt;             &lt;/intent-filter&gt;         &lt;/activity&gt;     &lt;/application&gt;  &lt;/manifest&gt;   The setup tool already filled in the correct values for us, android:screenOrientation is set to “landscape”. If we wanted to run the game in portrait mode we would have set that attribute to “portrait”.   We also want to conserve battery and disable the accelerometer and compass. We do this in the AndroidLauncher.java file in android/src/… (or drop-android), which should look something like this:   package com.badlogic.drop.android;  import android.os.Bundle;  import com.badlogic.gdx.backends.android.AndroidApplication; import com.badlogic.gdx.backends.android.AndroidApplicationConfiguration; import com.badlogic.drop.Drop;  public class AndroidLauncher extends AndroidApplication {    @Override    protected void onCreate(Bundle savedInstanceState) {       super.onCreate(savedInstanceState);       AndroidApplicationConfiguration config = new AndroidApplicationConfiguration();       config.useAccelerometer = false;       config.useCompass = false;       initialize(new Drop(), config);    } }   We cannot define the resolution of the Activity, as it is set by the Android operating system. As we defined earlier, we’ll simply scale the 800x480 target resolution to whatever the resolution of the device is.   Finally we want to make sure the HTML5 project also uses a 800x480 drawing area. For this we modify the HtmlLauncher.java file in html/src/… (or drop-html):   package com.badlogic.drop.client;  import com.badlogic.gdx.ApplicationListener; import com.badlogic.gdx.backends.gwt.GwtApplication; import com.badlogic.gdx.backends.gwt.GwtApplicationConfiguration; import com.badlogic.drop.Drop;  public class HtmlLauncher extends GwtApplication {    @Override    public GwtApplicationConfiguration getConfig () {       return new GwtApplicationConfiguration(800, 480);    }     @Override    public ApplicationListener createApplicationListener () {       return new Drop();    } }   All our starter classes are now correctly configured, let’s move on to implementing our fabulous game.   The Code  We want to split up our code into a few sections. For the sake of simplicity we keep everything in the Drop.java file of the Core project, located in core/src/… (or drop-core in Eclipse).   Loading the Assets  Our first task is to load the assets and store references to them. Assets are usually loaded in the ApplicationAdapter.create() method, so let’s do that:   package com.badlogic.drop;  import com.badlogic.gdx.ApplicationAdapter; import com.badlogic.gdx.Gdx; import com.badlogic.gdx.audio.Music; import com.badlogic.gdx.audio.Sound; import com.badlogic.gdx.graphics.Texture;  public class Drop extends ApplicationAdapter {    private Texture dropImage;    private Texture bucketImage;    private Sound dropSound;    private Music rainMusic;     @Override    public void create() {       // load the images for the droplet and the bucket, 64x64 pixels each       dropImage = new Texture(Gdx.files.internal(\"droplet.png\"));       bucketImage = new Texture(Gdx.files.internal(\"bucket.png\"));        // load the drop sound effect and the rain background \"music\"       dropSound = Gdx.audio.newSound(Gdx.files.internal(\"drop.wav\"));       rainMusic = Gdx.audio.newMusic(Gdx.files.internal(\"rain.mp3\"));        // start the playback of the background music immediately       rainMusic.setLooping(true);       rainMusic.play();        // ... more to come ...    }     // rest of class omitted for clarity   For each of our assets we have a field in the Drop class so we can later refer to it. The first two lines in the create() method load the images for the raindrop and the bucket. A Texture represents a loaded image that is stored in video ram. One can usually not draw to a Texture. A Texture is loaded by passing a FileHandle to an asset file to its constructor. Such FileHandle instances are obtained through one of the methods provided by Gdx.files. There are different types of files, we use the “internal” file type here to refer to our assets. Internal files are located in the assets directory of the Android project. As seen before, the desktop and HTML5 projects reference the same directory.   Next we load the sound effect and the background music. libGDX differentiates between sound effects, which are stored in memory, and music, which is streamed from wherever it is stored. Music is usually too big to be kept in memory completely, hence the differentiation. As a rule of thumb, you should use a Sound instance if your sample is shorter than 10 seconds, and a Music instance for longer audio pieces.   Note: libGDX supports MP3, OGG and WAV files. Which format you should use, depends on your specific needs, as each format has its own advantages and disadvantages. For example, WAV files are quite large compared to other formats, OGG files don’t work on RoboVM (iOS) nor with Safari (GWT), and MP3 files have issues with seemless looping.   Loading of a Sound or Music instance is done via Gdx.audio.newSound() and Gdx.audio.newMusic(). Both of these methods take a FileHandle, just like the Texture constructor.   At the end of the create() method we also tell the Music instance to loop and start playback immediately. If you run the application you’ll see a nice pink background and hear the rain fall.   A Camera and a SpriteBatch  Next up, we want to create a camera and a SpriteBatch. We’ll use the former to ensure we can render using our target resolution of 800x480 pixels no matter what the actual screen resolution is. The SpriteBatch is a special class that is used to draw 2D images, like the textures we loaded.   We add two new fields to the class, let’s call them camera and batch:      private OrthographicCamera camera;    private SpriteBatch batch;   In the create() method we first create the camera like this:      camera = new OrthographicCamera();    camera.setToOrtho(false, 800, 480);   This will make sure the camera always shows us an area of our game world that is 800x480 units wide. Think of it as a virtual window into our world. We currently interpret the units as pixels to make our life a little easier. There’s nothing preventing us from using other units though, e.g. meters or whatever you have. Cameras are very powerful and allow you to do a lot of things we won’t cover in this basic tutorial. Check out the rest of the developer guide for more information.   Next we create the SpriteBatch (we are still in the create() method):      batch = new SpriteBatch();   We are almost done with creating all the things we need to run this simple game.   Adding the Bucket  The last bits that are missing are representations of our bucket and the raindrop. Let’s think about what we need to represent those in code:      A bucket/raindrop has an x/y position in our 800x480 units world.   A bucket/raindrop has a width and height, expressed in the units of our world.   A bucket/raindrop has a graphical representation, we already have those in form of the Texture instances we loaded.   So, to describe both the bucket and raindrops we need to store their position and size. libGDX provides a Rectangle class which we can use for this purpose. Let’s start by creating a Rectangle that represents our bucket. We add a new field:      // add this import and NOT the one in the standard library    import com.badlogic.gdx.math.Rectangle;     private Rectangle bucket;   In the create() method we instantiate the Rectangle and specify its initial values. We want the bucket to be 20 pixels above the bottom edge of the screen, and centered horizontally.      bucket = new Rectangle();    bucket.x = 800 / 2 - 64 / 2;    bucket.y = 20;    bucket.width = 64;    bucket.height = 64;   We center the bucket horizontally and place it 20 pixels above the bottom edge of the screen. Wait, why is bucket.y set to 20, shouldn’t it be 480 - 20? By default, all rendering in libGDX (and OpenGL) is performed with the y-axis pointing upwards. The x/y coordinates of the bucket define the bottom left corner of the bucket, the origin for drawing is located in the bottom left corner of the screen. The width and height of the rectangle are set to 64x64, our small-ish portion of our target resolutions height.   Note: it is possible to change this setup so the y-axis points down and the origin is in the upper left corner of the screen. OpenGL and the camera class are so flexible that you use have pretty much any kind of viewing angle you want, in 2D and 3D. However, this is not recommended.   Rendering the Bucket  Time to render our bucket. The first thing we want to do is to clear the screen with a dark blue color. Simply change the render() method to look like this:      @Override    public void render() {       ScreenUtils.clear(0, 0, 0.2f, 1);        ... more to come here ...    }   The arguments for ScreenUtils.clear(r, g, b, a) are the red, green, blue and alpha component of that color, each within the range [0, 1].   Next we need to tell our camera to make sure it is updated. Cameras use a mathematical entity called a matrix that is responsible for setting up the coordinate system for rendering. These matrices need to be recomputed every time we change a property of the camera, like its position. We don’t do this in our simple example, but it is generally a good practice to update the camera once per frame:      camera.update();   Now we can render our bucket:      batch.setProjectionMatrix(camera.combined);    batch.begin();    batch.draw(bucketImage, bucket.x, bucket.y);    batch.end();   The first line tells the SpriteBatch to use the coordinate system specified by the camera. As stated earlier, this is done with something called a matrix, to be more specific, a projection matrix. The camera.combined field is such a matrix. From there on the SpriteBatch will render everything in the coordinate system described earlier.   Next we tell the SpriteBatch to start a new batch. Why do we need this and what is a batch? OpenGL hates nothing more than telling it about individual images. It wants to be told about as many images to render as possible at once.   The SpriteBatch class helps make OpenGL happy. It will record all drawing commands in between SpriteBatch.begin() and SpriteBatch.end(). Once we call SpriteBatch.end() it will submit all drawing requests we made at once, speeding up rendering quite a bit. This all might look cumbersome in the beginning, but it is what makes the difference between rendering 500 sprites at 60 frames per second and rendering 100 sprites at 20 frames per second.   Making the Bucket Move (Touch/Mouse)  Time to let the user control the bucket. Earlier we said we’d allow the user to drag the bucket. Let’s make things a little bit easier. If the user touches the screen (or presses a mouse button), we want the bucket to center around that position horizontally. Adding the following code to the bottom of the render() method will do this:      if(Gdx.input.isTouched()) {       Vector3 touchPos = new Vector3();       touchPos.set(Gdx.input.getX(), Gdx.input.getY(), 0);       camera.unproject(touchPos);       bucket.x = touchPos.x - 64 / 2;    }   First we ask the input module whether the screen is currently touched (or a mouse button is pressed) by calling Gdx.input.isTouched(). Next we want to transform the touch/mouse coordinates to our camera’s coordinate system. This is necessary because the coordinate system in which touch/mouse coordinates are reported might be different than the coordinate system we use to represent objects in our world.   Gdx.input.getX() and Gdx.input.getY() return the current touch/mouse position (libGDX also supports multi-touch, but that’s a topic for a different article). To transform these coordinates to our camera’s coordinate system, we need to call the camera.unproject() method, which requests a Vector3, a three dimensional vector. We create such a vector, set the current touch/mouse coordinates and call the method. The vector will now contain the touch/mouse coordinates in the coordinate system our bucket lives in. Finally we change the position of the bucket to be centered around the touch/mouse coordinates.   Note: it is very, very bad to instantiate a lot of new objects, such as the Vector3 instance. The reason for this is the garbage collector has to kick in frequently to collect these short-lived objects. While on the desktop this not such a big deal (due to the resources available), on Android the GC can cause pauses of up to a few hundred milliseconds, which results in stuttering. In this particular case, if you want to solve this issue, simply make touchPos a private final field of the Drop class instead of instantiating it all the time.   Note: touchPos is a three dimensional vector. You might wonder why that is if we only operate in 2D. OrthographicCamera is actually a 3D camera which takes into account z-coordinates as well. Think of CAD applications, they use 3D orthographic cameras as well. We simply abuse it to draw 2D graphics.   Making the Bucket Move (Keyboard)  On the desktop and in the browser we can also receive keyboard input. Let’s make the bucket move when the left or right cursor key is pressed.   We want the bucket to move without acceleration, at two hundred pixels/units per second, either to the left or the right. To implement such time-based movement we need to know the time that passed in between the last and the current rendering frame. Here’s how we can do all this:      if(Gdx.input.isKeyPressed(Input.Keys.LEFT)) bucket.x -= 200 * Gdx.graphics.getDeltaTime();    if(Gdx.input.isKeyPressed(Input.Keys.RIGHT)) bucket.x += 200 * Gdx.graphics.getDeltaTime();   The method Gdx.input.isKeyPressed() tells us whether a specific key is pressed. The Keys enumeration contains all the keycodes that libGDX supports. The method Gdx.graphics.getDeltaTime() returns the time passed between the last and the current frame in seconds. All we need to do is modify the bucket’s x-coordinate by adding/subtracting 200 units times the delta time in seconds.   We also need to make sure our bucket stays within the screen limits:      if(bucket.x &lt; 0) bucket.x = 0;    if(bucket.x &gt; 800 - 64) bucket.x = 800 - 64;   Adding the Raindrops  For the raindrops we keep a list of Rectangle instances, each keeping track of the position and size of a raindrop. Let’s add that list as a field:      private Array&lt;Rectangle&gt; raindrops;   The Array class is a libGDX utility class to be used instead of standard Java collections like ArrayList. The problem with the latter is that they produce garbage in various ways. The Array class tries to minimize garbage as much as possible. libGDX offers other garbage collector aware collections such as hash-maps or sets as well.   We also need to keep track of the last time we spawned a raindrop, so we add another field:      private long lastDropTime;   We’ll store the time in nanoseconds, that’s why we use a long.   To facilitate the creation of raindrops we’ll write a method called spawnRaindrop() which instantiates a new Rectangle, sets it to a random position at the top edge of the screen and adds it to the raindrops array.      private void spawnRaindrop() {       Rectangle raindrop = new Rectangle();       raindrop.x = MathUtils.random(0, 800-64);       raindrop.y = 480;       raindrop.width = 64;       raindrop.height = 64;       raindrops.add(raindrop);       lastDropTime = TimeUtils.nanoTime();    }   The method should be pretty self-explanatory. The MathUtils class is a libGDX class offering various math related static methods. In this case it will return a random value between zero and 800 - 64. The TimeUtils is another libGDX class that provides some very basic time related static methods. In this case we record the current time in nano seconds based on which we’ll later decide whether to spawn a new drop or not.   In the create() method we now instantiate the raindrops array and spawn our first raindrop:   We need to instantiate that array in the create() method:      raindrops = new Array&lt;Rectangle&gt;();    spawnRaindrop();   Next we add a few lines to the render() method that will check how much time has passed since we spawned a new raindrop, and creates a new one if necessary:      if(TimeUtils.nanoTime() - lastDropTime &gt; 1000000000) spawnRaindrop();   We also need to make our raindrops move, let’s take the easy route and have them move at a constant speed of 200 pixels/units per second. If the raindrop is beneath the bottom edge of the screen, we remove it from the array.      for (Iterator&lt;Rectangle&gt; iter = raindrops.iterator(); iter.hasNext(); ) {       Rectangle raindrop = iter.next();       raindrop.y -= 200 * Gdx.graphics.getDeltaTime();       if(raindrop.y + 64 &lt; 0) iter.remove();    }   The raindrops need to be rendered. We’ll add that to the SpriteBatch rendering code which looks like this now:      batch.begin();    batch.draw(bucketImage, bucket.x, bucket.y);    for(Rectangle raindrop: raindrops) {       batch.draw(dropImage, raindrop.x, raindrop.y);    }    batch.end();   One final adjustment: if a raindrop hits the bucket, we want to playback our drop sound and remove the raindrop from the array. We simply add the following lines to the raindrop update loop:         if(raindrop.overlaps(bucket)) {          dropSound.play();          iter.remove();       }   The Rectangle.overlaps() method checks if this rectangle overlaps with another rectangle. In our case, we tell the drop sound effect to play itself and remove the raindrop from the array.   Cleaning Up  A user can close the application at any time. For this simple example there’s nothing that needs to be done. However, it is in general a good idea to help out the operating system a little and clean up the mess we created.   Any libGDX class that implements the Disposable interface and thus has a dispose() method needs to be cleaned up manually once it is no longer used. In our example that’s true for the textures, the sound and music and the SpriteBatch. Being good citizens, we override the ApplicationAdapter.dispose() method as follows:      @Override    public void dispose() {       dropImage.dispose();       bucketImage.dispose();       dropSound.dispose();       rainMusic.dispose();       batch.dispose();    }   Once you dispose of a resource, you should not access it in any way.   Disposables are usually native resources which are not handled by the Java garbage collector. This is the reason why we need to manually dispose of them. libGDX provides various ways to help with asset management. Read the rest of the development guide to discover them.   Handling Pausing/Resuming  Android has the notation of pausing and resuming your application every time the user gets a phone call or presses the home button. libGDX will do many things automatically for you in that case, e.g. reload images that might have gotten lost (OpenGL context loss, a terrible topic on its own), pause and resume music streams and so on.   In our game there’s no real need to handle pausing/resuming. As soon as the user comes back to the application, the game continues where it left. Usually one would implement a pause screen and ask the user to touch the screen to continue. This is left as an exercise for the reader - check out the ApplicationAdapter.pause() and ApplicationAdapter.resume() methods.   The Full Source  Here’s the tiny source for our simple game:   package com.badlogic.drop;  import java.util.Iterator;  import com.badlogic.gdx.ApplicationAdapter; import com.badlogic.gdx.Gdx; import com.badlogic.gdx.Input.Keys; import com.badlogic.gdx.audio.Music; import com.badlogic.gdx.audio.Sound; import com.badlogic.gdx.graphics.OrthographicCamera; import com.badlogic.gdx.graphics.Texture; import com.badlogic.gdx.graphics.g2d.SpriteBatch; import com.badlogic.gdx.math.MathUtils; import com.badlogic.gdx.math.Rectangle; import com.badlogic.gdx.math.Vector3; import com.badlogic.gdx.utils.Array; import com.badlogic.gdx.utils.ScreenUtils; import com.badlogic.gdx.utils.TimeUtils;  public class Drop extends ApplicationAdapter {    private Texture dropImage;    private Texture bucketImage;    private Sound dropSound;    private Music rainMusic;    private SpriteBatch batch;    private OrthographicCamera camera;    private Rectangle bucket;    private Array&lt;Rectangle&gt; raindrops;    private long lastDropTime;     @Override    public void create() {       // load the images for the droplet and the bucket, 64x64 pixels each       dropImage = new Texture(Gdx.files.internal(\"droplet.png\"));       bucketImage = new Texture(Gdx.files.internal(\"bucket.png\"));        // load the drop sound effect and the rain background \"music\"       dropSound = Gdx.audio.newSound(Gdx.files.internal(\"drop.wav\"));       rainMusic = Gdx.audio.newMusic(Gdx.files.internal(\"rain.mp3\"));        // start the playback of the background music immediately       rainMusic.setLooping(true);       rainMusic.play();        // create the camera and the SpriteBatch       camera = new OrthographicCamera();       camera.setToOrtho(false, 800, 480);       batch = new SpriteBatch();        // create a Rectangle to logically represent the bucket       bucket = new Rectangle();       bucket.x = 800 / 2 - 64 / 2; // center the bucket horizontally       bucket.y = 20; // bottom left corner of the bucket is 20 pixels above the bottom screen edge       bucket.width = 64;       bucket.height = 64;        // create the raindrops array and spawn the first raindrop       raindrops = new Array&lt;Rectangle&gt;();       spawnRaindrop();    }     private void spawnRaindrop() {       Rectangle raindrop = new Rectangle();       raindrop.x = MathUtils.random(0, 800-64);       raindrop.y = 480;       raindrop.width = 64;       raindrop.height = 64;       raindrops.add(raindrop);       lastDropTime = TimeUtils.nanoTime();    }     @Override    public void render() {       // clear the screen with a dark blue color. The       // arguments to clear are the red, green       // blue and alpha component in the range [0,1]       // of the color to be used to clear the screen.       ScreenUtils.clear(0, 0, 0.2f, 1);        // tell the camera to update its matrices.       camera.update();        // tell the SpriteBatch to render in the       // coordinate system specified by the camera.       batch.setProjectionMatrix(camera.combined);        // begin a new batch and draw the bucket and       // all drops       batch.begin();       batch.draw(bucketImage, bucket.x, bucket.y);       for(Rectangle raindrop: raindrops) {          batch.draw(dropImage, raindrop.x, raindrop.y);       }       batch.end();        // process user input       if(Gdx.input.isTouched()) {          Vector3 touchPos = new Vector3();          touchPos.set(Gdx.input.getX(), Gdx.input.getY(), 0);          camera.unproject(touchPos);          bucket.x = touchPos.x - 64 / 2;       }       if(Gdx.input.isKeyPressed(Keys.LEFT)) bucket.x -= 200 * Gdx.graphics.getDeltaTime();       if(Gdx.input.isKeyPressed(Keys.RIGHT)) bucket.x += 200 * Gdx.graphics.getDeltaTime();        // make sure the bucket stays within the screen bounds       if(bucket.x &lt; 0) bucket.x = 0;       if(bucket.x &gt; 800 - 64) bucket.x = 800 - 64;        // check if we need to create a new raindrop       if(TimeUtils.nanoTime() - lastDropTime &gt; 1000000000) spawnRaindrop();        // move the raindrops, remove any that are beneath the bottom edge of       // the screen or that hit the bucket. In the latter case we play back       // a sound effect as well.       for (Iterator&lt;Rectangle&gt; iter = raindrops.iterator(); iter.hasNext(); ) {          Rectangle raindrop = iter.next();          raindrop.y -= 200 * Gdx.graphics.getDeltaTime();          if(raindrop.y + 64 &lt; 0) iter.remove();          if(raindrop.overlaps(bucket)) {             dropSound.play();             iter.remove();          }       }    }     @Override    public void dispose() {       // dispose of all the native resources       dropImage.dispose();       bucketImage.dispose();       dropSound.dispose();       rainMusic.dispose();       batch.dispose();    } }   Where to go from here  This was a very basic example of how to use libGDX to create a minimalistic game. There are quite a few things that can be improved from here on. Your next steps should most certainly entail looking at Screens and Games. To learn about these, there is a second tutorial following on from this one.  ",
        
        "url": "/wiki/start/a-simple-game" },{
        "title": "Accelerometer",
        "excerpt":
        
        "An accelerometer measures the acceleration of a device on three axes (at least on Android). From this acceleration one can derive the tilt or orientation of the device.   Acceleration is measured in meters per second per second (m/s²). If an axis is pointing straight towards the center of the earth, its acceleration will be roughly -10 m/s². If it is pointing in the opposite direction, the acceleration will be 10 m/s².   The axes in an Android device are setup as follows:      Unfortunately, this configuration is different for tablets. Android devices have a notion called default orientation. For phones, portrait mode (as in the image above) is the default orientation. For tablets, landscape mode is the default orientation. A default landscape orientation device has its axes rotated, so that the y-axis points up the smaller side of the device and the x-axis points to the right of the wider side.   libGDX takes care of this and presents the accelerometer readings as shown in the image above, no matter the default orientation of the device (positive z-axis comes out of the screen, positive x-axis points to the right along the wider side of the device, positive y-axis points upwards along the smaller side of the device).   Checking Availability  Different Android devices have different hardware configurations. Checking whether the device has an accelerometer can be done as follows:   boolean available = Gdx.input.isPeripheralAvailable(Peripheral.Accelerometer);   Querying Current/Native Orientation  If your game needs to know the current orientation of the device, the following method can be used:   int orientation = Gdx.input.getRotation();   This will return a value of 0, 90, 180 or 270, giving you the angular difference between the current orientation and the native orientation.   The native orientation is either portrait mode (as in the image above) or landscape mode (mostly for tablets). It can be queried as follows:   Orientation nativeOrientation = Gdx.input.getNativeOrientation();   This returns either Orientation.Landscape or Orientation.Portrait.   Acceleration Readings   Accelerometer readings can only be accessed via polling in libgdx:       float accelX = Gdx.input.getAccelerometerX();     float accelY = Gdx.input.getAccelerometerY();     float accelZ = Gdx.input.getAccelerometerZ();   Platforms or devices that don’t have accelerometer support will return zero.   See the Super Jumper demo game for a demonstration on the usage of the accelerometer.   Rotation Matrix  If you want to use the orientation of your device for rendering, it might be beneficial to work with the rotation matrix. See this link for an explanation. You can plug the resulting matrix directly into your OpenGL rendering:   Matrix4 matrix = new Matrix4(); Gdx.input.getRotationMatrix(matrix.val); // use the matrix, Luke                  Prev       Next          ",
        
        "url": "/wiki/input/accelerometer" },{
        "title": "Adding new Keycodes",
        "excerpt":
        
        "Short info on how to add new keycodes:           Add the keycodes to Input.Keys (don’t forget to define Input.Keys.toString() for the new codes).       Android:  If they exist for Android, use same codes as in Android’s KeyEvent class. If not existent for Android, define a keycode non-clashing with Android’s codes and document it as a comment. Due to Android’s key code being same than libgdx, no change is needed in Android’s backend.   GWT: Changes needed in DefaultGwtInput.keyForCode(). GWT’s KeyCode.java is incomplete. Check with all browsers and OS at hand (might be slightly different) at keycode.info   RoboVM: Changes needed in DefaultIOSInput.getGdxKeyCode(). Check UIKeyboardHIDUsage enum (yes, you can do that on any OS).   Lwjgl: Use Keyboard constants in DefaultLwjglInput.getGdxKeyCode() and getLwjglKeyCode(), Lwjgl3 is similar. Don’t forget LwjglAwtInput   CHANGES: Document newly added keycodes and if they are completely new (unmapped before) or changing a mapping existing before   If you follow these steps, merge is highly likely.   For any controller issues, take a look at the gdx-controllers repo.   From MrStahlfelge’s comment at: #5389  ",
        
        "url": "/wiki/misc/adding-new-keycodes" },{
        "title": "Admob in libGDX",
        "excerpt":
        
        "   Introduction   Background   Setup   Initialization   Control   Code   iOS Setup (RoboVM)   Introduction   This article shows you how to set up AdMob with a libGDX app. This is current roughly with AdMob 4.0.4 and libGDX 0.9.1. The same instructions will work with Mobclix as well (and probably others), the only changes being the differences between the AdMob and Mobclix APIs. In the code snippets, I’m going to leave out the package and import lines for brevity. If you’re working in Eclipse, Ctrl-1 on any line showing an error will auto-fill-in the required imports.   I should note that this isn’t the only way to make this work. But it’s one approach that worked for me, so I decided to share it in the hopes that others might find it useful as well.   Please note that Google have deprecated 6.4.1 and earlier SDKs. For notes on how to use the new Google Mobile Ads approach, please see Google Mobile Ads. Thankfully the changes are very minimal from a developer/implementation point of view.   Background   Let’s look at the libGDX HelloWorld example and understand what it’s doing. There’s a HelloWorld class (in HelloWorld.java) that does all the libGDX stuff. There’s a HelloWorldDesktop class that creates and runs a HelloWorld on the desktop. And finally, there’s a HelloWorldAndroid class that creates and runs a HelloWorld on Android. Here’s what they look like:   HelloWorldDesktop:   public class HelloWorldDesktop {     public static void main (String[] argv) {         new LwjglApplication(new HelloWorld(), \"Hello World\", 480, 320, false);     } }   HelloWorldAndroid:   public class HelloWorldAndroid extends AndroidApplication {     @Override public void onCreate (Bundle savedInstanceState) {         super.onCreate(savedInstanceState);         initialize(new HelloWorld(), false);\t\t     } }   HelloWorldIOS:   public class HelloWorldIOS extends Delegate { \t@Override \tprotected IOSApplication createApplication() { \t\tfinal IOSApplicationConfiguration config = new IOSApplicationConfiguration(); \t\tconfig.orientationLandscape = false; \t\tconfig.orientationPortrait = true;  \t\treturn new IOSApplication(new HelloWorld(), config); \t} }   They’re pretty similar. They all create a new HelloWorld(), and pass that into something that sets up the application. On the desktop that’s a LwjglApplication, on Android that’s the initialize() method and on IOS it’s IOSApplication(). Then the HelloWorld class does all the work of the application.   Let’s take a closer look at the initialize() method. There are two forms, and one of them calls the other. If you follow that code through, here’s the stuff that sets up an Android application:       public void initialize(ApplicationListener listener, AndroidApplicationConfiguration config) {    \t graphics = new AndroidGraphics(this, ...  ...         requestWindowFeature(Window.FEATURE_NO_TITLE);        getWindow().setFlags(WindowManager.LayoutParams.FLAG_FULLSCREEN, WindowManager.LayoutParams.FLAG_FULLSCREEN);        getWindow().clearFlags(WindowManager.LayoutParams.FLAG_FORCE_NOT_FULLSCREEN);        setContentView(graphics.getView(), createLayoutParams());  ...   Let’s see what it’s doing:      Create an AndroidGraphics object with some parameters   Specify that the window has no title   Specify that the window is fullscreen   Call setContentView() with a View from the AndroidGraphics object, and some layout parameters   The last step hooks everything together, and sets up the Activity to use the libGDX view as its main (and only) View. I’m not going to explain what an Android Activity and View is, you can find that information easily enough with a Google search.   With that background, let’s look at what needs to be changed to add AdMob to the application.   Setup   To start out, follow the AdMob setup instructions as normal. Sign up, get your application key, configure your ad colors and refresh rate on the website, add the stuff needed to the AndroidManifest.xml file for permissions, activities, etc.   Initialization   The main thing to understand is that AdMob uses its own View. And we know that libGDX creates a View. If we call setContentView() with the libGDX View, then that’s what gets hooked up to the application, and there’s no place to add the AdMob view. So here’s what we need to do:      Create a Layout that can contain multiple views   Create the libGDX View, add that to the Layout   Create the AdMob view, add that to the Layout as well   Call setContentView() with the Layout   Let’s dive into some code. This example uses the RelativeLayout class, because I found it easy to get multiple overlapping Views on the screen using that. This assumes that you want your libGDX View to be full-screen, and the AdMob view to be overlaid on top. If you want some other layout, like the AdMob view sitting next to a partial-screen libGDX View, then you’ll probably have to use a different Layout and set it up accordingly.   First, create a RelativeLayout:           RelativeLayout layout = new RelativeLayout(this);   Easy enough. Then, we need to create the libGDX View. There’s a different initialization function, initializeForView(), that does this. It is similar to initialize(), but instead of calling setContentView() with the libGDX View, it returns the View to you so you can use it. The way you call it is identical to calling initialize():           View gameView = initializeForView(new HelloWorld(), false);   Let’s take a closer look inside initializeForView(). Again, there are two versions, and the bulk of the work is here:       public View initializeForView(ApplicationListener listener, AndroidApplicationConfiguration config) {    \t graphics = new AndroidGraphics(this, ...  ...         return graphics.getView();     }   You’ll note that this time, the function didn’t do anything about setting the Activity to be full-screen, removing the title, and so on. Most likely, you will want all those things. So make sure you call them in your AndroidActivity:           requestWindowFeature(Window.FEATURE_NO_TITLE);         getWindow().setFlags(WindowManager.LayoutParams.FLAG_FULLSCREEN,          \t\tWindowManager.LayoutParams.FLAG_FULLSCREEN);         getWindow().clearFlags(WindowManager.LayoutParams.FLAG_FORCE_NOT_FULLSCREEN);          View gameView = initializeForView(new HelloWorld(), false);   That completes the setup for the libGDX View. Next, we create the AdMob view and kick it off by calling loadAd(). This assumes that you want it to start fetching ads immediately. If you want more customized behavior, then you’ll have to configure the AdView accordingly. I’m not going to cover that here.           AdView adView = new AdView(this);         adView.setAdSize(AdSize.BANNER);         adView.setAdUnitId(\"xxxxxxxx\"); // Put in your secret key here          AdRequest adRequest = new AdRequest.Builder().build();         adView.loadAd(adRequest);   Now we have a Layout, and two Views. All that’s left is to add both the Views to the Layout, and then tell Android to use the Layout as the thing to display for the Activity.   Views get stacked in the sequence in which you add them, so make sure you add the libGDX View first, since you want that under the AdMob View. If you do the reverse, the full-screen libGDX View will hide the AdMob View, and you’ll be left wondering why your ads aren’t showing up.   Add the libGDX View. You can use the simpler form of addView(), because the libGDX View is full-screen, so there’s no positioning information needed.           layout.addView(gameView);   Now we need to add the AdMob View. Here, you probably want some more control, because you want to position the ads in some specific area of the screen. There’s another addView() that takes some layout parameters, so let’s use that. In this example, I’m telling Android that I want the AdMob View to be as large as necessary to show the full ad, and I want to align it with the top right of the screen (technically, it’s aligning to the top right of the parent, but the parent is the Layout which fills the screen, so it’s effectively aligning with the screen):           RelativeLayout.LayoutParams adParams =          \tnew RelativeLayout.LayoutParams(RelativeLayout.LayoutParams.WRAP_CONTENT,          \t\t\tRelativeLayout.LayoutParams.WRAP_CONTENT);         adParams.addRule(RelativeLayout.ALIGN_PARENT_TOP);         adParams.addRule(RelativeLayout.ALIGN_PARENT_RIGHT);   Then we add the AdMob View using these layout parameters:           layout.addView(adView, adParams);   And finally, all that’s needed is to tell Android to use this Layout:           setContentView(layout);   Putting all of that together, here’s what it looks like:   public class HelloWorldAndroid extends AndroidApplication {     @Override public void onCreate (Bundle savedInstanceState) {         super.onCreate(savedInstanceState);          // Create the layout         RelativeLayout layout = new RelativeLayout(this);          // Do the stuff that initialize() would do for you         requestWindowFeature(Window.FEATURE_NO_TITLE);         getWindow().setFlags(WindowManager.LayoutParams.FLAG_FULLSCREEN,          \t\tWindowManager.LayoutParams.FLAG_FULLSCREEN);         getWindow().clearFlags(WindowManager.LayoutParams.FLAG_FORCE_NOT_FULLSCREEN);          // Create the libGDX View         View gameView = initializeForView(new HelloWorld(), false);          // Create and setup the AdMob view         AdView adView = new AdView(this);         adView.setAdSize(AdSize.BANNER);         adView.setAdUnitId(\"xxxxxxxx\"); // Put in your secret key here          AdRequest adRequest = new AdRequest.Builder().build();         adView.loadAd(adRequest);          // Add the libGDX view         layout.addView(gameView);          // Add the AdMob view         RelativeLayout.LayoutParams adParams =          \tnew RelativeLayout.LayoutParams(RelativeLayout.LayoutParams.WRAP_CONTENT,          \t\t\tRelativeLayout.LayoutParams.WRAP_CONTENT);         adParams.addRule(RelativeLayout.ALIGN_PARENT_TOP);         adParams.addRule(RelativeLayout.ALIGN_PARENT_RIGHT);          layout.addView(adView, adParams);          // Hook it all up         setContentView(layout);     }   And that’s it. Now you should have ads showing on top of your libGDX app. If you want the ads to be visible all the time, this should be all you need to do. If you want to control when the ads are visible from within your libGDX app, then there’s a little more work left.   Control   This example will show you how to turn the AdMob View’s visibility on and off from within your libGDX app. Note that this is probably not the best way to control AdMob. If your ad View is invisible, but still fetching ads in the background, then you’re wasting ad impressions, and that will negatively impact your ad revenue. I’m not going to talk about the best way to control the AdMob view - that varies from application to application. Also, there are things you can do on the website, and things you can do in your app. So look through the AdMob documentation for more information on that.   The first thing you need to do is to make the AdView a member of your AndroidApplication, because you’ll need to refer to it later.   public class HelloWorldAndroid extends AndroidApplication {     protected AdView adView;      @Override public void onCreate (Bundle savedInstanceState) {  ...        adView = new AdView(this, AdSize.BANNER, \"xxxxxxxx\"); // Put in your secret key here   The next thing you need to understand is a Handler. This is an Android mechanism that lets you send messages between threads. The HelloWorld app runs in its own thread (let’s call it the ‘game thread’), which is different from the thread where you created the AdView (the ‘UI thread’). AdMob (and many other libraries, for that matter) gets cranky if you try to manipulate the AdView from any thread other than the UI thread. So what can you do? From the other thread, you send a message back to your UI thread. The next time the UI thread is scheduled to run, it picks up the message, and takes the action. All of this is handled by something called a Handler. Let’s see how you set that up:   public class HelloWorldAndroid extends AndroidApplication {      private final int SHOW_ADS = 1;     private final int HIDE_ADS = 0;      protected Handler handler = new Handler()     {         @Override         public void handleMessage(Message msg) {             switch(msg.what) {                 case SHOW_ADS:                 {                     adView.setVisibility(View.VISIBLE);                     break;                 }                 case HIDE_ADS:                 {                     adView.setVisibility(View.GONE);                     break;                 }             }         }     };   That’s the part that will process the message. You’ve defined two constants - SHOW_ADS and HIDE_ADS. If the message contains either one of those constants, this code makes the AdView visible or invisible, as instructed.   That leaves two things:      Some code to send a message containing the SHOW_ADS or HIDE_ADS command   Some way to call that code from within the libGDX app’s game thread   There are multiple ways to solve that problem. One solution is to use an interface that defines functionality that the AndroidApplication can provide, like the ability to show and hide ads. Here’s the definition of such an interface. Note that this should go in its own Java file:   IActivityRequestHandler.java:   public interface IActivityRequestHandler {    public void showAds(boolean show); }   This interface defines just one function, showAds(). If you later want to add other functionality that you need to call from the game thread and have it run in the UI thread, you can add more methods in here.   Now that we have an interface, our AndroidApplication must implement that interface:   public class HelloWorldAndroid extends AndroidApplication implements IActivityRequestHandler  {  ...      // This is the callback that posts a message for the handler     @Override     public void showAds(boolean show) {        handler.sendEmptyMessage(show ? SHOW_ADS : HIDE_ADS);     }   And there’s our implementation of the showAds() function. It’s pretty simple - it sends a message to our Handler. sendEmptyMessage is handy when the message only needs to contain a single code, as in this case. If you need to convey more information in the message, you’ll have to send a more complex message, and the Handler code for processing that message must change accordingly.   Now we have a mechanism to send a message to the UI thread, so that the UI thread can process it. Next, we need a way to get the game thread to call this functionality. To do this, the HelloWorld object needs to know about the HelloWorldAndroid object, so it can call the showAds() function. One way to do this is to define a constructor for HelloWorld that takes a parameter:   public class HelloWorld implements ApplicationListener {     private IActivityRequestHandler myRequestHandler;      public HelloWorld(IActivityRequestHandler handler) {         myRequestHandler = handler;     }   That lets you save the reference to the IActivityRequestHandler that was passed in by the caller. Now, you can access myRequestHandler from anywhere in this object, so when you want to turn ads on, all you do is:       myRequestHandler.showAds(true);   Back in the HelloWorldAndroid class, we need to create the HelloWorld with this new constructor:           View gameView = initializeForView(new HelloWorld(this), false);   Note the this parameter in the HelloWorld constructor - that’s how you pass in a reference to the HelloWorldAndroid object to the HelloWorld object.   There’s just one thing left. Remember the desktop version - that needs to be updated to call the new constructor as well. To do this, HelloWorldDesktop must implement the IActivityRequestHandler interface as well, so HelloWorld has something on which it can call showAds(). But the implementation can be empty here, since we don’t have any ads to show on the desktop. We need to cheat a little here, because main() is a static function, so there’s no this when you’re in that function. The following solution works, but there are probably better ways of handling this:   public class HelloWorldDesktop implements IActivityRequestHandler {     private static HelloWorldDesktop application;     public static void main (String[] argv) {         if (application == null) {             application = new HelloWorldDesktop();         } \t\t         new LwjglApplication(new HelloWorld(application), \"Hello World\", 480, 320, false);     }      @Override     public void showAds(boolean show) {         // TODO Auto-generated method stub \t     } }   That’s it. Now you can show and hide AdMob ads from within your libGDX app.   Code   Here’s the full code for the various classes, for completeness:   HelloWorldAndroid.java:   /*******************************************************************************  * Copyright 2011 See AUTHORS file.  *   * Licensed under the Apache License, Version 2.0 (the \"License\");  * you may not use this file except in compliance with the License.  * You may obtain a copy of the License at  *   *   http://www.apache.org/licenses/LICENSE-2.0  *   * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an \"AS IS\" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  ******************************************************************************/ /*  * Copyright 2010 Mario Zechner (contact@badlogicgames.com), Nathan Sweet (admin@esotericsoftware.com)  *   * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the  * License. You may obtain a copy of the License at  *   * http://www.apache.org/licenses/LICENSE-2.0  *   * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\"  * BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language  * governing permissions and limitations under the License.  */  package com.badlogic.gdx;  import android.os.Bundle; import android.os.Handler; import android.os.Message; import android.view.View; import android.view.Window; import android.view.WindowManager; import android.widget.RelativeLayout;  import com.badlogic.gdx.backends.android.AndroidApplication; import com.badlogic.gdx.helloworld.HelloWorld; import com.badlogic.gdx.helloworld.IActivityRequestHandler; import com.mobclix.android.sdk.MobclixMMABannerXLAdView;  public class HelloWorldAndroid extends AndroidApplication implements IActivityRequestHandler  {      protected AdView adView;      private final int SHOW_ADS = 1;     private final int HIDE_ADS = 0;      protected Handler handler = new Handler()     {         @Override         public void handleMessage(Message msg) {             switch(msg.what) {                 case SHOW_ADS:                 {                     adView.setVisibility(View.VISIBLE);                     break;                 }                 case HIDE_ADS:                 {                     adView.setVisibility(View.GONE);                     break;                 }             }         }     };      @Override public void onCreate (Bundle savedInstanceState) {         super.onCreate(savedInstanceState);          // Create the layout         RelativeLayout layout = new RelativeLayout(this);          // Do the stuff that initialize() would do for you         requestWindowFeature(Window.FEATURE_NO_TITLE);         getWindow().setFlags(WindowManager.LayoutParams.FLAG_FULLSCREEN,          \t\tWindowManager.LayoutParams.FLAG_FULLSCREEN);         getWindow().clearFlags(WindowManager.LayoutParams.FLAG_FORCE_NOT_FULLSCREEN);          // Create the libGDX View         View gameView = initializeForView(new HelloWorld(this), false);          // Create and setup the AdMob view         AdView adView = new AdView(this);         adView.setAdSize(AdSize.BANNER);         adView.setAdUnitId(\"xxxxxxxx\"); // Put in your secret key here          AdRequest adRequest = new AdRequest.Builder().build();         adView.loadAd(adRequest);          // Add the libGDX view         layout.addView(gameView);          // Add the AdMob view         RelativeLayout.LayoutParams adParams =          \tnew RelativeLayout.LayoutParams(RelativeLayout.LayoutParams.WRAP_CONTENT,          \t\t\tRelativeLayout.LayoutParams.WRAP_CONTENT);         adParams.addRule(RelativeLayout.ALIGN_PARENT_TOP);         adParams.addRule(RelativeLayout.ALIGN_PARENT_RIGHT);          layout.addView(adView, adParams);          // Hook it all up         setContentView(layout);     }      // This is the callback that posts a message for the handler     @Override     public void showAds(boolean show) {        handler.sendEmptyMessage(show ? SHOW_ADS : HIDE_ADS);     } }   HelloWorldDesktop.java:   /*******************************************************************************  * Copyright 2011 See AUTHORS file.  *   * Licensed under the Apache License, Version 2.0 (the \"License\");  * you may not use this file except in compliance with the License.  * You may obtain a copy of the License at  *   *   http://www.apache.org/licenses/LICENSE-2.0  *   * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an \"AS IS\" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  ******************************************************************************/ package com.badlogic.gdx.helloworld;  import com.badlogic.gdx.backends.lwjgl.LwjglApplication;  public class HelloWorldDesktop implements IActivityRequestHandler {     private static HelloWorldDesktop application;     public static void main (String[] argv) {         if (application == null) {             application = new HelloWorldDesktop();         } \t\t         new LwjglApplication(new HelloWorld(application), \"Hello World\", 480, 320, false);     }      @Override     public void showAds(boolean show) {         // TODO Auto-generated method stub \t     } }   HelloWorld.java:   /*******************************************************************************  * Copyright 2011 See AUTHORS file.  *   * Licensed under the Apache License, Version 2.0 (the \"License\");  * you may not use this file except in compliance with the License.  * You may obtain a copy of the License at  *   *   http://www.apache.org/licenses/LICENSE-2.0  *   * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an \"AS IS\" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  ******************************************************************************/ package com.badlogic.gdx.helloworld;  import com.badlogic.gdx.ApplicationListener; import com.badlogic.gdx.Files.FileType; import com.badlogic.gdx.Gdx; import com.badlogic.gdx.graphics.Color; import com.badlogic.gdx.graphics.GL10; import com.badlogic.gdx.graphics.Texture; import com.badlogic.gdx.graphics.Texture.TextureFilter; import com.badlogic.gdx.graphics.Texture.TextureWrap; import com.badlogic.gdx.graphics.g2d.BitmapFont; import com.badlogic.gdx.graphics.g2d.SpriteBatch; import com.badlogic.gdx.math.Vector2;  public class HelloWorld implements ApplicationListener { \tSpriteBatch spriteBatch; \tTexture texture; \tBitmapFont font; \tVector2 textPosition = new Vector2(100, 100); \tVector2 textDirection = new Vector2(1, 1);      private IActivityRequestHandler myRequestHandler;      public HelloWorld(IActivityRequestHandler handler) {         myRequestHandler = handler;     }          @Override public void create () { \t\tfont = new BitmapFont(); \t\tfont.setColor(Color.RED); \t\ttexture = new Texture(Gdx.files.internal(\"data/badlogic.jpg\")); \t\tspriteBatch = new SpriteBatch(); \t}  \t@Override public void render () { \t\tint centerX = Gdx.graphics.getWidth() / 2; \t\tint centerY = Gdx.graphics.getHeight() / 2;  \t\tGdx.graphics.getGL10().glClear(GL10.GL_COLOR_BUFFER_BIT); \t\t \t\t// more fun but confusing :) \t\t//textPosition.add(textDirection.tmp().mul(Gdx.graphics.getDeltaTime()).mul(60)); \t\ttextPosition.x += textDirection.x * Gdx.graphics.getDeltaTime() * 60; \t\ttextPosition.y += textDirection.y * Gdx.graphics.getDeltaTime() * 60;  \t\tif (textPosition.x &lt; 0 ) { \t\t\ttextDirection.x = -textDirection.x; \t\t\ttextPosition.x = 0; \t\t} \t\tif(textPosition.x &gt; Gdx.graphics.getWidth()) { \t\t\ttextDirection.x = -textDirection.x; \t\t\ttextPosition.x = Gdx.graphics.getWidth(); \t\t} \t\tif (textPosition.y &lt; 0) { \t\t\ttextDirection.y = -textDirection.y; \t\t\ttextPosition.y = 0;\t\t\t \t\t} \t\tif (textPosition.y &gt; Gdx.graphics.getHeight()) { \t\t\ttextDirection.y = -textDirection.y; \t\t\ttextPosition.y = Gdx.graphics.getHeight();\t\t\t \t\t}  \t\tspriteBatch.begin(); \t\tspriteBatch.setColor(Color.WHITE); \t\tspriteBatch.draw(texture,  \t\t\t\t\t\t\t  centerX - texture.getWidth() / 2,  \t\t\t\t\t\t\t  centerY - texture.getHeight() / 2,  \t\t\t\t\t\t\t  0, 0, texture.getWidth(), texture.getHeight());\t\t \t\tfont.draw(spriteBatch, \"Hello World!\", (int)textPosition.x, (int)textPosition.y); \t\tspriteBatch.end(); \t}  \t@Override public void resize (int width, int height) { \t\tspriteBatch.getProjectionMatrix().setToOrtho2D(0, 0, width, height); \t\ttextPosition.set(0, 0); \t}  \t@Override public void pause () {  \t}  \t@Override public void resume () {  \t} \t \t@Override public void dispose () {  \t} \t }   iOS Setup (RoboVM)   For admob to work on IOS it’s best to make sure you are doing the following things:           Make sure you project is using the latest libGDX version            Make sure you are on the latest RoboVM            Make sure you are using the latest admob bindings found here robovm-ios-bindings - admob            Admob needs a separate ad unit for iOS, so make sure you create a new app the key will be different than the one used for Android.                 Follow the AdMob RoboPod installation instructions.            Now with the configuration complete you can make calls to the AdMob API from Java and configure ads as desired. The following is an example on how to integrate a Banners ad at the top of the screen:       WARNING the following code may be a bit outdated for latest RoboPod version, use it as a reference   package com.badlogic.gdx;  import org.robovm.apple.coregraphics.CGRect; import org.robovm.apple.coregraphics.CGSize; import org.robovm.apple.foundation.NSArray; import org.robovm.apple.foundation.NSAutoreleasePool; import org.robovm.apple.foundation.NSObject; import org.robovm.apple.foundation.NSString; import org.robovm.apple.uikit.UIApplication; import org.robovm.apple.uikit.UIScreen; import org.robovm.bindings.admob.GADAdSizeManager; import org.robovm.bindings.admob.GADBannerView; import org.robovm.bindings.admob.GADBannerViewDelegateAdapter; import org.robovm.bindings.admob.GADRequest; import org.robovm.bindings.admob.GADRequestError;  import com.badlogic.gdx.Application; import com.badlogic.gdx.backends.iosrobovm.IOSApplication; import com.badlogic.gdx.backends.iosrobovm.IOSApplication.Delegate; import com.badlogic.gdx.backends.iosrobovm.IOSApplicationConfiguration; import com.badlogic.gdx.utils.Logger;  public class HelloWorldIOS extends Delegate implements IActivityRequestHandler { \tprivate static final Logger log = new Logger(HelloWorldIOS.class.getName(), Application.LOG_DEBUG); \tprivate static final boolean USE_TEST_DEVICES = true; \tprivate GADBannerView adview; \tprivate boolean adsInitialized = false; \tprivate IOSApplication iosApplication;  \t@Override \tprotected IOSApplication createApplication() { \t\tfinal IOSApplicationConfiguration config = new IOSApplicationConfiguration(); \t\tconfig.orientationLandscape = false; \t\tconfig.orientationPortrait = true;  \t\tiosApplication = new IOSApplication(new HelloWorld(this), config); \t\treturn iosApplication; \t}  \tpublic static void main(String[] argv) { \t\tNSAutoreleasePool pool = new NSAutoreleasePool(); \t\tUIApplication.main(argv, null, HelloWorldIOS.class); \t\tpool.close(); \t}  \t@Override \tpublic void hide() { \t\tinitializeAds();  \t\tfinal CGSize screenSize = UIScreen.getMainScreen().getBounds().size(); \t\tdouble screenWidth = screenSize.width();  \t\tfinal CGSize adSize = adview.getBounds().size(); \t\tdouble adWidth = adSize.width(); \t\tdouble adHeight = adSize.height();  \t\tlog.debug(String.format(\"Hidding ad. size[%s, %s]\", adWidth, adHeight));  \t\tfloat bannerWidth = (float) screenWidth; \t\tfloat bannerHeight = (float) (bannerWidth / adWidth * adHeight);  \t\tadview.setFrame(new CGRect(0, -bannerHeight, bannerWidth, bannerHeight)); \t}  \t@Override \tpublic void show() { \t\tinitializeAds();  \t\tfinal CGSize screenSize = UIScreen.getMainScreen().getBounds().size(); \t\tdouble screenWidth = screenSize.width();  \t\tfinal CGSize adSize = adview.getBounds().size(); \t\tdouble adWidth = adSize.width(); \t\tdouble adHeight = adSize.height();  \t\tlog.debug(String.format(\"Showing ad. size[%s, %s]\", adWidth, adHeight));  \t\tfloat bannerWidth = (float) screenWidth; \t\tfloat bannerHeight = (float) (bannerWidth / adWidth * adHeight);  \t\tadview.setFrame(new CGRect((screenWidth / 2) - adWidth / 2, 0, bannerWidth, bannerHeight)); \t}  \tpublic void initializeAds() { \t\tif (!adsInitialized) { \t\t\tlog.debug(\"Initalizing ads...\");  \t\t\tadsInitialized = true;  \t\t\tadview = new GADBannerView(GADAdSizeManager.smartBannerPortrait()); \t\t\tadview.setAdUnitID(\"xxxxxxxx\"); //put your secret key here \t\t\tadview.setRootViewController(iosApplication.getUIViewController()); \t\t\tiosApplication.getUIViewController().getView().addSubview(adview);  \t\t\tfinal GADRequest request = GADRequest.request(); \t\t\tif (USE_TEST_DEVICES) { \t\t\t\tfinal NSArray&lt;?&gt; testDevices = new NSArray&lt;NSObject&gt;( \t\t\t\t\t\tnew NSString(GADRequest.GAD_SIMULATOR_ID)); \t\t\t\trequest.setTestDevices(testDevices); \t\t\t\tlog.debug(\"Test devices: \" + request.getTestDevices()); \t\t\t}  \t\t\tadview.setDelegate(new GADBannerViewDelegateAdapter() { \t\t\t\t@Override \t\t\t\tpublic void didReceiveAd(GADBannerView view) { \t\t\t\t\tsuper.didReceiveAd(view); \t\t\t\t\tlog.debug(\"didReceiveAd\"); \t\t\t\t}  \t\t\t\t@Override \t\t\t\tpublic void didFailToReceiveAd(GADBannerView view, \t\t\t\t\t\tGADRequestError error) { \t\t\t\t\tsuper.didFailToReceiveAd(view, error); \t\t\t\t\tlog.debug(\"didFailToReceiveAd:\" + error); \t\t\t\t} \t\t\t});  \t\t\tadview.loadRequest(request);  \t\t\tlog.debug(\"Initalizing ads complete.\"); \t\t} \t}      @Override     public void showAds(boolean show) {     \tinitializeAds();         \tfinal CGSize screenSize = UIScreen.getMainScreen().getBounds().size(); \t\tdouble screenWidth = screenSize.width();  \t\tfinal CGSize adSize = adview.getBounds().size(); \t\tdouble adWidth = adSize.width(); \t\tdouble adHeight = adSize.height();  \t\tlog.debug(String.format(\"Hidding ad. size[%s, %s]\", adWidth, adHeight));  \t\tfloat bannerWidth = (float) screenWidth; \t\tfloat bannerHeight = (float) (bannerWidth / adWidth * adHeight);  \t\tif(show) { \t\t\tadview.setFrame(new CGRect((screenWidth / 2) - adWidth / 2, 0, bannerWidth, bannerHeight)); \t\t} else { \t\t\tadview.setFrame(new CGRect(0, -bannerHeight, bannerWidth, bannerHeight)); \t\t}     } }  ",
        
        "url": "/wiki/third-party/admob-in-libgdx" },{
        "title": "APK Expansions support",
        "excerpt":
        
        "Whenever the assets of a game exceed a certain size, in order to release the game on Google Play, the assets have to be placed into a separate file, to keep the APK size under 100MB. For more information about this you can read here.   How to use the feature  libGDX now has built-in support for detecting and reading assets from expansion files. The expansion file format accepted by libGDX is uncompressed (stored) zip file. Don’t use the jobb tool provided with the android SDK. Simply move some or all of your assets from android/assets into the zip file. Usually the file will be automatically downloaded from Google Play, however if you want to test it you will have to copy the file on the device manually.   The Android application then has to call from AndroidFiles the function setAPKExpansion() like this:  ((AndroidFiles)Gdx.files).setAPKExpansion(1, 0)  where the return value of the function can be used to check whether the expansion file(s) was opened successfully. The parameters are the version numbers of the main respectively patch files which on Google Play have to match the APK version against which these files were uploaded.   Afterwards your assets can be accessed using:  Gdx.files.internal(\"assetname.ext\")  just like you would normally.   Keep in mind that on lots of small files this can be inefficient because AndroidFiles first checks if a file exists in the assets and only afterwards it creates an AndroidZipFileHandle. If you want to optimize loading time use a FileHandleResolver like the ZipFileHandleResolver.  ",
        
        "url": "/wiki/misc/apk-expansions-support" },{
        "title": "Artificial Intelligence",
        "excerpt":
        
        "   Since libGDX 1.4.1, the gdx-ai extension has been moved to a separate repository under the libGDX umbrella. This allows us to have independent life-cycles. Besides offering some benefits in terms of project maintenance and visibility, this means that from now on the two projects will have distinct version numbers.   Please refer to the official gdx-ai wiki for information on how to implement and design intelligent agents. _:video_game:  ",
        
        "url": "/wiki/extensions/artificial-intelligence" },{
        "title": "Audio",
        "excerpt":
        
        "Introduction   libGDX provides methods to playback small sound effects as well as stream larger music pieces directly from disk. It also provides convenient read and write access to the audio hardware.   All access to the audio facilities is done through the audio module, referenced by:   Audio audio = Gdx.audio;   libGDX will automatically pause and resume all audio playback for you if your application is paused and resumed.   Audio on Android   libGDX Android backend uses the SoundPool API to play Sounds and the MediaPlayer for Music. These API have some limitations and known issues in certain scenarios:     Latency is not great and the default implementation is not recommended for latency sensitive apps like rhythm games.   Playing several sounds at the same time may cause performance issues on some devices. An easy way to fix it (with the limitation some methods are unsupported) is using the alternative Android implementation AsynchronousAndroidAudio by implementing createAudio()on AndroidLauncher like this:   @Override public AndroidAudio createAudio(Context context, AndroidApplicationConfiguration config) { \treturn new AsynchronousAndroidAudio(context, config); }   Generally speaking, Audio on Android is problematic and there may be other scenarios or device especific issues.   In an attempt to fix some of these issues Google created Oboe that can be used on libGDX projects thanks to libGDX Oboe.   ",
        
        "url": "/wiki/audio/audio" },{
        "title": "Back and menu key catching",
        "excerpt":
        
        "When a user presses the back button on an Android device, this usually kills the currently running activity. Games might chose to display a confirmation dialog before letting the user exit. For that to work one needs to catch the back key so it is not passed on to the operating system:   Gdx.input.setCatchKey(Input.Keys.BACK, true);   You will still receive key events if you have registered an InputProcessor, but the operating system will not close your application.   @Override public boolean keyDown(int keycode) {     if(keycode == Keys.BACK){        // Respond to the back button click here        return true;     }     return false; }   Note that the general paradigm in Android is to have the back key close the current activity. Deviating from this is usually viewed as bad practice.   Another key that might need to be caught is the menu key. If uncaught, it will bring up the on-screen keyboard after a long press. Catching this key can be done as follows:   Gdx.input.setCatchKey(Input.Keys.MENU, true);   There might be other keys to catch as well. You should catch all keys used to control your game to tell the operating system to prevent triggering behaviour outside your apps. This could affect media control keys on Android TV, and some general keys if you target HTML5 as well (see HTML 5 specifics article for more information)                  Prev       Next          ",
        
        "url": "/wiki/input/back-and-menu-key-catching" },{
        "title": "Bitmap fonts",
        "excerpt":
        
        "libGDX makes use of bitmap files (pngs) to render fonts. Each glyph in the font has a corresponding TextureRegion.   BitmapFont class (code)   BitmapFont was refactored for the libGDX 1.5.6 release. This blog post has details about the changes and also a small example showing how to move from pre 1.5.6 code to the new API.   A tutorial on using BitmapFont is available on LibGDX.info   File format specifications for the font file   References point to bmFont being originally created by Andreas Jönsson over at AngelCode   BMFont - the original specification for the file format.   Glyph Designer - Details about output, include a binary format.   Tools for Creating Bitmaps   Hiero - a utility for converting a system font to a bitmap   ShoeBox  - lets you load customized glyphs from an image, and then create a bitmap font from them. There’s a great tutorial for using it with libgdx.   Glyph Designer - a commercial bitmap font tool with a wide variety of options for shadows, gradients, stroke, etc.   Littera - online bitmap font generator, with a great amount of customizations (needs Adobe Flash).   Other Tools   FreeTypeFontGenerator - generating bitmaps for fonts instead of supplying a pre-rendered bitmap made by utilities like Hiero      Examples   (more)      FreeTypeFontGenerator generator = new FreeTypeFontGenerator(Gdx.files.internal(“data/unbom.ttf”));       FreeTypeFontParameter parameter = new FreeTypeFontParameter();   parameter.size = 18;   parameter.characters = “한국어/조선�?”;       BitmapFont koreanFont = generator.generateFont(parameter);       parameter.characters = FreeTypeFontGenerator.DEFAULT_CHARS;   generator = new FreeTypeFontGenerator(Gdx.files.internal(“data/russkij.ttf”));   BitmapFont cyrillicFont = generator.generateFont(parameter);   generator.dispose();       Distance field fonts - useful for scaling/rotating fonts without ugly artifacts   gdx-smart-font - unofficial libGDX addon for automatically generating and caching bitmap fonts based on screen size. (Uses FreeTypeFontGenerator)   Fonts in 3D space  While libGDX does not support placing text in 3D space directly, it is still possible to do so relatively easily. Check this gist for an example. Note that such text won’t be occluded by objects in front of it, as SpriteBatch draws with constant z, but it would not be hard to fix that with a custom shader, that would set the appropriate ‘z’ from an uniform.   Fixed-Width Fonts  Fonts that need to be displayed with the same width for every glyph present a special problem. The initial blank space at left before narrow chars such as | won’t be shown by default, and the narrow char will “cling” to just after the previous char, without the intended blank space. This also can cause issues with the width of that char being smaller, and that makes multiple lines of text unaligned with each other. There’s an existing BitmapFont#setFixedWidthGlyphs(CharSequence) method, which solves all this for the chars you have in the given CharSequence (usually a String). The catch is, you need to list every glyph in the font that needs to have the same width, and this can be a significant hassle for large fonts. If you’re sticking to ASCII or a small extension of it, Hiero has ASCII and NeHe buttons to fill the text field with those common smaller character sets, and you can copy that text into a String you pass to setFixedWidthGlyphs(). If you have a fixed-width font where the list of all chars that you could use is very large or unknown, you can use this code to set every glyph to fixed-width, using the largest glyph width for every glyph:          public static void setAllFixedWidth(BitmapFont font) {             BitmapFont.BitmapFontData data = font.getData();             int maxAdvance = 0;             for (int index = 0, end = 65536; index &lt; end; index++) {                 BitmapFont.Glyph g = data.getGlyph((char) index);                 if (g != null &amp;&amp; g.xadvance &gt; maxAdvance) maxAdvance = g.xadvance;             }             for (int index = 0, end = 65536; index &lt; end; index++) {                 BitmapFont.Glyph g = data.getGlyph((char) index);                 if (g == null) continue;                 g.xoffset += (maxAdvance - g.xadvance) / 2;                 g.xadvance = maxAdvance;                 g.kerning = null;                 g.fixedWidth = true;             }         }  ",
        
        "url": "/wiki/graphics/2d/fonts/bitmap-fonts" },{
        "title": "Box2d",
        "excerpt":
        
        "Setting up Box2D with libGDX   Box2D is a 2D physics library. It is one of the most popular physics libraries for 2D games and has been ported to many languages and many different engines, including libGDX. The Box2D implementation in libGDX is a thin Java wrapper around the C++ engine. Therefore, their documentation may come in handy.   Box2D is an extension and not included with libGDX by default. Thus a manual installation is required.   Table of Contents      Initialization   Creating a World   Debug Renderer   Stepping the simulation   Rendering   Objects/Bodies            Dynamic Bodies       Static Bodies       Kinematic Bodies           Impulses/Forces   Joints and Gears   Fixture Shapes   Sprites and Bodies   Sensors   Contact Listeners   Resources   Tools   Initialization   To initialize Box2D it is necessary to call Box2D.init(). For backwards compatibility, creating a World for the first time will have the same effect, but using the Box2D class should be preferred.   Creating a World   When setting up Box2D the first thing we need is a world. The world object is basically what holds all your physics objects/bodies and simulates the reactions between them. It does not however render the objects for you; for that you will use libGDX graphics functions. That said, libGDX does come with a Box2D debug renderer which is extremely handy for debugging your physics simulations, or even for testing your game-play before writing any rendering code.   To create the world we use the following code:   World world = new World(new Vector2(0, -10), true);   The first argument we supply is a 2D vector containing the gravity: 0 to indicate no gravity in the horizontal direction, and -10 is a downwards force like in real life (assuming your y axis points upwards). These values can be anything you like, but remember to stick to a constant scale. In Box2D 1 unit = 1 meter.   The second value in the world creation is a boolean value which tells the world if we want objects to sleep or not. Generally we want objects to sleep as this conserves CPU usage, but there are situations where you might not want your objects to sleep.   It is advised to use the same scale you use for Box2D to draw graphics. This means drawing a Sprite with a width/height in meters. To scale up the graphics to make them visible, you should use a camera with a viewportWidth / viewportHeight also in meters. E.g: drawing a Sprite with a width of 2.0f (2 meters) and using a camera viewportWidth of 20.0f, the Sprite will fill 1/10th of the width on the window.   A common mistake is measuring your world in pixels instead of meters. Box2D objects can only travel so fast. If pixels are used (such as 640 by 480) instead of meters (such as 12 by 9), objects will always move slowly no matter what you do.   Debug Renderer   The next thing we are going to do is setup our debug renderer. You generally will not use this in a released version of your game, but for testing purposes we will set it up now like so:   Box2DDebugRenderer debugRenderer = new Box2DDebugRenderer();   Stepping the simulation   To update our simulation we need to tell our world to step. Stepping basically updates the world objects through time. The best place to call our step function is at the end of our render() loop. In a perfect world everyone’s frame rate is the same   world.step(1/60f, 6, 2);   The first argument is the time-step, or the amount of time you want your world to simulate. In most cases you want this to be a fixed time step. libGDX recommends using a value between 1/60f (which is 1/60th of a second) and 1/240f (1/240th of a second).   The other two arguments are velocityIterations and positionIterations. For now we will leave these at 6 and 2, but you can read more about them in the Box2D documentation.   Stepping your simulation is a topic unto itself. See this article for an excellent discussion on the use of variable time steps.   The result might look similar to this:   private float accumulator = 0;  private void doPhysicsStep(float deltaTime) {     // fixed time step     // max frame time to avoid spiral of death (on slow devices)     float frameTime = Math.min(deltaTime, 0.25f);     accumulator += frameTime;     while (accumulator &gt;= Constants.TIME_STEP) {         WorldManager.world.step(Constants.TIME_STEP, Constants.VELOCITY_ITERATIONS, Constants.POSITION_ITERATIONS);         accumulator -= Constants.TIME_STEP;     } }   Rendering   It is recommended that you render all your graphics before you do your physics step, otherwise it will be out of sync. To do this with our debug renderer we do the following:   debugRenderer.render(world, camera.combined);   The first argument is our Box2D world and the second argument is our libGDX camera.   Objects/Bodies   Now if you run your game it will be pretty boring as nothing happens. The world steps but we don’t see anything as we don’t have anything to interact with it. So now we’re going to add some objects.   In Box2D our objects are called bodies, and each body is made up of one or more fixtures, which have a fixed position and orientation within the body. Our fixtures can be any shape you can imagine or you can combine a variety of different shaped fixtures to make the shape you want.   A fixture has a shape, density, friction and restitution attached to it. Shape is obvious. Density is the mass per square metre: a bowling ball is very dense, yet a balloon isn’t very dense at all as it is mainly filled with air. Friction is the amount of opposing force when the object rubs/slides along something: a block of ice would have a very low friction but a rubber ball would have a high friction. Restitution is how bouncy something is: a rock would have a very low restitution but a basketball would have a fairly high restitution. A body with a restitution of 0 will come to a halt as soon as it hits the ground, whereas a body with a restitution of 1 would bounce to the same height forever.   Bodies come in three different types: dynamic, kinematic and static. Each type is described below.   Dynamic Bodies   Dynamic bodies are objects which move around and are affected by forces and other dynamic, kinematic and static objects. Dynamic bodies are suitable for any object which needs to move and be affected by forces.   We have now learned about fixtures which make up our bodies, so let’s get dirty and start to create some bodies and add fixtures to them!   // First we create a body definition BodyDef bodyDef = new BodyDef(); // We set our body to dynamic, for something like ground which doesn't move we would set it to StaticBody bodyDef.type = BodyType.DynamicBody; // Set our body's starting position in the world bodyDef.position.set(5, 10);  // Create our body in the world using our body definition Body body = world.createBody(bodyDef);  // Create a circle shape and set its radius to 6 CircleShape circle = new CircleShape(); circle.setRadius(6f);  // Create a fixture definition to apply our shape to FixtureDef fixtureDef = new FixtureDef(); fixtureDef.shape = circle; fixtureDef.density = 0.5f; fixtureDef.friction = 0.4f; fixtureDef.restitution = 0.6f; // Make it bounce a little bit  // Create our fixture and attach it to the body Fixture fixture = body.createFixture(fixtureDef);  // Remember to dispose of any shapes after you're done with them! // BodyDef and FixtureDef don't need disposing, but shapes do. circle.dispose();   Now we have created a ball like object and added it to our world. If you run the game now you should see a ball fall down the screen. This is still fairly boring though, as it has nothing to interact with. So let’s create a floor for our ball to bounce on.   Static Bodies   Static bodies are objects which do not move and are not affected by forces. Dynamic bodies are affected by static bodies. Static bodies are perfect for ground, walls, and any object which does not need to move. Static bodies require less computing power.   Let’s go ahead and create our floor as a static body. This is much like creating our dynamic body earlier.   // Create our body definition BodyDef groundBodyDef = new BodyDef();   // Set its world position groundBodyDef.position.set(new Vector2(0, 10));    // Create a body from the definition and add it to the world Body groundBody = world.createBody(groundBodyDef);    // Create a polygon shape PolygonShape groundBox = new PolygonShape();   // Set the polygon shape as a box which is twice the size of our view port and 20 high // (setAsBox takes half-width and half-height as arguments) groundBox.setAsBox(camera.viewportWidth, 10.0f); // Create a fixture from our polygon shape and add it to our ground body   groundBody.createFixture(groundBox, 0.0f); // Clean up after ourselves groundBox.dispose();   See how we created a fixture without the need to define a FixtureDef? If all you need to specify is a shape and a density, the createFixture method has a useful overload for that.   Now if you run the game you should see a ball fall and then bounce on our newly created ground. Play around with some of the different values for the ball like density and restitution and see what happens.   Kinematic Bodies   Kinematic bodies are somewhat in between static and dynamic bodies. Like static bodies, they do not react to forces, but like dynamic bodies, they do have the ability to move. Kinematic bodies are great for things where you, the programmer, want to be in full control of a body’s motion, such as a moving platform in a platform game.   It is possible to set the position on a kinematic body directly, but it’s usually better to set a velocity instead, and letting Box2D take care of position updates.   You can create a kinematic body in much the same way as the dynamic and static bodies above. Once created, you can control the velocity like this:   // Move upwards at a rate of 1 meter per second kinematicBody.setLinearVelocity(0.0f, 1.0f);   Impulses/Forces   Impulses and Forces are used to move a body in addition to gravity and collision.   Forces occur gradually over time to change the velocity of a body. For example, a rocket lifting off would slowly have forces applied as the rocket slowly begins to accelerate.   Impulses on the other hand make immediate changes to the body’s velocity. For example, playing Pac-Man the character always moved at a constant speed and achieved instant velocity upon being moved.   First you will need a Dynamic Body to apply forces/impulses to, see the Dynamic Bodies section above.   Applying Force   Forces are applied in Newtons at a World Point. If the force is not applied to the center of mass, it will generate torque and affect the angular velocity.   // Apply a force of 1 meter per second on the X-axis at pos.x/pos.y of the body slowly moving it right dynamicBody.applyForce(1.0f, 0.0f, pos.x, pos.y, true);  // If we always want to apply force at the center of the body, use the following dynamicBody.applyForceToCenter(1.0f, 0.0f, true);   Applying Impulse   Impulses are just like Forces with the exception that they immediately modify the velocity of a body. As with forces, if the impulse is not applied at the center of a body, it will create torque which modifies angular velocity. Impulses are applied in Newton-seconds or kg-m/s.   // Immediately set the X-velocity to 1 meter per second causing the body to move right quickly dynamicBody.applyLinearImpulse(1.0f, 0, pos.x, pos.y, true);   Keep in mind applying forces or impulses will wake the body. Sometimes this behavior is undesired. For example, you may be applying a steady force and want to allow the body to sleep to improve performance. In this case you can set the wake boolean value to false.   // Apply impulse but don't wake the body dynamicBody.applyLinearImpulse(0.8f, 0, pos.x, pos.y, false);   Player Movement Example   In this example, we will make a player run left or right and accelerate to a maximum velocity, just like Sonic the Hedgehog. For this example we have already created a Dynamic Body named ‘player’. In addition we have defined a MAX_VELOCITY variable so our player won’t accelerate beyond this value. Now it’s just a matter of applying a linear impulse when a key is pressed.   Vector2 vel = this.player.body.getLinearVelocity(); Vector2 pos = this.player.body.getPosition();  // apply left impulse, but only if max velocity is not reached yet if (Gdx.input.isKeyPressed(Keys.A) &amp;&amp; vel.x &gt; -MAX_VELOCITY) {\t\t\t      this.player.body.applyLinearImpulse(-0.80f, 0, pos.x, pos.y, true); }  // apply right impulse, but only if max velocity is not reached yet if (Gdx.input.isKeyPressed(Keys.D) &amp;&amp; vel.x &lt; MAX_VELOCITY) {      this.player.body.applyLinearImpulse(0.80f, 0, pos.x, pos.y, true); }   Joints and Gears   Every joint requires to have definition set up before creating it by box2d world. Using initialize helps with ensuring that all joint parameters are set.   Note that destroying the joint after the body will cause crash. Destroying the body also destroys joints connected to it.   DistanceJointDef defJoint = new DistanceJointDef (); defJoint.length = 0; defJoint.initialize(bodyA, bodyB, new Vector2(0,0), new Vector2(128, 0));  DistanceJoint joint = (DistanceJoint) world.createJoint(defJoints); // Returns subclass Joint.   DistanceJoint   Distance joint makes length between bodies constant.   Distance joint definition requires defining an anchor point on both bodies and the non-zero length of the distance joint.   The definition uses local anchor points so that the initial configuration can violate the constraint slightly. This helps when saving and loading a game.   Do not use a zero or short length!   // DistanceJointDef.initialize (Body bodyA, Body bodyB, Vector2 anchorA, Vector2 anchorB)  DistanceJointDef defJoint = new DistanceJointDef (); defJoint.length = 0; defJoint.initialize(bodyA, bodyB, new Vector2(0,0), new Vector2(128, 0));   FrictionJoint   Friction joint is used for top-down friction. It provides 2D translational friction and angular friction.   FrictionJointDef jointDef = new FrictionJointDef (); jointDef.maxForce = 1f; jointDef.maxTorque = 1f; jointDef.initialize(bodyA, bodyB, anchor);   GearJoint   A gear joint is used to connect two joints together. Either joint can be a revolute or prismatic joint. You specify a gear ratio to bind the motions together: coordinate1 + ratio * coordinate2 = constant The ratio can be negative or positive. If one joint is a revolute joint and the other joint is a prismatic joint, then the ratio will have units of length or units of 1/length.   GearJointDef jointDef = new GearJointDef (); // has no initialize   MotorJoint  A motor joint is used to control the relative motion between two bodies. A typical usage is to control the movement of a dynamic body with respect to the ground.   MotorJointDef jointDef = new MotorJointDef (); jointDef.angularOffset = 0f; jointDef.collideConnected = false; jointDef.correctionFactor = 0f; jointDef.maxForce = 1f; jointDef.maxTorque = 1f; jointDef.initialize(bodyA, bodyB);   MouseJoint  The mouse joint is used in the testbed to manipulate bodies with the mouse. It attempts to drive a point on a body towards the current position of the cursor. There is no restriction on rotation.   MouseJointDef jointDef = new MouseJointDef(); jointDef.target = new Vector2(Gdx.input.getX(), Gdx.input.getY());  MouseJoint joint = (MouseJoint) world.createJoint(jointDef); joint.setTarget(new Vector2(Gdx.input.getX(), Gdx.input.getY()));   PrismaticJoint   A prismatic joint allows for relative translation of two bodies along a specified axis. A prismatic joint prevents relative rotation. Therefore, a prismatic joint has a single degree of freedom.   PrismaticJointDef jointDef = new PrismaticJointDef (); jointDef.lowerTranslation = -5.0f; jointDef.upperTranslation = 2.5f;  jointDef.enableLimit = true; jointDef.enableMotor = true;  jointDef.maxMotorForce = 1.0f; jointDef.motorSpeed = 0.0f;   PulleyJoint   A pulley is used to create an idealized pulley. The pulley connects two bodies to ground and to each other. As one body goes up, the other goes down. The total length of the pulley rope is conserved according to the initial configuration.   JointDef jointDef = new JointDef (); float ratio = 1.0f; jointDef.Initialize(myBody1, myBody2, groundAnchor1, groundAnchor2, anchor1, anchor2, ratio);   RevoluteJoint   A revolute joint forces two bodies to share a common anchor point, often called a hinge point. The revolute joint has a single degree of freedom: the relative rotation of the two bodies. This is called the joint angle   RevoluteJointDef jointDef = new RevoluteJoint(); jointDef.initialize(bodyA, bodyB, new Vector2(0,0), new Vector2(128, 0));  jointDef.lowerAngle = -0.5f * b2_pi; // -90 degrees jointDef.upperAngle = 0.25f * b2_pi; // 45 degrees  jointDef.enableLimit = true; jointDef.enableMotor = true;  jointDef.maxMotorTorque = 10.0f; jointDef.motorSpeed = 0.0f;   RopeJoint   A rope joint enforces a maximum distance between two points on two bodies. It has no other effect. Warning: if you attempt to change the maximum length during the simulation you will get some non-physical behavior. A model that would allow you to dynamically modify the length would have some sponginess, so I chose not to implement it that way. See b2DistanceJoint if you want to dynamically control length.   RopeJointDef jointDef = new RopeJointDef (); // has no initialize   WeldJoint   A weld joint essentially glues two bodies together. A weld joint may distort somewhat because the island constraint solver is approximate.   WeldJointDef jointDef = new WeldJointDef (); jointDef.initialize(bodyA, bodyB, anchor);   WheelJoint   A wheel joint. This joint provides two degrees of freedom: translation along an axis fixed in bodyA and rotation in the plane. You can use a joint limit to restrict the range of motion and a joint motor to drive the rotation or to model rotational friction. This joint is designed for vehicle suspensions.   WheelJointDef jointDef = new WheelJointDef(); jointDef.maxMotorTorque = 1f; jointDef.motorSpeed = 0f; jointDef.dampingRatio = 1f; jointDef.initialize(bodyA, bodyB, anchor, axis); // axis is Vector2(1,1)  WheelJoint joint = (WheelJoint) physics.createJoint(jointDef); joint.setMotorSpeed(1f);   Fixture Shapes   As mentioned previously, a fixture has a shape, density, friction and restitution attached to it. Out of the box you can easily create boxes (as seen in the section Static Bodies section) and circle shapes (as seen in the Dynamic Bodies section).   You can programatically define more complex shapes using the following classes     ChainShape,   EdgeShape,   PolygonShape   However using third party tools you can simply define your shapes and import them into your game.   Importing Complex Shapes using box2d-editor   box2d-editor is a free open source tool to define complex shapes and load them into your game. An example of how to import a shape into your game using box2d-editor is available on Libgdx.info.   Check out the Tools section for more tools.   In a nutshell, if you are using Box2d-editor:     Create your shape within Box2d-editor.   Export your scene and copy the file into your asset folder.   Copy file BodyEditorLoader.java into your “core” module source folder.   Then in your game you can do:   BodyEditorLoader loader = new BodyEditorLoader(Gdx.files.internal(\"box2d_scene.json\"));  BodyDef bd = new BodyDef(); bd.type = BodyDef.BodyType.KinematicBody; body = world.createBody(bd);  // 2. Create a FixtureDef, as usual. FixtureDef fd = new FixtureDef(); fd.density = 1; fd.friction = 0.5f; fd.restitution = 0.3f;  // 3. Create a Body, as usual. loader.attachFixture(body, \"gear\", fd, scale);   Sprites and Bodies   The easiest way to manage a link between your sprites or game objects and Box2D is with Box2D’s User Data. You can set the user data to your game object and then update the object’s position based on the Box2D body.   Setting a body’s user data is easy   body.setUserData(Object);   This can be set to any Java object. It is also good to create your own game actor/object class which allows you to set a reference to its physics body.   Fixtures can also have user data set to them in the same way.   fixture.setUserData(Object);   To update all your actors/sprites you can loop through all the world’s bodies easily in your game/render loop.   // Create an array to be filled with the bodies // (better don't create a new one every time though) Array&lt;Body&gt; bodies = new Array&lt;Body&gt;(); // Now fill the array with all bodies world.getBodies(bodies);  for (Body b : bodies) {     // Get the body's user data - in this example, our user     // data is an instance of the Entity class     Entity e = (Entity) b.getUserData();      if (e != null) {         // Update the entities/sprites position and angle         e.setPosition(b.getPosition().x, b.getPosition().y);         // We need to convert our angle from radians to degrees         e.setRotation(MathUtils.radiansToDegrees * b.getAngle());     } }   Then render your sprites using a libGDX SpriteBatch as usual.   Sensors  Sensors are Bodies that do not produce automatic responses during a collision (such as applying force). This is useful when one needs to be in complete control of what happens when two shapes collide. For example, think of a drone that has some kind of circular distance of sight. This body should follow the drone but shouldn’t have a physical reaction to it, or any other bodies. It should detect when some target is inside it’s shape.   To configure a body to be a sensor, set the ‘isSensor’ flag to true. An example would be:   //At the definition of the Fixture fixtureDef.isSensor = true;   In order to listen to this sensor contact, we need to implement the ContactListener interface methods.   Contact Listeners  The Contact Listeners listen for collisions events on a specific fixture. The methods are passed a Contact object, which contain information about the two bodies involved. The beginContact method is called when the object overlaps another. When the objects are no longer colliding, the endContact method is called.   public class ListenerClass implements ContactListener { \t\t@Override \t\tpublic void endContact(Contact contact) {  \t\t}  \t\t@Override \t\tpublic void beginContact(Contact contact) {  \t\t} \t};   This class needs to be set as the world’s contact listener in the screen’s show() or init() method.   world.setContactListener(ListenerClass);   We might get information about the bodies from the contact fixtures. Depending on the application design, the Entity class should be referenced in the Body or Fixture user data, so we can use it from the Contact and make some changes (e.g. change the player health).   Resources   There are a lot of really good Box2D resources out there and most of the code can be easily converted to libgdx.      A basic implementation and code sample for Box2D with Scene2D is also available on LibGDX.info.   Box2D documentation and Discord are a great place to find help.   A really good tutorial series on Box2D. Covers a lot of different problems which you will more than likely run across in your game development.   Tools   The following is a list of tools for use with box2d and libgdx:   Free Open Source      Physics Body Editor   Code sample available on LibGDX.info   Commercial      RUBE editor for creating box2d worlds. UseRubeLoader for loading RUBE data into libgdx.   PhysicsEditor  ",
        
        "url": "/wiki/extensions/physics/box2d" },{
        "title": "Building the Bullet wrapper",
        "excerpt":
        
        "   Modifying the wrapper   Building the Java module   Compiling the native Bullet libraries   Testing on Android   Testing on Desktop   The Bullet physics library extension is a Java wrapper for the C++ engine. It is generated by the interface compiler called SWIG.   Modifying the wrapper  New or modified functionality can be added to the Bullet wrapper by modifying the SWIG interface files in libgdx/extensions/gdx-bullet/jni/swig. Consult the SWIG documentation for details.   Bullet wrapper custom classes which add extra functionality should be put into libgdx/extensions/gdx-bullet/jni/src/custom. To use the new classes in Java, include them in the SWIG interface files.   Building the Java module  After the SWIG interface files have been modified, build the gdx-bullet Java module.  cd libgdx/extensions/gdx-bullet/jni/swig ant -f build.xml  This will generate Java classes from the Bullet source in libgdx/extensions/gdx-bullet/jni/src, using the SWIG interface files.   If a new C++ source file was added, it is necessary to update the Android.mk build file before compiling the native Bullet code. This is done by running the Java program libgdx/extensions/gdx-bullet/src/com/badlogic/gdx/physics/bullet/BulletBuild.java using the working directory libgdx/extensions/gdx-bullet.   Compiling the native Bullet libraries  After the SWIG module has been built, the native libraries for the different architectures can be compiled.  cd libgdx/extensions/gdx-bullet/jni ant -f build.xml all  In order to check for problems, verbose output can be enabled with the -v flag. To compile for Android, the NDK_HOME environment variable must be set. If Linux is used to compile, it may be necessary to append the options -DndkSuffix=\"\" -Denv.NDK_HOME=\"/opt/android-ndk\".   The resulting native binaries will be placed into libgdx/extensions/gdx-bullet/libs. Note that running ant -f fetch.xml in the libGDX root directory will overwrite them.   Testing on Android  Copy the new native libraries into the corresponding Android test paths  cp libgdx/extensions/gdx-bullet/libs/armeabi-v7a/* libgdx/tests/gdx-tests-android/libs/armeabi-v7a/ cp libgdx/extensions/gdx-bullet/libs/armeabi/* libgdx/tests/gdx-tests-android/libs/armeabi/ cp libgdx/extensions/gdx-bullet/libs/arm64-v8a/*  libgdx/tests/gdx-tests-android/libs/arm64-v8a cp libgdx/extensions/gdx-bullet/libs/x86/*  libgdx/tests/gdx-tests-android/libs/x86 cp libgdx/extensions/gdx-bullet/libs/x86_64/*  libgdx/tests/gdx-tests-android/libs/x86_64  Then build and run the Android test suite.  gradlew tests:gdx-tests-android:installDebug  Testing on Desktop  The new native libraries will be automatically used for the desktop test suite, which you can run using the Gradle wrapper script.  gradlew tests:gdx-tests-lwjgl  ",
        
        "url": "/wiki/misc/building-the-bullet-wrapper" },{
        "title": "Bullet physics",
        "excerpt":
        
        "Bullet is a 3D Collision Detection and Rigid Body Dynamics Library. The Library is Open Source and free for commercial use, under the zlib license (more info).   The Bullet physics extension is a Java wrapper for the C++ engine. This page provides information about using that wrapper. For more documentation related to the Bullet engine, please visit bulletphysics.org. Any Bullet questions not related to libGDX specifically can best be asked on their forum.   Practical information and examples on how to use the bullet wrapper can also be found at the Bullet tests.   These tutorials provide an introduction in using the Bullet wrapper:     Part1: collision detection   Part2: rigid body dynamics   Using Bullet in your project     Setup   Using the wrapper   Using models   Contact callbacks   Custom classes   Debugging  ",
        
        "url": "/wiki/extensions/physics/bullet/bullet-physics" },{
        "title": "Bullet Wrapper Contact callbacks",
        "excerpt":
        
        "Contact callbacks allow you to be notified when a contact/collision on two objects occur (more info and a performance related warning).   By default there are three callbacks: onContactAdded, onContactProcessed and onContactDestroyed) . The wrapper adds two additional callbacks: onContactStarted and onContactEnded (more info). The callbacks are global (independent of e.g. the collision world), there can be only one implementation per callback active at any given time.   Contact Listeners  You can extend the ContactListener class to implement one or more callbacks:  public class MyContactListener extends ContactListener { \t@Override \tpublic void onContactStarted (btCollisionObject colObj0, btCollisionObject colObj1) { \t\t// implementation \t} \t@Override \tpublic void onContactProcessed (int userValue0, int userValue1) { \t\t// implementation \t} }   Note that there can only be one listener enabled for each callback at a time. You can use the enable(); method to set the listener active, which disables any other listeners on that particular callback. Use the disable(); method to stop being notified for that particular callback. Instantiating a listener automatically enables that callback and destroying (dispose(); method) automatically disables it.   The ContactListener class provides one or more method signatures per callback you can override. For example the onContactAdded callback can be overridden using the following signatures:   boolean onContactAdded(btManifoldPoint cp, btCollisionObjectWrapper colObj0Wrap, int partId0, int index0, btCollisionObjectWrapper colObj1Wrap, int partId1, int index1);  boolean onContactAdded(btManifoldPoint cp, btCollisionObject colObj0, int partId0, int index0, btCollisionObject colObj1, int partId1, int index1);  boolean onContactAdded(btManifoldPoint cp, int userValue0, int partId0, int index0, int userValue1, int partId1, int index1);  boolean onContactAdded(btCollisionObjectWrapper colObj0Wrap, int partId0, int index0, btCollisionObjectWrapper colObj1Wrap, int partId1, int index1);  boolean onContactAdded(btCollisionObject colObj0, int partId0, int index0, btCollisionObject colObj1, int partId1, int index1);  boolean onContactAdded(int userValue0, int partId0, int index0, int userValue1, int partId1, int index1);   As you can see it has three methods which provide the btManifoldPoint and three which don’t. To provide the actual collision objects, you can choose between either the btCollisionObjectWrapper, btCollisionObject or the userValue.   Make sure to override the method that only provides the arguments you are actually going to use. For example, if you are not going to use the btManifoldPoint then it wouldn’t make sense to create an object for that argument each time the callback is called. Likewise using btCollisionObject is more performant than using btCollisionObjectWrapper, because the btCollisionObject is reused. The userValue is even more performant, because the object isn’t mapped at all  (see [#btCollisionObject btCollisionObject] on how to use the useValue).   The onContactAdded callback will only be triggered if at least one of the two colliding bodies has the CF_CUSTOM_MATERIAL_CALLBACK set:  body.setCollisionFlags(e.body.getCollisionFlags() | btCollisionObject.CollisionFlags.CF_CUSTOM_MATERIAL_CALLBACK);   To identify a contact along the added, processed and destroyed callbacks, you can use the setUserValue(int); and getUserValue(); of the btManifoldPoint instance that the callback provides. This is also the value supplied to the onContactDestroyed(int) method of the ContactListener class. Note that the onContactDestroyed callback is only triggered if the user value is non-zero.   Contact Filtering   Contact callbacks are invoked a lot. JNI bridging between C++ and Java on every call adds quite an overhead, which decreases performance. Therefor the bullet wrapper allows you to specify for which objects you would like to receive contacts. This is done by contact filtering.   Similar to collision filtering, for every btCollisionObject, you can specify a flag using the setContactCallbackFlag(int); method and a filter using the setContactCallbackFilter(int); method. The filter of object A matches object B if A.filter &amp; B.flag == B.flag. Only if one or both of the filters match the contact is passed to the listener.   static int PLAYER_FLAG = 2; // second bit static int COIN_FLAG = 4; // third bit btCollisionObject player; btCollisionObject coin; ... player.setContactCallbackFlag(PLAYER_FLAG); coin.setContactCallbackFilter(PLAYER_FLAG); // The listener will only be called if a coin collides with player   By default the contactCallbackFlag of a btCollisionObject is set to 1 and the contactCallbackFilter is set to 0. Note that setting the flag to zero will cause the callback always to be invoked for that object (because x &amp; 0 == 0).   Whether or not contact filtering is used, is decided by which method signature you override. For every callback that supports contact filtering the ContactListener class provides method signatures with the boolean match0 and boolean match1 arguments. If you override such method, contact filtering is used on that method. If you override a method that doesn’t have the boolean match arguments, then contact filtering is not used for that method.   You can use the boolean match0 and boolean match1 values to check which of both filters matches.  public class MyContactListener extends ContactListener { \t@Override \tpublic void onContactEnded (int userValue0, boolean match0, int userValue1, boolean match1) { \t\tif (match0) { \t\t\t// collision object 0 (userValue0) matches \t\t} \t\tif (match1) { \t\t\t// collision object 1 (userValue1) matches \t\t} \t} }   Even when using contact filtering, the callbacks can be invoked quite often on collision. To avoid this you can set the filter to zero after processing. For example, in the case of the player and the coin, it’s best to let the coin collide with the player and than set it’s filter to zero on first contact, instead of letting the player collide with the coin.  ",
        
        "url": "/wiki/extensions/physics/bullet/bullet-wrapper-contact-callbacks" },{
        "title": "Bullet Wrapper Custom classes",
        "excerpt":
        
        "In some cases it’s not possible to wrap a C++ bullet class/method in a Java class/method, in which case a custom class or method is used to bridge the two. The following list describes those. Note that the list might not be complete.   btCollisionObject   The btCollisionObject is modified to reuse Java objects instead of creating a new Java object every time. This is done using the static btCollisionObject.instances map. To remove an object from the map and delete the native object use the dispose method.   Besides reusing instances, the Bullet wrappers allows you to provide a unique number to identify the instance. For example the index/ID of the entity in your entity system. Some frequently called methods allow you to use that value instead of the instance itself. This completely eliminates the overhead of mapping C++ and Java instances. You can set this value using the setUserValue(int); method and retrieve the value using the getUserValue(); method.   public class MyGameObject {   public btCollisionObject body; } ... Array&lt;MyGameObject&gt; gameObjects; ... gameObjects.add(myGameObject); myGameObject.body.setUserValue(gameObjects.size-1);   You can use the userData field to add some additional data. For example:  btCollisionObject obj = new btCollisionObject(); obj.userData = myGameObject; ... if (obj.userData instanceof MyGameObject)   myGameObject = (MyGameObject)obj.userData;   btCollisionObject adds the methods takeOwnership and releaseOwnership which can be used to remove or make the wrapper responsable for destroying the native object when the Java object is destroyed by the garbage collector.   The btCollisionObject also adds the following methods:     getAnisotropicFriction(Vector3)   getWorldTransform(Matrix4)   getInterpolationWorldTransform(Matrix4)   getInterpolationLinearVelocity(Vector3)   getInterpolationAngularVelocity(Vector3)   getContactCallbackFlag() and setContactCallbackFlag(int)   getContactCallbackFilter() and setContactCallbackFilter(int)   ClosestNotMeConvexResultCallback  The ClosestNotMeConvexResultCallback class is a custom ClosestConvexResultCallback implementation which you can use to perform a convexSweepTest on all objects except the specified one.   ClosestNotMeRayResultCallback   The ClosestNotMeRayResultCallback class is a custom ClosestRayResultCallback implementation which you can use to perform a rayTest on all objects except the specified one.   InternalTickCallback   The InternalTickCallback is implemented to bridge the callback required by btDynamicsWorld#setInternalTickCallback to a java class. You can extend the class and override onInternalTick method. You can use the attach and detach methods to start and stop getting tick callbacks.   btDefaultMotionState   In some cases it’s easier to use btDefaultMotionState instead of extending btMotionState. The following custom methods are available for btDefaultMotionState.     getGraphicsWorldTrans(Matrix4)   getCenterOfMassOffset(Matrix4)   getStartWorldTrans(Matrix4) Note that extending btMotionState with your own implementation is the preferred method.   btCompoundShape   The btCompoundShape class allows to keep a reference to child shapes, so you don’t have to do that. To use it, use the addChildShape with the third managed argument set to true. Note that this will delete the managed child shape if the compound shape is deleted. Therefor the managed shapes should be exclusive for the compound shape.   btIndexedMesh   The btIndexedMesh class adds the constructor:     btIndexedMesh(Mesh) And the methods:   setTriangleIndexBase(ShortBuffer)   setVertexBase(FloatBuffer)   set(Mesh) For easy constructing or setting a btIndexedMesh based on a Mesh instance or a vertex and index buffer. The buffers itself are not managed by the wrapper and should out-live the object.   btTriangleIndexVertexArray   The btTriangleIndexVertexArray class adds the ability to maintain a reference to the Java btIndexedMesh classes it holds. To use it call addIndexedMesh with the last argument managed set to true. When the btTriangleIndexVertexArray is destroyed it will also destroy it’s managed btIndexedMesh children.   Also, the btTriangleIndexVertexArray class adds the addMesh and addModel methods and likewise constructors, for easy constructing and setting the class.   btBvhTriangleMeshShape   The btBvhTriangleMeshShape class adds the ability to maintain a reference to the Java btStridingMeshInterface class. To use it construct the class with the argument managed set to true. When the btBvhTriangleMeshShape is destroyed it will also destroy the managed btStridingMeshInterface.   Also, the btBvhTriangleMeshShape class add constructors for easy constructing one or more Mesh or Model instances.   btConvexHullShape   The btConvexHullShape class adds a convenience constructor btConvexHullShape(btShapeHull).   btBroadphasePairArray   The btBroadphasePairArray class adds methods to get all collision objects within it at once:  btBroadphasePairArray.getCollisionObjects(Array&lt;btCollisionObject&gt; out, btCollisionObject other, int[] tempArray) btBroadphasePairArray.getCollisionObjectsValue(int[] out, btCollisionObject other)  FilterableVehicleRaycaster   The FilterableVehicleRaycaster class extends btDefaultVehicleRaycaster and adds support for collision filtering using groups and masks:  FilterableVehicleRaycaster raycaster = new FilterableVehicleRaycaster(dynamicsWorld); raycaster.setCollisionFilterGroup(FILTER_GROUP); raycaster.setCollisionFilterMask(FILTER_MASK); btRaycastVehicle vehicle = new btRaycastVehicle(vehicleTuning, chassis, raycaster);  ",
        
        "url": "/wiki/extensions/physics/bullet/bullet-wrapper-custom-classes" },{
        "title": "Bullet Wrapper Debugging",
        "excerpt":
        
        "   Make sure to use the latest nightly (not stable) or to work directly with the latest libGDX code (also make sure to manually update natives in the latter case). The issue you’re having might already be solved.    If you encounter a problem when using the Bullet Wrapper, it can be sometimes difficult to find the cause of the problem. For example, you might see an error like:  # A fatal error has been detected by the Java Runtime Environment: # #  EXCEPTION_ACCESS_VIOLATION (0xc0000005) at pc=0x000000006a49a450, pid=4040, tid=3912 # # JRE version: Java(TM) SE Runtime Environment (8.0_05-b13) (build 1.8.0_05-b13) # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.5-b02 mixed mode windows-amd64 compressed oops) # Problematic frame: # C  [gdx-bullet64.dll+0x19a450] # # Failed to write core dump. Minidumps are not enabled by default on client versions of Windows # # An error report file with more information is saved as: # C:\\Xoppa\\code\\libgdx\\tests\\gdx-tests-lwjgl\\hs_err_pid4040.log # # If you would like to submit a bug report, please visit: #   http://bugreport.sun.com/bugreport/crash.jsp # The crash happened outside the Java Virtual Machine in native code. # See problematic frame for where to report the bug. # AL lib: (EE) alc_cleanup: 1 device not closed  If you look at the log file it generated, then you will see that it contains a bit more information. E.g. the stack trace of the java call. But it will not provide you much useful information about what actually went wrong inside the bullet wrapper. This is because the wrapper delegates the java calls to the native (C++) library. By default this library (e.g. on windows this would be a .dll file) doesn’t contain any debug information.   It is possible to compile the Bullet Wrapper with debug information and even trace into the C++ wrapper and bullet code. For most problems, however, this is not necessary. The cause of most common problems can be found by inspecting the java code.      The most common problem with the Bullet wrapper is caused by not properly maintaining references when you actually still need it. This will cause the garbage collector to destroy the native (C++) object and your application to crash. Because you can’t control the garbage collector, this problem might appear more frequently on different devices. See the creating and destroying objects section for more information.    Debugging on Windows  To debug the native C++ code it is necessary to build the bullet.dll with debug information.   Getting the sources  To do this we will need the source code of the bullet wrapper. So head to the libGDX github repo and either download the whole repository as a .zip file, or clone it via git (git clone https://github.com/libgdx/libgdx.git). Remember that the Java project you’ll be debugging has to use the very same libGDX version for which you’ll be compiling gdx-bullet (preferably bleeding-edge SNAPSHOT one) - otherwise you’ll probably get nasty low-level Java exceptions thrown everywhere due to export mismatches etc.   Getting the compiler/IDE  Furthermore we need an IDE for the debugging as well as the compiler to build the dll. We get both these things with Microsoft Visual Studio Express 2013. An .iso file with the installation can be downloaded here. After downloading it, to install it you will either need to burn the image on a CD or mount it via a virtual CD drive (for example with Daemon Tools). The following steps will work out-of-the-box only on VS Express 2013. For example, to build on VS 2010 SP1, you’ll have to change ToolsVersion=\"12.0\" to ToolsVersion=\"4.0\" &amp; &lt;PlatformToolset&gt;v120&lt;/PlatformToolset&gt; to &lt;PlatformToolset&gt;v100&lt;/PlatformToolset&gt; in all the project files. The code should compile without errors on any reasonably recent build toolset (verified for VS 2010 SP1 as of November 2014), YMMV.   Building the debug .dll  This is really easy as you will find a Visual Studio Project (.sln “Solution”) in the sources which should be able to work out of the box. You can find it in the libGDX repository at libgdx\\extensions\\gdx-bullet\\jni\\vs\\gdxBullet\\gdxBullet.sln.      After opening the solution with Visual Studio there should 6 projects visible. The most important one is gdxBullet. In the toolbar at the top you need to select the correct build configurations. Make sure to select Debug and either Win32 or x64. The platform does not depend on your Windows version, but on the version of the JVM which you are going to use to run your application. On a 64bit Windows system it is still possible (and quite common) to run a 32bit Java version.   Now right-click the gdxBullet project in the solution exporer and hit “Build”. This will build the gdxBullet.dll, which might take a few minutes.      Loading the correct DLL  Usually to load the natives to run bullet it is necessary to call Bullet.init(), which will do exactly that. Since we do not want to load the default natives without debug information included, we have to manually load our newly created gdxBullet.dll ourselves. You can use the following code snippet to do so. Just replace the customDesktopLib string with the actual path. Then call the initBullet() method where you’d normally call the Bullet.init() method (e.g. in the create method).   // Set this to the path of the lib to use it on desktop instead of the default lib. private final static String customDesktopLib = \"C:\\\\......\\\\libgdx\\\\extensions\\\\gdx-bullet\\\\jni\\\\vs\\\\gdxBullet\\\\x64\\\\Debug\\\\gdxBullet.dll\"; private final static boolean debugBullet = true;  static void initBullet() { \t// Need to initialize bullet before using it. \tif (Gdx.app.getType() == ApplicationType.Desktop &amp;&amp; debugBullet) { \t\tSystem.load(customDesktopLib); \t} else { \t\tBullet.init(); \t} \tGdx.app.log(\"Bullet\", \"Version = \" + LinearMath.btGetVersion()); }   Attaching the debugger  Now it’s time to start your app and attach the C++ debugger. First, we need to start the Java app. It is a nice way to set a breakpoint at the startup of your app and then run it in debug mode. That way we’ll have a lot of time to attach the C++ debugger. To do that, we switch to Visual Studio and Select Debug -&gt; Attach to Process....      Then select the correct javaw.exe process of your app in the list of available processes and attach the debugger with the “Attach” button.   Note: with MS VC++ 2010 Express you’ll have to first enable Tools-&gt;Settings-&gt;Expert Settings to get an option to attach to process.   Debugging  For testing if this setup works one might add a breakpoint to btDiscreteDynamicsWorld.stepSimulation. The file containing this code is in gdxBullet -&gt; Source Files -&gt; BulletDynamics -&gt; Dynamics -&gt; btDiscreteDynamicsWorld.cpp.      Search for int btDiscreteDynamicsWorld::stepSimulation( btScalar timeStep,int maxSubSteps, btScalar fixedTimeStep) and add a breakpoint via a doubleclick left of any code within this method.      In case you stopped in a breakpoint on the Java side of the application, it is now time to remove that and resume the process. As soon as your Java code will call btDiscreteDynamicsWorld.stepSimulation you should now be able to switch to Visual Studio and see that the debugger kicked in and you are able to step through the code.  ",
        
        "url": "/wiki/extensions/physics/bullet/bullet-wrapper-debugging" },{
        "title": "Bullet Wrapper Setup",
        "excerpt":
        
        "The Bullet wrapper (gdx-bullet extension) is currently supported on desktop, android and ios. The Bullet wrapper isn’t supported for GWT at the moment.   The easiest method to setup your project to use the Bullet Wrapper is by using the setup utility which has an option to include the gdx-bullet extension.   The instruction for manually adding the Bullet Wrapper to your Gradle project can be found here   If you’re not using Gradle, then you can manually add it:     To use bullet physics in your project, you’ll need to add gdx-bullet.jar to your core project. Alternatively you can add the gdx-bullet project to the projects of the build path of your main project.   For your desktop project you’ll need to add the gdx-bullet-natives.jar to the libraries.   For your android project you’ll need to copy the armeabi/libgdx-bullet.so and armeabi-v7a/libgdx-bullet.so files to the libs folder in your android project.  ",
        
        "url": "/wiki/extensions/physics/bullet/bullet-wrapper-setup" },{
        "title": "Bullet Wrapper Using models",
        "excerpt":
        
        "Using models  Model and ModelInstance are typically used for the visual representation of objects. btCollisionObject or btRigidBody are used for the physical representation of these objects.   Using motion states  To synchronize the location and orientation between a ModelInstance and btRigidBody, Bullet provides the btMotionState class that you can extend. A very basic example of such synchronization is:  static class MyMotionState extends btMotionState {     Matrix4 transform;     @Override     public void getWorldTransform (Matrix4 worldTrans) {         worldTrans.set(transform);     }     @Override     public void setWorldTransform (Matrix4 worldTrans) {         transform.set(worldTrans);     } }  Which you can use as follows:  btRigidBody body; ModelInstance instance; MyMotionState motionState; ... motionState = new MyMotionState(); motionState.transform = instance.transform; body.setMotionState(motionState);  Now the location and orientation of ModelInstance will be updated (by Bullet) whenever the btRigidBody moves. This approach is not restricted to ModelInstance, it will work for any object that contains a Matrix4 transformation, like e.g. also Renderable. Moreover, it is possible to add simple logic to a motion state, for example:  static class PlayerMotionState extends btMotionState {     final static Vector3 position = new Vector3();     Player player;     @Override     public void getWorldTransform (Matrix4 worldTrans) {         worldTrans.set(player.transform);     }     @Override     public void setWorldTransform (Matrix4 worldTrans) {         player.transform.set(worldTrans);         player.transform.getTranslation(position);         if (position.y &lt; 0)             player.die();     } }  Note that the transformation (location and rotation) of a btRigidBody is typically relative to the center of mass (most commonly the center of the shape). When needed, a btCompoundShape can be used to move the center of mass. It is advised to keep the origin of the visual model the same as the origin of the physical object. If this is not possible, then you can modify the transformation in the motion state accordingly.      Keep in mind that Bullet’s transformation only supports translation (location) and rotation (orientation). Any other transformation, like scaling, is not supported.    The motion state has to be disposed when no longer needed: motionState.dispose();.   Create a collision object from a model  A Model boils down to a bunch of triangles with some properties which are rendered with a specific transformation. It is optimized for rendering, not for physics. Therefore a Model is rarely useful for an efficient representation of a physics shape.   To understand why this is, consider a simple box model. The physics shape of a box would contain eight corners. The visual model however, will contain 24 corners (vertices). This is because the vertices are specified for each face of the box, where each vertex contains the “normal” of the face. Otherwise visual effects, like lighting, would not be possible. So, instead of a solid box, the visual model is actually made up of six independent rectangles. Theses rectangles (or the triangles it is made up) are infinitely thin, they have no volume. This makes it unsuitable for dynamic physics.   There are several more issues, e.g. a model typically contains more detail than would be needed for the physics. In fact, for some shapes it is possible to use a much cheaper collision detection algorithm than using the model’s vertices. For example, in case of the box shape, it would be possible to use a single detection against a box, instead of a detection against the 12 triangles it is made of.   There are several ways to work around these problems, ranging from approximating a model using primitive shapes to using a dedicated model or sharing vertices between visual model and physics shape. The Bullet manual provides a decision chart to help you decide which method you should choose:    For the case of a static model, the Bullet wrapper provides a convenient method to create a collision shape of it:  btCollisionShape shape = Bullet.obtainStaticNodeShape(model.nodes);  In this case the collision shape will share the same data (vertices) as the model. This will include node transformation by using a btCompoundShape if needed, but will not include any scaling applied to nodes.  ",
        
        "url": "/wiki/extensions/physics/bullet/bullet-wrapper-using-models" },{
        "title": "Bullet Wrapper Using the wrapper",
        "excerpt":
        
        "Initializing Bullet   Before you can use Bullet, you’ll need to load the libraries. This can be done by adding the following line in your create method:  Bullet.init();   Be aware not to use bullet before it is initialized. For example, the following will result in an error because the btGhostPairCallback is created before the library is loaded.  public class InvokeRuntimeExceptionTest {   final static btGhostPairCallback ghostPairCallback = new btGhostPairCallback(); }   Working with Bullet wrapper  The wrapper tends to follow the original bullet class names. Meaning that most classes are prefixed with “bt”. There are a few exceptions on this, which are mostly nested structs. These are custom implemented directly into the com.badlogic.gdx.physics.bullet package. Unfortunately some nested structs and some base classes are not suitable for a one on one translation. See the custom classes section for more information on that. If you find a class that is missing you can post it on the forums or issue tracker (https://github.com/libgdx/libgdx/issues), so it can be added to the wrapper.   Callbacks   Callbacks require some special attention. By default the wrapper only supports a one way interaction (from Java to C++). Callback interfaces, where C++ needs to call Java code are custom implemented. If you find a callback interface that isn’t implemented yet, you can post it on the forums so it can be added to the wrapper.   List of callback interfaces (might not be complete):     LocalShapeInfo   LocalRayResult   RayResultCallback   ClosestRayResultCallback   AllHitsRayResultCallback   LocalConvexResult   ConvexResultCallback   ClosestConvexResultCallback   ContactResultCallback   btMotionState   btIDebugDraw   InternalTickCallback   ContactListener   ContactCache   Properties  Properties are encapsulated by getter and setter methods. The naming of the getter and setter methods omits the m_ prefix. For example, the m_collisionObject member of the native class btCollisionObjectWrapper is implemented as getCollisionObject() and setCollisionObject(...).   Creating and destroying objects  Every time you create a bullet class in Java it also creates the corresponding class in C++. While the Java object is maintained by the garbage collector, the C++ object isn’t. To avoid having orphaned C++ objects resulting in memory leaks, the C++ object is by default automatically destroyed when the Java object is destroyed by the garbage collector.   While this might be useful in some cases, it’s merely a fail-safe and you shouldn’t rely on it. Since you can’t control the garbage collector, you can’t control if, when and in which order the objects are actually being destroyed. Therefore the wrapper logs an error when an object is automatically destroyed by the garbage collector. You can disable this error logging using the second argument of the Bullet.init() method, but you should preferably use the method in the following paragraph.   In order to ensure correct garbage collection you should keep a reference to every object you create until it’s not needed anymore and then destroy it yourself. You can destroy the C++ object by calling the .dispose() method on the Java object, after which you should remove all references to the Java object since it’s unusable after that.   The above is only true for the objects you are responsible of, which are all Bullet classes you create with the new keyword as well as classes you create using helper methods. You don’t have to dispose objects that are returned by regular methods or provided to you in callback methods.   Referencing objects  As stated above, you should keep a reference to every Bullet class and call the dispose method when it’s no longer needed. When your application becomes more complex and objects are shared amongst multiple other objects, it can become difficult to keep track of references. Therefore the bullet wrapper support reference counting.   Reference counting is disabled by default. To enable it, call Bullet.init(); with the first argument set to true:  Bullet.init(true);   When using reference counting, you must call the obtain() method on each object you need to reference. When you no longer need to reference an object, you must call the release() method. The release method will dispose the object if it’s doesn’t have any more references to it.   Some wrapper classes help you in managing references. For example the btCompoundShape class obtains a reference to all its child shapes and releases them when it is disposed.   Extending classes  You can extend the bullet classes, but it’s recommended not to do so except for callback classes (in which case you should only override the intended methods). The information you add to a class is not available in C++. Furthermore the result of any method of the bullet wrapper that returns a class you’ve overridden will not implement that class. For example:  btCollisionShape shape = collisionObjectA.getCollisionShape();   This will create a new Java btCollisionShape class which doesn’t implement any extended class.   There is one exception to this for btCollisionObject, where the wrapper tries to reuse the same Java class. Furthermore the Java implementation of the btCollisionObject class adds a userData member which can be used to attach additional data to the object. To accomplish this the wrapper maintains an array with references to all btCollisionObject instances. You can access that array using the static field btCollisionObject.instances. Check the btCollisionObject ./Bullet Wrapper: Custom classes#btcollisionobject section for detailed information on this.   The upcast methods are not present because of a issue. There is no need for them for classes that are created in java. These classes can directly be casted.   Comparing classes  You can compare wrapper classes using the equals() method, which checks if the classes both wrap the same native class. To get the pointer to the underlying C++ class you can use the getCPointer method of the specific object. You can also compare these pointers to check whether the Java classes wrap the same C++ class.   Common classes  Bullet uses some classes also available in the libGDX core. While these bullet classes are available for you to use, the wrapper tries to use the libGDX class where possible. Currently these are implemented for:                  Bullet       Libgdx                       btVector3       Vector3                 btQuaternion       Quaternion                 btMatrix3x3       Matrix3                 btTransform       Matrix4                 btScalar       float           Note that the conversion from Matrix4 to btTransform might lose some information, because btTransform only contains an origin and rotation. In addition, note that btScalar is synonym for the primitive type float.   To avoid creating objects for these common classes, the wrapper reuses the same instances. Therefore, be aware of the following two cases:      The result of wrapper methods that return such a class are overwritten by the next method that returns the same type: ```java // Wrong method: Matrix4 transformA = collisionObjectA.getWorldTransform(); // transformA now holds the worldTransform of collisionObjectA Matrix4 transformB = collisionObjectB.getWorldTransform(); // transformA and transformB are the same object and now holds the worldTransform of collsionObjectB   // Correct method: transformA.set(collisionObjectA.getWorldTransform()); transformB.set(collisionObjectB.getWorldTransform());   2. The arguments of interface callbacks with arguments of such a class are unusable after the call: ```java // Wrong method: @Override public void setWorldTransform (final Matrix4 worldTrans) { \ttransform = worldTrans; } // Correct method: @Override public void setWorldTransform (final Matrix4 worldTrans) { \ttransform.set(worldTrans); }   Using arrays  Where possible the wrapper uses direct ByteBuffer objects to pass arrays from Java to C++. This avoids copying the array on the call and allows you to share the same byte buffer for both OpenGL ES and Bullet. If needed you can create a new ByteByffer using BufferUtils.newUnsafeByteBuffer, which you should manually delete using BufferUtils.disposeUnsafeByteBuffer.   In cases where ByteBuffer can’t be used or is unwanted, a normal array is used. By default this means that the array is copied using iteration from Java to C++ at start of the method and copied back at the end of the method. To avoid this overhead the wrapper tries to use the Java array directly from within C++ where possible using critical arrays. During such method Java garbage collecting is blocked. An example of such method is btBroadphasePairArray.getCollisionObjects.  ",
        
        "url": "/wiki/extensions/physics/bullet/bullet-wrapper-using-the-wrapper" },{
        "title": "Bundling a JRE",
        "excerpt":
        
        "Java apps need a Java Runtime Environment to run. Typically this is installed by the user and hopefully already available when they go to run your app. Unfortunately users may not have Java installed and there are differences between JREs that can cause problems with your app. These can be difficult for users to explain and worse, difficult for them to fix themselves. Also, you may require, as a minimum, a certain JRE version.   The solution is to bundle a JRE with your app. This way you know exactly what users will be running and users will have fewer problems and they will not have to install a JVM.   Packaging  There are a number of tools available for bundling a JRE:   jpackage   Jpackage is a modern solution for providing native packaging options on Windows, MacOS and Linux introduced with JEP-343. It can be used to create an EXE that starts your bundled application via an embedded JRE.   See this guide for more information on how to use it. A video version can be found here.   Packr  A packaging tool created and maintained by the libGDX team. Take a look at the repository if you are interested in using it.   Parcl  A Gradle plugin that performs similar actions as launch4j. See its README for instructions.   launch4j  – seems to be no longer maintained –   MacOS Specifics   If you’re planning to deploy to MacOS as well, notarization (MacOS 10.15+) can be an issue. See here on how to notarize your libGDX app.   Reducing Size   There are a number of files and classes that can be removed from the JRE to reduce the size. Below is a list of files to delete from the Windows JRE. Other platforms are very similar, though you may need classes on some platforms but not others (eg, xml classes are needed on Linux to use java.util.preferences). This list leaves Swing intact, if you don’t need Swing the size could be reduced further.   **.diz **.exe except javaw.exe bin\\client\\ lib\\applet\\ lib\\charsets.jar lib\\ext\\localedata.jar lib\\management\\ lib\\management-agent.jar lib\\zi\\ lib\\rt.jar\\com\\sun\\org\\ lib\\rt.jar\\com\\sun\\xml\\ lib\\rt.jar\\com\\sun\\corba\\ lib\\rt.jar\\com\\sun\\media\\ lib\\rt.jar\\com\\sun\\jndi\\ lib\\rt.jar\\com\\sun\\imageio\\ lib\\rt.jar\\com\\sun\\jmx\\ lib\\rt.jar\\com\\sun\\rowset\\ lib\\rt.jar\\com\\sun\\java\\util\\ lib\\rt.jar\\javax\\imageio\\ lib\\rt.jar\\javax\\management\\ lib\\rt.jar\\javax\\print\\ lib\\rt.jar\\javax\\naming\\ lib\\rt.jar\\javax\\sound\\ lib\\rt.jar\\javax\\sql\\ lib\\rt.jar\\javax\\xml\\ lib\\rt.jar\\javax\\swing\\plaf\\nimbus\\ lib\\rt.jar\\javax\\swing\\text\\html\\ lib\\rt.jar\\org\\ lib\\rt.jar\\sun\\applet\\ lib\\rt.jar\\sun\\management\\ lib\\rt.jar\\sun\\rmi\\ lib\\rt.jar\\sun\\security\\jgss\\ lib\\rt.jar\\sun\\security\\krb5\\ lib\\rt.jar\\sun\\security\\tools\\ lib\\resources.jar\\com\\sun\\corba\\ lib\\resources.jar\\com\\sun\\imageio\\ lib\\resources.jar\\com\\sun\\jndi\\ lib\\resources.jar\\com\\sun\\org\\ lib\\resources.jar\\com\\sun\\rowset\\ lib\\resources.jar\\com\\sun\\servicetag\\ lib\\resources.jar\\com\\sun\\xml\\ lib\\jsse.jar\\sun\\security\\ssl\\   To make this list I went through the files and JARs sorting by largest size first. I then deleted the largest files that looked like that were not needed and ran my app to make sure everything still works.   This list reduces the JRE size to about 36MB. Note that for faster start up the JRE JARs are not compressed. After zipping the entire JRE, the size is reduced to about 13.5MB. If Swing packages are also removed from rt.jar, the zipped size goes down to about 9.8MB.  ",
        
        "url": "/wiki/deployment/bundling-a-jre" },{
        "title": "Circles, planes, rays, etc.",
        "excerpt":
        
        "Introduction   libGDX has several geometric classes for dealing with shapes, areas, and volumes. These include:      Circle   Frustum (sometimes written frustrum)   Plane   Spline   Polygon   Ray   Rectangle   A full explanation of these concepts is beyond the current scope, but the above links may provide a starting point for further understanding. What follows is an overview of their use and implementation in Libgdx.     Bounding Boxes (code)   An axis-aligned bounding box useful for simple volume intersection tests. It is defined by a minimum and maximum point describing the rectangular extents of its volume. A point can be tested for containment within the box and the box can be easily resized to contain a given point. Intersection can also be tested against a Frustum (code) or with a Ray (code) using the Intersector (code) class.     Circles (code)   A simple class which describes a circle with a center point and a radius. Provides the ability to test a point for containment within the circle. Intersection with another circle, a line segment, or a Rectangle (code) can be tested for using the Intersector (code) class.     Frustum (code)   A frustum is a four-sided pyramid with the top cut off. It can be used to describe the volume visible to a rectangular view into a space with a perspective projection such as the case with a 3D scene projected onto a 2D monitor.   Frustums can be useful for simplifying a scene by culling objects which do not intersect its volume and are therefore outside of the user’s view. Simply pass the combined view-projection matrix of your camera to a frustum instance to create a frustum describing the viewing volume of the screen. libGDX then provides methods for testing points and simple collision volumes such as bounding boxes (code) and Spheres (code) against the volume of the frustum to determine visibility of scene objects. This is known as “frustum culling,” a very common scene optimization technique.     Planes (code)   A plane is an infinite two-dimensional surface in three-dimensional space described by a point on that surface and a surface normal. Planes can be useful for partitioning spaces and determining in which sector an object resides. Intersection with a ray (code) or line segment can be tested for using the Intersector (code) class.     Polygons(code)   A simple class defining a two-dimensional polygon from a list of points. It can be easily translated, rotated, and scaled. It also provides the ability to test a point for containment within the polygon. Polygon to polygon intersection can be tested by using the Intersector (code) class, assuming the polygons are convex.     Rays (code)   A ray is a line segment infinite in one direction. It is defined by an origin and a unit-length direction. Rays can be tested for intersection with bounding boxes (code), planes (code), spheres (code), and triangles by using the Intersector (code) class.   Rays are particularly useful in picking operations. Cameras can provide a ray describing the current mouse or touch point in a window projected out into the scene from the point of view of the camera.     Rectangles (code)   A simple class which describes a two-dimensional axis-aligned rectangle described by its bottom-left corner point and a width and height. Provides the ability to test a point for containment within the rectangle. Rectangles can also be tested for intersection with other rectangles as well as with circles (code) by using the Intersector (code) class.     Segments (code)   A simple class which describes a line segment in three-dimensional space defined by its two end points. Line segments can be tested for intersection with other line segments, circles (code), and planes(code) by using the Intersector (code) class. This class is merely for convenience in grouping end points as the above tests accept endpoints directly for parameters.     Spheres (code)   A simple class which describes a three-dimensional sphere defined by a center point and a radius. Can be tested for intersection with Frustum (code) as well as rays (code) by using the Intersector (code) class.  ",
        
        "url": "/wiki/math-utils/circles-planes-rays-etc" },{
        "title": "Clearing the screen",
        "excerpt":
        
        "To clear the screen in libGDX is not unlike clearing the screen in a regular OpenGL application. The only difference is in how one accesses the OpenGL context.   The following example accesses the context in an OpenGL ES2 application to clear the frame and depth buffers, setting the color buffer to a solid red color:   @Override public void render() {    Gdx.gl.glClearColor( 1, 0, 0, 1 );   Gdx.gl.glClear( GL20.GL_COLOR_BUFFER_BIT | GL20.GL_DEPTH_BUFFER_BIT );    // scene render code... }   Simply set the desired clear color and then call glClear() with the desired buffers to clear. You are then free to render a fresh frame with new scene graphics.  ",
        
        "url": "/wiki/graphics/clearing-the-screen" },{
        "title": "Clipping, with the use of scissorstack",
        "excerpt":
        
        "Clipping   Rectangle scissors = new Rectangle(); Rectangle clipBounds = new Rectangle(x,y,w,h); ScissorStack.calculateScissors(camera, spriteBatch.getTransformMatrix(), clipBounds, scissors); if (ScissorStack.pushScissors(scissors)) {     spriteBatch.draw(...);     spriteBatch.flush();     ScissorStack.popScissors(); }   This will limit rendering to within the bounds of the rectangle “clipBounds”. The actual drawing is encapsulated by the if-statement because the program would otherwise crash in some situations where the scissor couldn’t be pushed to the stack (happens for example when the window is minimized on desktop, it’s ok to not draw in this case though). You may also need to flush or end the spriteBatch before starting the active scissor region (that is, before calling ScissorStack.pushScissors) to prevent queued draw calls from before the scissor start getting flushed inside the active scissor region.   It is also possible to push multiple rectangles. Only the pixels of the sprites that are within all of the rectangles will be rendered.  ",
        
        "url": "/wiki/graphics/2d/clipping-with-the-use-of-scissorstack" },{
        "title": "Collections",
        "excerpt":
        
        "Lists   Array (code)   A resizable, ordered or unordered array of objects. It often replaces ArrayList. It provides direct access to the backing array, which can be of a specific type rather than just Object[]. It can also be unordered, acting like a bag/multiset. In this case, a memory copy is avoided when removing elements (the last element is moved to the removed element’s position).   The iterator returned by iterator() is always the same instance, allowing the Array to be used with the enhanced for-each (for( : )) syntax without creating garbage. Note however that this differs from most iterable collections! It cannot be used in nested loops, else it will cause hard to find bugs.   Primitive lists      IntArray (code)   FloatArray (code)   BooleanArray (code)   CharArray (code)   LongArray (code)   ByteArray (code)   These are identical to Array except they use primitive types instead of objects. This avoids boxing and unboxing.   Specialized lists   SnapshotArray (code)   This is identical to Array except it guarantees that array entries provided by begin(), between indexes 0 and the size at the time begin was called, will not be modified until end() is called. This can be used to avoid concurrent modification. It requires iteration to be done a specific way:   SnapshotArray array = new SnapshotArray(); // ... Object[] items = array.begin(); for (int i = 0, n = array.size; i &lt; n; i++) { \tObject item = items[i]; \t// ... } array.end();   If any code inside begin() and end() would modify the SnapshotArray, the internal backing array is copied so that the array being iterated over is not modified. The extra backing array created when this occurs is kept so it can be reused if a concurrent modification occurs again in the future.   DelayedRemovalArray (code)   This is identical to Array except any removals done after begin() is called are queued and only occur once end() is called. This can be used to avoid concurrent modification. Note that code using this type of list must be aware that removed items are not actually removed immediately. Because of this, often SnapshotArray is easier to use.   PooledLinkedList (code)   A simple linked list that pools its nodes.   SortedIntList (code)   A sorted double linked list which uses ints for indexing.   Maps   ObjectMap (code)   An unordered map. This implementation is a cuckoo hash map using three hashes, random walking, and a small stash for problematic keys. Null keys are not allowed, null values are. No allocation is done except when growing the table size.   Keys may only be in one of three locations in the backing array, allowing this map to perform very fast get, containsKey, and remove. Put can be a bit slower, depending on hash collisions. Load factors greater than 0.91 greatly increase the chances the map will have to rehash to the next higher POT backing array size.   Primitive maps      IntMap (code)   LongMap (code)   ObjectIntMap (code)   ObjectFloatMap (code)   These maps are identical to ObjectMap except they use primitive types for the keys, except ObjectIntMapand ObjectFloatMap which use primitive values. This avoids boxing and unboxing.   Specialized maps   OrderedMap (code)   This map is identical to ObjectMap except keys are also stored in an Array. This adds overhead to put and remove but provides ordered iteration. The key Array can be sorted directly to change the iteration order.   IdentityMap (code)   This map is identical to ObjectMap except keys are compared using identity comparison (== instead of .equals()).   ArrayMap (code)   This map uses arrays for both the keys and values. This means get does a comparison for each key in the map, but provides fast, ordered iteration and entries can be looked up by index.   Sets   ObjectSet (code)   Exactly like ObjectMap, except only keys are stored. No values are stored for each key.   Primitive sets      IntSet (code)   These maps are identical to ObjectSet except they use primitive types for the keys. This avoids boxing and unboxing.   Other collections   BinaryHeap (code)   A binary heap. Can be a min-heap or a max-heap.   Benchmarks  The benchmark below shows the difference between array and hashtable lookup (.contains() or .get()) using libGDX collection methods. If you have less than 1024 elements in a list, you shouldn’t bother whether you’re using arrays or hashtables (Maps or Sets). Mind the fact that hashtables have significantly slower iteration than arrays and cannot be ordered.                     array.size         GdxArray.contains()     GdxObjectSet.contains()                            2                         0ms                         0ms                            4                         0ms                         0ms                            8                         0ms                         1ms                           16                         0ms                         1ms                           32                         0ms                         0ms                           64                         0ms                         0ms                          128                         0ms                         0ms                          256                         1ms                         0ms                          512                         1ms                         0ms                         1024                         2ms                         0ms                         2048                         4ms                         0ms                         4096                        11ms                         0ms                         8192                        22ms                         0ms                        16384                        80ms                         0ms                        32768                       112ms                         0ms                        65536                       403ms                         0ms                       131072                       615ms                         1ms  ",
        
        "url": "/wiki/utils/collections" },{
        "title": "Color Markup Language",
        "excerpt":
        
        "The BitmapFontCache class supports in-string colored text through a simple markup language.   Markup is disabled by default. Use the public member font.getData().markupEnabled to turn it on/off.   The markup syntax is really simple but still versatile:     [name] Sets the color by name. There are a few predefined colors, see the Colors.reset() method for an exhaustive list. Users can define their own colors through the methods of the Colors class.   [#xxxxxxxx] Sets the color specified by the hex value xxxxxxxx in the form RRGGBBAA where AA is optional and defaults to 0xFF.   [] Sets the color to the previous color (kind of optional end tag)   [[ Escapes the left bracket.   Notice that color names are case-sensitive, cannot be empty, cannot start with neither # nor [, and cannot contain ]. Also, any occurrence of [ in the color name must not be escaped.   Unknown colors, illegal hex codes and unclosed tags are silently ignored and treated as normal text.   For a sample code see the test class BitmapFontTest.   Note: When using with Scene2D you have to remove the fontColor property from LabelStyle definition in the skin.json file to have markup coloring work with a Label.  ",
        
        "url": "/wiki/graphics/2d/fonts/color-markup-language" },{
        "title": "Compass",
        "excerpt":
        
        "Some Android devices and iOS devices have an integrated magnetic field sensor that provides information on how the device is oriented with respect to the magnetic north pole.   NOTE: The compass is currently not available on iOS devices since there is no implementation in the RoboVM - backend yet. The compass appears to be present with the Intel MOE backend on iOS.   Querying whether the compass is available works as follows:   boolean compassAvail = Gdx.input.isPeripheralAvailable(Peripheral.Compass);   Once you determined that the compass is indeed available, you can poll its state:   float azimuth = Gdx.input.getAzimuth(); float pitch = Gdx.input.getPitch(); float roll = Gdx.input.getRoll();   The angles are given in degrees. Here’s the interpretation of these values:      The azimuth is the angle of the device’s orientation around the z-axis. The positive z-axis points towards the earths center.   The pitch is the angle of the device’s orientation around the x-axis. The positive x-axis roughly points to the west and is orthogonal to the z- and y-axis.   The roll is the angle of the device’s orientation around the y-axis. The positive y-axis points toward the magnetic north pole of the earth while remaining orthogonal to the other two axes.   Here’s an illustration of the axis relative to the earth.                     Prev       Next          ",
        
        "url": "/wiki/input/compass" },{
        "title": "Configuration and Querying",
        "excerpt":
        
        "Sometimes it is necessary to know which input devices are supported. It is also often the case that your game does not need the full range of input devices supported, e.g. you might not need the accelerometer or compass. It is good practice to disable those input devices in that case to preserve battery on Android. The following sections will show you how to perform these actions.   Disabling Accelerometer &amp; Compass (Android, iOS and Html)  The AndroidApplicationConfiguration class has a couple of public fields you can set before you hand it of to the AndroidApplication.initialize() method.   Assuming our game doesn’t need the accelerometer and compass, we can disable this input devices as follows:   public class MyGameActivity extends AndroidApplication {    @Override    public void onCreate (Bundle savedInstanceState) {       super.onCreate(savedInstanceState);       AndroidApplicationConfiguration config = new AndroidApplicationConfiguration();       config.useAccelerometer = false;       config.useCompass = false;       initialize(new MyGame(), config);    } }   Both the accelerometer and the compass are enabled by default. The above code disables them and will thus preserve some precious battery.   Enabling Gyroscope (Android and Html)  The gyroscope is disabled by default to preserve battery, you can enable it as follows:   public class MyGameActivity extends AndroidApplication {    @Override    public void onCreate (Bundle savedInstanceState) {       super.onCreate(savedInstanceState);       AndroidApplicationConfiguration config = new AndroidApplicationConfiguration();       config.useGyroscope = true;       initialize(new MyGame(), config);    } }   Querying Available Input Devices  To check whether a specific input device is available on the platform the application currently runs, you can use the Input.isPeripheralAvailable() method.      boolean hardwareKeyboard = Gdx.input.isPeripheralAvailable(Peripheral.HardwareKeyboard);    boolean multiTouch = Gdx.input.isPeripheralAvailable(Peripheral.MultitouchScreen);   Please refer to the Peripheral enumeration to see the rest of the available constants.   Note that only a few Android devices have a hardware keyboard. Even if the keyboard is physically present, the user might not have slid it out. The method shown above will return false in this case.   Next  ",
        
        "url": "/wiki/input/configuration-and-querying" },{
        "title": "Console support?",
        "excerpt":
        
        "Console support for libGDX is a hotly debated topic. This page tries to give a broad overview of how you might go about getting your game to work on your favourite consoles, including Xbox, PlayStation and Nintendo Switch.   The General Idea  To get your libGDX game running on consoles, you need to port your code base to a different language. While (in theory) this can be done manually, a considerably less time-consuming (but technologically complex) way of porting your game is to compile/transpile it to code that can run on your targeted platform. Inspiration can be drawn from libGDX’s Web (see GWT) and iOS (see RoboVM) backends, which already do this.   In addition, you need a custom libGDX backend with bindings for the device in question.   Successful Examples  There are a couple of games, which have successfully done this in the past:     Slay the Spire, which was ported by Sickhead Games first to C# and then to C++[1]   Pathway, which uses a custom fork of RoboVM and a SDL backend[2]   Orangepixel’s games (check out his devlogs for more information)   Different Approaches &amp; Other Resources     TheLogicMaster has a working PoC of a Nintendo Switch homebrew backend over at SwitchGDX, which uses a custom fork of CodenameOne’s ParparVM   IKVM (which was used by libGDX in the past) could prove as another viable option; see here for IKVM’s current main fork   Anuken’s Ark has a SDL backend here; SDL has a working port for Nintendo Switch   GWT/TeaVM could be used to transpile code to Javascript, which could then be used in a UWP app; see here and here for some thoughts on this approach   A similar goal is being pursued by the mini2Dx project.  ",
        
        "url": "/wiki/articles/console-support" },{
        "title": "Continuous and Non Continuous Rendering",
        "excerpt":
        
        "By default in libGDX, the rendering thread calls the render() method of your ApplicationListener class continuously, with a frequency that depends on your hardware (30-50-80 times per second).   If you have many still frames in your game (think about a card game) you can save precious battery power by disabling the continuous rendering, and calling it only when you really need it.   All you need is put the following lines in your ApplicationListener’s create() method   Gdx.graphics.setContinuousRendering(false); Gdx.graphics.requestRendering();   The first line tells the game to stop calling the render() method automatically. The second line triggers the render() method once. You have to use the second line wherever you want the render() method to be called.   If continuous rendering is set to false, the render() method will be called only when the following things happen.      An input event is triggered   Gdx.graphics.requestRendering() is called   Gdx.app.postRunnable() is called   UI Actions: Many Actions, such as the default fade-in and fade-out of dialogs, have a duration in which they need rendering to occur, so they will call Gdx.graphics.requestRendering() on your behalf. This is enabled by default. To disable it, you can call:   Stage.setActionsRequestRendering(false);    Good article about this topic: https://bitiotic.com/blog/2012/10/01/enabling-non-continuous-rendering-in-libgdx/   Official libGDX blog post: https://web.archive.org/web/20201028180041/https://www.badlogicgames.com/wordpress/?p=2289  ",
        
        "url": "/wiki/graphics/continuous-and-non-continuous-rendering" },{
        "title": "Controllers",
        "excerpt":
        
        "Controllers extension has been moved to an own repo.   See the Manual wiki page how to use.  ",
        
        "url": "/wiki/input/controllers" },{
        "title": "Coordinate systems",
        "excerpt":
        
        "When working with libGDX (or any other OpenGL based system), you will have to deal with various coordinate systems. This is because OpenGL abstracts away device dependent units, making it more convenient to target multiple devices and to focus on game logic. Sometimes you may need to convert between coordinate systems for which libGDX offers various methods.   It is crucial to understand in which coordinate system you are working. Otherwise it can be easy to get confused and to make assumptions which aren’t correct.   On this page the various coordinate systems are listed. It is highly recommended to first get familiar with the Cartesian coordinate system, which is the most widely used coordinate system.      Touch coordinates   Screen or image coordinates   Pixmap and texture coordinates   Normalized render coordinates   Normalized texture (UV) coordinates   World coordinates   GUI/HUD coordinates   Game coordinates   Touch coordinates     Units: pixels   System: y-down   Type: integer, can’t be fractional   Range: (0,0) (upper left corner) to (Gdx.graphics.getWidth()-1, Gdx.graphics.getHeight()-1) (lower right corner)   Usage: touch/mouse coordinates   Dependence: device specific   Starts at the upper left pixel of the (application portion of the) physical screen and has the size of the (application portion of the) physical screens width and height in pixels.      Each coordinate is an index in the 2D array of this grid, representing a physical pixel on the screen. Therefore these coordinates are always represented as integers, they can’t be fractional.   This coordinate system is based on the classic representation of the display, which is usually also closest to the device/OS specific implementation. If you’re familiar with canvas graphics or basic image editors, then you are probably already familiar with these coordinates. You might even lean towards using these as your default/favorite, which you shouldn’t.   Whenever working with mouse or touch coordinates, you’ll be using this coordinate system. You typically want to convert these coordinates as soon as possible to a more convenient coordinate system. E.g. the camera.unproject or viewport.unproject method let’s you convert them to world coordinates (see below).   Screen or image coordinates     Units: pixels   System: y-up   Type: integer, can’t be fractional   Range: (0,0) (lower left corner) to (Gdx.graphics.getWidth()-1, Gdx.graphics.getHeight()-1) (upper right corner)   Usage: viewport, scissors and pixmap   Dependence: device/resource/asset specific   This is OpenGL’s counterpart to touch coordinates; that is: it is used to specify (index) a pixel of the (portion of the) physical screen. It is also used as indexer for an image in memory. Likewise, these are integers, they can’t be fractional.   The only difference between touch and screen coordinates is that touch coordinates are y-down, while screen coordinates are y-up. Converting between them is therefore quite easy:   y = Gdx.graphics.getHeight() - 1 - y;   You typically use these coordinates to specify which portion of the screen to render onto. For example when calling glViewport, glScissor or manipulating a pixmap (see next). In the majority of use-cases you don’t need this coordinate system a lot, if any, and it should be isolated from your game logic and its coordinate system. The camera.project and viewport.project methods can be used to convert world units to screen coordinates.   Pixmap and texture coordinates   Pixmap coordinates are an exception. Pixmaps are commonly used to upload texture data. For example when loading a PNG image file to a texture, it is first decoded (uncompressed) to a Pixmap, which is the raw pixel data of the image, then it is copied to the GPU for use as texture. The texture can then be used to render to the screen. It is also possible to modify or create a pixmap by code, e.g. before uploading as texture data.   The “problem” with this is that OpenGL expects the texture data to be in image coordinates, which is y-up. However, most image formats store the image data comparable to touch coordinates, which is y-down. libGDX does not translate the image data between the two (which would involve copying the image line by line), instead it simply copies the data as is. This practically causes a Texture loaded from Pixmap to be up-side-down.   To compensate for this up-side-down texture, SpriteBatch flips the texture (UV) coordinates (see below) on the y axis when rendering. Likewise, fbx-conv has the option to flip texture coordinates on the y axis as well. However, when you use a texture which isn’t loaded from a pixmap, for example a Framebuffer, then this might cause that texture to appear up-side-down.   Normalized render coordinates     Units: one   System: y-up   Type: floating point   Range: (-1,-1) (lower left corner) to (+1,+1) (upper right corner)   Usage: shaders   Dependence: none   The above coordinate systems have one big issue in common: they are device specific. To solve that, OpenGL allows you to use a device independent coordinate system which is automatically mapped to screen coordinates when rendering. This coordinate system is normalized in the range [-1,-1] and [+1,+1] with (0,0) exactly in the center of the screen or framebuffer (the render target).      The vertex shader outputs (gl_Position) its coordinates in this coordinate system. But other than that, you should never have to use this coordinate system in a practical use-case. It is sometimes used in tutorials and such, though, to show the basics.   It might be worth to note that the normalization does not respect the aspect ratio. That is: the scale in the X direction does not have to match the scale in the Y direction. They are both within the range of -1 to +1, regardless aspect ratio. It is up to the application to decide how to deal with various aspect ratios (see world units, below).   The coordinates are floating point and no longer indexers. The device (GPU) will map these coordinates to the actual screen pixels using rasterisation. A good article (although targeting DirectX it also applies to OpenGL) for more information on that can be found here.   Normalized texture (UV) coordinates     Units: one   System: y-up   Type: floating point   Range: (0,0) (lower left corner) to (1,1) (upper right corner)   Usage: shaders, mesh, texture region, sprite   Dependence: none   Likewise to the normalized render coordinates, OpenGL also uses normalized texture coordinates. The only difference is that these ranges from [0,0] to [1,1]. Depending on the specified wrap function, values outside that range will be mapped within that range.      These coordinates are also called UV coordinates. In many use cases you don’t have to deal with them. Typically these values are stored in the mesh or TextureRegion.   The use of normalized texture coordinates is very important, because it makes them independent of the asset size. Or in other words: it allows you to replace your assets with a scaled down or scaled up version, without having to modify the UV coordinates. An example where this is used are mipmaps.   When rendering, the GPU converts the UV coordinates to a texel (texture pixel). This is called “texture sampling” and is based on the texture filtering.   World coordinates     Units: application specific, e.g. SI Units   System: application specific, but usually y-up   Type: floating point   Range: application specific   Usage: game logic   Dependence: game/application   Typically, your game logic should use a coordinate system which best fits the game logic. It should not depend on device or asset size. For example, a commonly used unit is meters.   The world coordinates are converted, in the vertex shader, to normalized render coordinates. The Camera or Viewport is used to define the strategy on how to do that. For example, to maintain aspect ratio, black bars can be added. The camera is used to calculate the view matrix, which translates your world coordinates into coordinates relative to the camera, by taking in consideration the location and rotation of the camera. It also calculates the projection matrix, which converts the world coordinates to the normalized render coordinates in the range [-1,-1] to [+1,+1]. In 2D games, you mostly dont need the distinction between these two matrices and only need the combined transformation matrix instead. You can pass this matrix to the shader, for example, by calling spriteBatch.setProjectionMatrix(camera.combined);      You can have multiple camera’s or viewports and likewise, you can also have multiple world coordinate systems. A typical game has at least two of those, namely:   GUI/HUD coordinates  These are buttons, labels and such which are stationary and always visible on the screen. Often they involve rendering text. For example in Super Mario the clock is always visible in the upper right corner of the screen and does not move when mario moves in the game world.   Most commonly scene2d is used for the HUD, which means that you’d use a Viewport to define the coordinate system. This coordinates system is typically in a range that is close to the device resolution, to give the best results when rendering the font. These coordinates are called banana units. This camera is practically never moved or rotated, it sits stationary at a location so that the world coordinate (0, 0) is located at the bottom left corner of the screen.   Game coordinates  This is what suits best for your game and is used to implement game logic. It is good practice to keep the values around one for the best floating point precision. For example, your main character is 1.8 meter in height, the tree is 10 meter in height, etc. If you are making a galactic game where units and distance are very high, you might want to use e.g. kilometers instead.   The camera is used to look into your game world, just like it would when you use a video camera in the real world. The camera can be moved, rotated and scaled to display another portion of the world on the screen.  ",
        
        "url": "/wiki/articles/coordinate-systems" },{
        "title": "Creating a Separate Assets Project in Eclipse",
        "excerpt":
        
        "This will walk you through step by step on how to setup a separate project to manage your assets in Eclipse, and is a great solution if you like to keep your assets separate from your code projects. Doing so will allow you to manage them in source control differently or not at all. Additionally, you can have a nice clean separate builder for your assets project should your game be involved enough.*   The only downside I’ve found is that unless you modify each project’s builder to check for updates to your Assets Project, you will have to refresh (F5) the project before clicking Run, as Eclipse won’t auto-detect changes to most files in the separate Assets Project. No big deal.   Note to Gradle Builders: This may not work for your build unless you modify the gradle script to pull in the assets.   Lets assume that you have a project named MyGame that was created using the setup tool and the project structures are how they usually are in a bare bones Eclipse scenario.      In Eclipse, create a new General Project. (File &gt; New &gt; Project…)   Name this project MyGame-Assets or something useful   Go to the MyGame-Assets project, and add a new Folder named “assets”   Add a new subfolder of assets named “data”   In the data folder, I recommend adding subfolders for graphics, sounds etc. Whatever you need.   Locate and expand the MyGame-Android project created by the setup tool   Locate the “assets” folder and back up any assets someplace else temporarily or move them to MyGame-Assets/assets/data. I recommend right clicking and using Export… if you have the project under source control.   Delete the “assets” folder in your MyGame-Android project   At the top level of the MyGame-Android project, right click and select New &gt; Folder   At the bottom of the “New Folder” dialog, click the “Advanced »” button   Select the “Link to Alternate Location” radio button   Copy and paste, without quotes: “WORKSPACE_LOC” into the blank folder path   Now finish typing up the path to the newly created assets folder. Ex: /MyGame-Assets/assets   Before clicking “OK”, ensure the “Folder Name” in the middle of the dialog is “assets”   Double check the full path and click OK.   Go to the newly linked assets folder and expand. It should have “data” and all of the subfolders displayed. If not, you made a boo boo, so go delete it and then step back and follow the instructions more carefully.   Right click the MyGame-Android project and select Properties   Go to the “Java Build Path”, and select the “Source” tab   Click the “Add Folder…” button on the right side of the dialog   In the tree that comes up, your new “assets” subfolder should be displayed. Check that bad boy and click “OK”.   Select the “Order and Export” tab. Ensure that the new assets folder is checked.   Click “OK” on the Properties for… window. You’ll notice now that the icons for the assets subfolders should have changed from the usual folder icon to the Java package icon.   Are you assets in the MyGame-Assets project in the correct folder structure?   Click your MyGame-Android project, and press F5. Or clean. Whatever.   Run the MyGame-Android project and verify that it builds and your game art is present.   If you setup a Desktop or iOS project, repeat the steps to modify each project similarly to use a linked folder named “assets”.   Bask in the glow of your new organizational bliss and wonder if the setup tool will do this for you at some point in the future.   My MyGame-Assets projects usually look like the following tree, with organized subfolders in each, depending on what suites the particular game best. If you setup a builder, or have artwork per device… No problem. Organize your MyGame-Assets project and recreate the linked folder.   /assets (where the optimized game binaries are actually stored)      /data           /graphics           /maps           /screens           /shaders           /sounds /assets-workfiles (where source/unoptimized files go, with a matching structure to assets)   * I wouldn’t recommend setting up a builder for just your assets until it makes sense, which is usually at the later stages of a game’s development cycle. Keep it simple and real until you’re closing in on production ready, determine if you absolutely need it, isolate what works best, and only THEN isolate how to optimize the packaging of your assets. At that point, you’ll be able to measure the effects and maximize your effort to results ratio.  ",
        
        "url": "/wiki/articles/creating-a-separate-assets-project-in-eclipse" },{
        "title": "Cursor Visibility and Catching",
        "excerpt":
        
        "Cursor catching   For some games like first-person shooters, it is often necessary to catch the cursor so it stays in the center of the screen, and only use position deltas to rotate the camera. Other times we might want to position the cursor manually. Both things can be done as follows:   Gdx.input.setCursorCatched(true); Gdx.input.setCursorPosition(x, y);   Cursor catching is only available on the desktop and GWT backends, and positioning is only available on the desktop backends.   Custom cursor   Changing the cursor to a custom image can be done like so. The following example turns this 32×32 image into a cursor:    Pixmap pixmap = new Pixmap(Gdx.files.internal(\"badcursor.png\")); // Set hotspot to the middle of it (0,0 would be the top-left corner) int xHotspot = 15, yHotspot = 15; Cursor cursor = Gdx.graphics.newCursor(pixmap, xHotspot, yHotspot); pixmap.dispose(); // We don't need the pixmap anymore Gdx.graphics.setCursor(cursor);   Note: You should call dispose() on your cursor if you don’t need it anymore.   Only cursors of power-of-two resolutions are supported. For example, if your cursor is 24×24, you must pad it to 32×32. Remember that your cursor may appear small on HDPI monitors if you don’t account for them. Custom cursors are supported only on the desktop and GWT backends.   System cursors   You can also change the cursor to one of the other system cursors. This only works on the LWJGL3, GWT and Android backends. It can be done as follows:  Gdx.graphics.setSystemCursor(SystemCursor.Crosshair);   Supported system cursors   Cursor appearance varies depending on operating system and user preferences. The images below are from Ubuntu. Windows and macOS cursors are not displayed out of respect for copyright, but they look similar.   You can hover over each row in the table with your mouse to see what the cursors look like on your own system.    \t \t\tSystemCursor \t\tAppearance \t\tNotes \t \t \t\tArrow \t\t \t\tThe default cursor \t \t \t\tIbeam \t\t \t\tIndicates text can be selected \t \t \t\tCrosshair \t\t \t\tUsed for finer precision than the default arrow cursor \t \t \t\tHand \t\t \t\tIndicates hyperlink can be followed \t \t \t\tHorizontalResize \t\t \t\tIndicates item can be resized horizontally \t \t \t\tVerticalResize \t\t \t\tIndicates item can be resized vertically \t \t \t\tNWSEResize \t\t \t\tIndicates item corner can be resized inwards or outwards  \t\tmacOS: Uses private system API and may fail in future  \t\tLinux: Uses newer standard that not all cursor themes support \t \t \t\tNESWResize \t\t \t\tIndicates item corner can be resized inwards or outwards  \t\tmacOS: Uses private system API and may fail in future  \t\tLinux: Uses newer standard that not all cursor themes support \t \t \t\tAllResize \t\t \t\tIndicates the ability to scroll/pan in all directions \t \t \t\tNotAllowed \t\t \t\tIndicates an action is prohibited  \t\tLinux: Uses newer standard that not all cursor themes support \t \t \t\tNone \t\t \t\tHides the cursor for when it may be unwanted, such as during video playback \t   Note that NWSEResize onwards are new in libGDX 1.11.0. They aren’t present in earlier versions.   Additional resources   If you wish to let your HTML5 game use system cursors libGDX doesn’t support, this is a good starting point:      CSS cursor values   Interfacing with platform specific code   JSNI                  Prev       Next          ",
        
        "url": "/wiki/input/cursor-visibility-and-catching" },{
        "title": "Decals",
        "excerpt":
        
        "A Decal is basically a Sprite that can be manipulated and rendered in 3D space. They allow you to draw a 2D texture within your 3D world very efficiently using a DecalBatch, with an API similar to that of the Sprite and SpriteBatch.   Decal   A Decal represents a sprite in 3d space. Typical 3d transformations such as translation, rotation and scaling are supported.   Creation  A Decal requires a DecalMaterial to do anything meaningful. While it’s possible to instantiate a DecalMaterial directly, it’s far simpler to use one of the helpful newDecal methods of the Decal class. For example, if we have a TextureRegion:   Decal decal = Decal.newDecal(textureRegion);   If you have a texture that uses transparency, then you can use an alternative method:   Decal decal = Decal.newDecal(textureRegion, true);   Additional methods are available for further specifying the size and blending properties of the region.   Manipulation  There are many methods for transforming a Decal in 3D space. Some examples:      Translation: translate(x, y, z), translateX(x), etc   Rotation: rotate(x, y, z), rotateX(), etc   Scale: setScale(x, y), setScale(x), etc   Position: setPosition(x, y, z), setPosition(Vector3), etc   For a more complete list of available methods, see here.   Billboard  If we wanted a decal to always face a PerspectiveCamera, we can easily do this using the lookAt method.   // For perspective camera decal.lookAt(camera.position, camera.up);   DecalBatch  Similar to SpriteBatch, we can use a DecalBatch to batch together many Decals for efficient rendering. The DecalBatch will allow us to queue as many Decals as we need before pushing big chunks of geometry to the graphics pipeline. We should create a DecalBatch only once, rather than every game loop.   // e.g. called at start of scene DecalBatch decalBatch = new DecalBatch(groupStrategy);   Group Strategy  The way a batch handles things depends on the GroupStrategy. Different strategies can be used to customize shaders, states, culling etc. Essentially, a GroupStrategy evaluates the group a sprite falls into, and adjusts settings before and after rendering a group.   For ease of use, the following GroupStrategy implementations exist:      SimpleOrthoGroupStrategy   CameraGroupStrategy   CameraGroupStrategy is likely the most appropriate. You can pass it a Camera instance and it will handle the rest.   decalBatch = new DecalBatch(new CameraGroupStrategy(camera));   Adding Decals  Unlike a SpriteBatch, we do not need to call a begin method. Instead, we just add decals to the batch.   decalBatch.add(decal);   It typically does not matter what order we add the decals to the batch, as the GroupStrategy and depth buffer will handle overlapping decals.   Rendering  Once we have added all our decals, we should tell the DecalBatch to flush them to the graphics pipeline.   decalBatch.flush();   The DecalBatch will then process all queued decals and render them.   Performance Tweaking  The default decal pool size is 1000. Adding more than this many decals in one go will trigger the DecalBatch to submit a chunk. If it is known before hand that not as many decals be needed on average the batch can be downsized to save memory.   We can set the pool size in the constructor:   DecalBatch decalBatch = new DecalBatch(500, groupStrategy);   Complete Example   You can see a working example that shows off the correct usage of Decals here:   https://github.com/libgdx/libgdx/blob/master/tests/gdx-tests/src/com/badlogic/gdx/tests/SimpleDecalTest.java  ",
        
        "url": "/wiki/graphics/3d/decals" },{
        "title": "Demos & Tutorials",
        "excerpt":
        
        "Tutorials   After you have created your very first libGDX project, we highly recommend our A Simple Game and Extending the Simple Game pages. If you’re completely new to game dev and have never developed a game before, this (even more straight-forward) tutorial by tann is also worth a look as an alternative.   From then on, our wiki is your best companion: it provides extensive documentation on nearly every feature offered by libGDX. For any further questions, our official Discord server is good starting point.   There are also many third-party tutorials. A (non-exhaustive) list of them can be found here.   Demos   If you want to inspect some open source libGDX games for reference, this wiki article is the right place to start looking.   The official libGDX organization on GitHub also offers a few demo projects that you can play around with to get a feeling for how to do things. Note that many of these demos have been created during game jams and may not necessarily reflect best practices. To use them setup your development environment, then simple clone the repositories on Github (or click on the “Download ZIP” button of a project’s site) and import the project into your favorite IDE as a Gradle project.      Cuboc - a simple platformer with a twist   Gdx Invaders - a 3D space invaders clone   Pax Britannica - a one button RTS game   Super Jumper - a Doodle jump clone with programmer’s art   Vector Pinball - a Box2D-based pinball game   The Plane that couldn’t fly good - a flappy bird’s clone   You can also try all these demos right in your browser!  ",
        
        "url": "/wiki/start/demos-and-tutorials" },{
        "title": "Dependency management with Gradle",
        "excerpt":
        
        "Contents      Useful Links   Guide to build.gradle   libGDX Dependencies   Available libGDX extensions   External Dependencies   Adding Repositories   Mavenizing Local Dependencies   File Dependencies            Android pitfall           Declaring Dependencies with HTML   libGDX Extension Inherits   Dependency Management for Libraries   Useful links  Dependency management with Gradle is easy to understand, and has many different approaches. If you are familiar with Maven or Ivy, Gradle is fully compatible with both approaches, as well as being able to support custom approaches. If you aren’t familiar with Gradle, there are great resources on their site to learn, it is recommended you give them a read to get comfortable with Gradle.     Gradle’s User Guide   Gradle’s Dependency Management Guide   Declare your dependencies   Guide to build.gradle  Gradle projects are managed by build.gradle files in their root directory. If you have used the gdx-setup.jar to build your libGDX project you will notice the structure: Structure Example   The root directory, and each sub directory contains a build.gradle file, for clarity we will define the dependencies in the root directory’s build.gradle file. (Note it can be done in each of the build.gradle scripts in the sub directories, it is just cleaner and easier to follow when it is handled all in one place)   Here is a small section of the default buildscript that is generated from the setup:   Full script you will see will differ slightly depending on what other modules you have  //Configuration for the script itself (aka, listing the dependencies of the script that lists dependencies - InSCRIPTion!) buildscript {     //Defines the repositories required by this script, e.g. hosting the android plugin     repositories {         //local maven repository (advanced use)         mavenLocal()         //maven central repository, needed for the android plugin         mavenCentral()         //snapshot repository (in case this script depends on snapshot/prerelease artifacts)         maven { url \"https://oss.sonatype.org/content/repositories/snapshots/\" }     }     //Defines the artifacts this script depends on, e.g. the android plugin     dependencies {         //Adds the android gradle plugin as a dependency of this buildscript         classpath 'com.android.tools.build:gradle:1.5.0'     } }  //Configuration common to all projects (:core, :desktop and :android in this example) allprojects {     //Defines gradle plugins used by all projects.     //A plugin extends gradle with additional tasks, configurations, etc., with defaults set according to conventions.     apply plugin: \"eclipse\"     apply plugin: \"idea\"      //Version of your game     version = \"1.0\"     //Defines 'extra' (custom) properties for all projects     ext {         appName = \"the-name-of-your-game\"         //Versions of the libGDX dependencies (used further below on those 'compile' lines)         gdxVersion = \"1.9.3\"         roboVMVersion = '2.1.0'         box2DLightsVersion = '1.4'         ashleyVersion = '1.7.0'         aiVersion = '1.8.0'     }      //Defines all repositories needed for all projects     repositories {         mavenLocal()         mavenCentral()         maven { url \"https://oss.sonatype.org/content/repositories/snapshots/\" }         maven { url \"https://oss.sonatype.org/content/repositories/releases/\" }     } }  //Configuration for the :desktop project project(\":desktop\") {     //Uses the java plugin (provides compiling, execution, etc.).     //That one is bundled with gradle, so we didn’t have to define it in the buildscript section.     apply plugin: \"java\"      //Defines dependencies for the :desktop project     dependencies {         //Adds dependency on the :core project as well as the gdx lwjgl backend and native dependencies         implementation project(\":core\")         implementation \"com.badlogicgames.gdx:gdx-backend-lwjgl:$gdxVersion\"         implementation \"com.badlogicgames.gdx:gdx-platform:$gdxVersion:natives-desktop\"     } }  //Configuration for the :android project project(\":android\") {     //Uses the android gradle plugin (provides compiling, copying on device, etc.)     apply plugin: \"android\"      configurations { natives }      //Defines dependencies for the :android project     dependencies {         //Adds dependencies on the :core project as well as the android backends and all platform natives.         //Note the 'natives' classifier in this project.         implementation project(\":core\")         implementation \"com.badlogicgames.gdx:gdx-backend-android:$gdxVersion\"                         natives \"com.badlogicgames.gdx:gdx-platform:$gdxVersion:natives-armeabi-v7a\"         natives \"com.badlogicgames.gdx:gdx-platform:$gdxVersion:natives-arm64-v8a\"         natives \"com.badlogicgames.gdx:gdx-platform:$gdxVersion:natives-x86\"         natives \"com.badlogicgames.gdx:gdx-platform:$gdxVersion:natives-x86_64\"     } }  //Configuration for the :core project project(\":core\") {     //Uses the java gradle plugin     apply plugin: \"java\"      dependencies {         //Defines dependencies for the :core project, in this example the gdx dependency         implementation \"com.badlogicgames.gdx:gdx:$gdxVersion\"     } }    libGDX Dependencies  Dependencies are configured in the root build.gradle file as shown in the build.gradle guide above. In order to add an external dependency to a project, you must declare the dependency correctly under the correct part of the build.script.   (Some) libGDX extensions are mavenized and pushed to the maven repo, which means we can very easily pull them into our projects from the build.gradle file. You can see in the list below of the format that these dependencies take. If you are familiar with maven, notice the format:  compile '&lt;groupId&gt;:&lt;artifactId&gt;:&lt;version&gt;:&lt;classifier&gt;'   Let’s take a quick example to see how this works with the root build.gradle file.   As mentioned earlier, you do not need to  modify the individual build.gradle files in each of the different platform-specific folders (e.g., -desktop, -ios, -core). You only need to modify the root build.gradle file.   Here we see the dependencies for the FreeType Extension, say we want our Android project to have this dependency. We locate our project(\":android\") stub in the root directory’s build.gradle:  project(\":android\") {     apply plugin: \"android\"      configurations { natives }      dependencies {         implementation project(\":core\")         implementation \"com.badlogicgames.gdx:gdx-backend-android:$gdxVersion\"                 natives \"com.badlogicgames.gdx:gdx-platform:$gdxVersion:natives-armeabi-v7a\"         natives \"com.badlogicgames.gdx:gdx-platform:$gdxVersion:natives-arm64-v8a\"         natives \"com.badlogicgames.gdx:gdx-platform:$gdxVersion:natives-x86\"         natives \"com.badlogicgames.gdx:gdx-platform:$gdxVersion:natives-x86_64\"     } }   We know our FreeType extension has declarations:  implementation \"com.badlogicgames.gdx:gdx-freetype:$gdxVersion\" natives \"com.badlogicgames.gdx:gdx-freetype-platform:$gdxVersion:natives-armeabi-v7a\" natives \"com.badlogicgames.gdx:gdx-freetype-platform:$gdxVersion:natives-arm64-v8a\" natives \"com.badlogicgames.gdx:gdx-freetype-platform:$gdxVersion:natives-x86\" natives \"com.badlogicgames.gdx:gdx-freetype-platform:$gdxVersion:natives-x86_64\"   So all we need to do is whack it in the dependencies stub   project(\":android\") {     apply plugin: \"android\"      configurations { natives }      dependencies {         implementation project(\":core\")         implementation \"com.badlogicgames.gdx:gdx-backend-android:$gdxVersion\"                 natives \"com.badlogicgames.gdx:gdx-platform:$gdxVersion:natives-armeabi-v7a\"         natives \"com.badlogicgames.gdx:gdx-platform:$gdxVersion:natives-arm64-v8a\"         natives \"com.badlogicgames.gdx:gdx-platform:$gdxVersion:natives-x86\"         natives \"com.badlogicgames.gdx:gdx-platform:$gdxVersion:natives-x86_64\"          implementation \"com.badlogicgames.gdx:gdx-freetype:$gdxVersion\"                 natives \"com.badlogicgames.gdx:gdx-freetype-platform:$gdxVersion:natives-armeabi-v7a\"         natives \"com.badlogicgames.gdx:gdx-freetype-platform:$gdxVersion:natives-arm64-v8a\"         natives \"com.badlogicgames.gdx:gdx-freetype-platform:$gdxVersion:natives-x86\"         natives \"com.badlogicgames.gdx:gdx-freetype-platform:$gdxVersion:natives-x86_64\"     } }  And we are done, our android project now has the freetype dependency. After this you will need to refresh your dependencies. Easy eh.   libGDX Extensions  Mavenized libGDX extensions ready to import from the build.gradle script include:     Box2D   Bullet   FreeTypeFont   Controllers   Tools   Box2DLights   Ashley   AI   Box2D Gradle  Core Dependency:  implementation \"com.badlogicgames.gdx:gdx-box2d:$gdxVersion\"  Desktop Dependency:  implementation \"com.badlogicgames.gdx:gdx-box2d-platform:$gdxVersion:natives-desktop\"  Android Dependency:  implementation \"com.badlogicgames.gdx:gdx-box2d:$gdxVersion\" natives \"com.badlogicgames.gdx:gdx-box2d-platform:$gdxVersion:natives-armeabi-v7a\" natives \"com.badlogicgames.gdx:gdx-box2d-platform:$gdxVersion:natives-arm64-v8a\" natives \"com.badlogicgames.gdx:gdx-box2d-platform:$gdxVersion:natives-x86\" natives \"com.badlogicgames.gdx:gdx-box2d-platform:$gdxVersion:natives-x86_64\"  iOS Dependency:  implementation \"com.badlogicgames.gdx:gdx-box2d-platform:$gdxVersion:natives-ios\"  HTML Dependency:  implementation \"com.badlogicgames.gdx:gdx-box2d:$gdxVersion:sources\" implementation \"com.badlogicgames.gdx:gdx-box2d-gwt:$gdxVersion:sources\"  and in ./html/src/yourgamedomain/GdxDefinition*.gwt.xml add &lt;inherits name=\"com.badlogic.gdx.physics.box2d.box2d-gwt\"/&gt;     Bullet Gradle  Core Dependency:  implementation \"com.badlogicgames.gdx:gdx-bullet:$gdxVersion\"  Desktop Dependency:  implementation \"com.badlogicgames.gdx:gdx-bullet-platform:$gdxVersion:natives-desktop\"  Android Dependency:  implementation \"com.badlogicgames.gdx:gdx-bullet:$gdxVersion\" natives \"com.badlogicgames.gdx:gdx-bullet-platform:$gdxVersion:natives-armeabi-v7a\" natives \"com.badlogicgames.gdx:gdx-bullet-platform:$gdxVersion:natives-arm64-v8a\" natives \"com.badlogicgames.gdx:gdx-bullet-platform:$gdxVersion:natives-x86\" natives \"com.badlogicgames.gdx:gdx-bullet-platform:$gdxVersion:natives-x86_64\"  iOS Dependency:  implementation \"com.badlogicgames.gdx:gdx-bullet-platform:$gdxVersion:natives-ios\"  HTML Dependency: Not compatible!     FreeTypeFont Gradle  Core Dependency:  implementation \"com.badlogicgames.gdx:gdx-freetype:$gdxVersion\"  Desktop Dependency:  implementation \"com.badlogicgames.gdx:gdx-freetype-platform:$gdxVersion:natives-desktop\"  Android Dependency:  implementation \"com.badlogicgames.gdx:gdx-freetype:$gdxVersion\" natives \"com.badlogicgames.gdx:gdx-freetype-platform:$gdxVersion:natives-armeabi-v7a\" natives \"com.badlogicgames.gdx:gdx-freetype-platform:$gdxVersion:natives-arm64-v8a\" natives \"com.badlogicgames.gdx:gdx-freetype-platform:$gdxVersion:natives-x86\" natives \"com.badlogicgames.gdx:gdx-freetype-platform:$gdxVersion:natives-x86_64\"  iOS Dependency:  implementation \"com.badlogicgames.gdx:gdx-freetype-platform:$gdxVersion:natives-ios\"  iOS-MOE Dependency:  natives \"com.badlogicgames.gdx:gdx-freetype-platform:$gdxVersion:natives-ios\"  HTML Dependency: Not compatible!     Controllers Gradle  Core Dependency:  implementation \"com.badlogicgames.gdx:gdx-controllers:$gdxVersion\"  Desktop Dependency:  implementation \"com.badlogicgames.gdx:gdx-controllers-desktop:$gdxVersion\" implementation \"com.badlogicgames.gdx:gdx-controllers-platform:$gdxVersion:natives-desktop\"  Android Dependency:  implementation \"com.badlogicgames.gdx:gdx-controllers:$gdxVersion\" implementation \"com.badlogicgames.gdx:gdx-controllers-android:$gdxVersion\"  iOS Dependency: Not supported, but you can still compile and run your iOS app. Controllers just won’t be available   HTML Dependency:  implementation \"com.badlogicgames.gdx:gdx-controllers:$gdxVersion:sources\" implementation \"com.badlogicgames.gdx:gdx-controllers-gwt:$gdxVersion\" implementation \"com.badlogicgames.gdx:gdx-controllers-gwt:$gdxVersion:sources\"  and in ./html/src/yourgamedomain/GdxDefinition*.gwt.xml add &lt;inherits name=\"com.badlogic.gdx.controllers.controllers-gwt\"/&gt;     Tools Gradle  Core Dependency: Don’t put me in core!   Desktop Dependency (LWJGL2 only):  implementation \"com.badlogicgames.gdx:gdx-tools:$gdxVersion\"  Android Dependency: Not compatible!   iOS Dependency: Not compatible!   HTML Dependency: Not compatible!     Box2DLights Gradle     Note: this extension also requires the Box2D extension   Core Dependency:  implementation \"com.badlogicgames.box2dlights:box2dlights:$box2DLightsVersion\"  Android Dependency:  implementation \"com.badlogicgames.box2dlights:box2dlights:$box2DLightsVersion\"  HTML Dependency:  implementation \"com.badlogicgames.box2dlights:box2dlights:$box2DLightsVersion:sources\"  and in ./html/src/yourgamedomain/GdxDefinition*.gwt.xml add &lt;inherits name=\"Box2DLights\"/&gt;     Ashley Gradle      Note: This extension release cycle is not dependent on the main libGDX library, and so it is not unusual to have a new version published between two libGDX releases. If you want to pull in a new (or different) version, check https://repo1.maven.org/maven2/com/badlogicgames/ashley/ashley/ and change the ashleyVersion value in the ext section.   Core Dependency:  implementation \"com.badlogicgames.ashley:ashley:$ashleyVersion\"  Android Dependency:  implementation \"com.badlogicgames.ashley:ashley:$ashleyVersion\"  HTML Dependency:  implementation \"com.badlogicgames.ashley:ashley:$ashleyVersion:sources\"  and in ./html/src/yourgamedomain/GdxDefinition*.gwt.xml add &lt;inherits name='com.badlogic.ashley_gwt' /&gt;     AI Gradle      Note: This extension release cycle is not dependent on the main libGDX library, and so it is not unusual to have a new version published between two libGDX releases. If you want to pull in a new (or different) version, check https://repo1.maven.org/maven2/com/badlogicgames/gdx/gdx-ai/ and change the aiVersion value in the ext section.   Core Dependency:  implementation \"com.badlogicgames.gdx:gdx-ai:$aiVersion\"  Android Dependency:  implementation \"com.badlogicgames.gdx:gdx-ai:$aiVersion\"  HTML Dependency:  implementation \"com.badlogicgames.gdx:gdx-ai:$aiVersion:sources\"  and in ./html/src/yourgamedomain/GdxDefinition*.gwt.xml add &lt;inherits name='com.badlogic.gdx.ai' /&gt;     External Dependencies  Adding external repositories  Gradle finds files defined as dependencies by looking through all the repositories defined in the buildscript. Gradle understands several repository formats, which include Maven and Ivy.   Under the allprojects stub, you can see how repositories are defined. Here is an example:  allprojects {         repositories {         mavenCentral() // Maven Central repo         mavenLocal() // local Maven repo         maven { url \"https://oss.sonatype.org/content/repositories/snapshots/\" } // remote Maven repo: Sonatype snapshots         maven { url \"https://jitpack.io\" } // remote Maven repo: Jitpack     } }  Adding Dependencies  External dependencies are identified by their group, name, version and sometimes classifier attributes.   dependencies {     implementation group: 'com.badlogicgames.gdx', name: 'gdx', version: '1.0-SNAPSHOT', classifier: 'natives-desktop' }  Gradle allows you to use shortcuts when defining external dependencies, the above configuration is the same as:   dependencies {     implementation 'com.badlogicgames.gdx:gdx:1.0-SNAPSHOT:natives-desktop' }   Mavenizing Local Dependencies  If you would prefer to use maven repositories to manage local .jar files, these two commands will take any local .jar file and install them (and their source) to your local maven repository.   mvn install:install-file -Dfile=&lt;path-to-file&gt; -DgroupId=&lt;group-id&gt; -DartifactId=&lt;artifact-id&gt; -Dversion=&lt;version&gt; -Dpackaging=&lt;packaging&gt;  mvn install:install-file -Dfile=&lt;path-to-source-file&gt; -DgroupId=&lt;group-id&gt; -DartifactId=&lt;artifact-id&gt; -Dversion=&lt;version&gt; -Dpackaging=&lt;packaging&gt; -Dclassifier=sources   To then set up gradle to include your new dependency, edit your build.gradle file in the root project directory and edit the core project entry:  project(\":core\") {    ...      dependencies {         ...         implementation \"&lt;group-id&gt;:&lt;artifact-id&gt;:&lt;version&gt;\"         implementation \"&lt;group-id&gt;:&lt;artifact-id&gt;:&lt;version&gt;:sources\"     } }   After this you will need to refresh your dependencies for your IDE to see, so run:  Command line - $ ./gradlew --refresh-dependencies  Eclipse - $ ./gradlew eclipse  IntelliJ - $ ./gradlew idea   Also, don’t forget that any dependencies added this way also need to be included in the GWT inheritance file.   File Dependencies  If you have a dependency that is not mavenized, you can still depend on them!   To do this, in your project stub in the root build.gradle file, locate the dependencies { } section as always, and add the following:   dependencies {     implementation fileTree(dir: 'libs', include: '*.jar') }   This will include all the .jar files in the libs directory as dependencies.   NOTE: “dir” is relative to the project root, if you add the dependencies to your android project, ‘libs’ would need to be in the android/ directory. If you added the dependencies in the core project, ‘libs’ would need to be in the core/ directory.   An example with a more complete script:  project(\":android\") {     apply plugin: \"android\"      configurations { natives }      dependencies {         implementation project(\":core\")         implementation \"com.badlogicgames.gdx:gdx-backend-android:$gdxVersion\"                 natives \"com.badlogicgames.gdx:gdx-platform:$gdxVersion:natives-armeabi-v7a\"         natives \"com.badlogicgames.gdx:gdx-platform:$gdxVersion:natives-arm64-v8a\"         natives \"com.badlogicgames.gdx:gdx-platform:$gdxVersion:natives-x86\"         natives \"com.badlogicgames.gdx:gdx-platform:$gdxVersion:natives-x86_64\"          implementation \"com.badlogicgames.gdx:gdx-freetype:$gdxVersion\"                 natives \"com.badlogicgames.gdx:gdx-freetype-platform:$gdxVersion:natives-armeabi-v7a\"         natives \"com.badlogicgames.gdx:gdx-freetype-platform:$gdxVersion:natives-arm64-v8a\"         natives \"com.badlogicgames.gdx:gdx-freetype-platform:$gdxVersion:natives-x86\"         natives \"com.badlogicgames.gdx:gdx-freetype-platform:$gdxVersion:natives-x86_64\"         implementation fileTree(dir: 'libs', include: '*.jar')     } }   It is worth nothing that these file dependencies are not included in the published dependency descriptor for your project, but they are included in transitive project dependencies within the same build.   Android Pitfall  When adding flat file dependencies to a project, for example the core project, you would need to duplicate the dependency declaration for the android project. This is because the Android Gradle plugin currently can’t handle transitive flat file dependencies.   For example, if you were to add the all the jars in your libs directory as dependencies for your project, you would need to do the following.   project(\":core\") {    ...    implementation fileTree(dir: '../libs', include: '*.jar')    ... }  // And also  project(\":android\") {    ...    implementation fileTree(dir: '../libs', include: '*.jar')    ... }   This is only required for the android project, all other projects inherit flat file dependencies OK.   Gwt Inheritance  Gwt is special, so in order to let the GWT compiler know what modules the project depends on, and inherits from, you need to let it know.   This is done in the gwt.xml files in the gwt sub directory. You will need to make the changes both to the GdxDefinition.gwt.xml and also the GdxDefinitionSuperdev.gwt.xml.   The default gwt.xml:  &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt; &lt;!DOCTYPE module PUBLIC \"-//Google Inc.//DTD Google Web Toolkit trunk//EN\" \"http://google-web-toolkit.googlecode.com/svn/trunk/distro-source/core/src/gwt-module.dtd\"&gt; &lt;module rename-to=\"html\"&gt; \t&lt;inherits name='com.badlogic.gdx.backends.gdx_backends_gwt' /&gt; \t&lt;inherits name='MyGameName' /&gt; \t&lt;entry-point class='com.badlogic.mygame.client.HtmlLauncher' /&gt;  \t&lt;set-configuration-property name=\"gdx.assetpath\" value=\"../android/assets\" /&gt; &lt;/module&gt;  We depend on the libGDX gwt backend, as well as the core project, so we have them defined in a  tag. So when you add your dependency via methods above, you need to add it here too!   libGDX Extension Inherits  These are the libGDX extensions that are supported in gwt      libGDX Core - &lt;inherits name='com.badlogic.gdx.backends.gdx_backends_gwt' /&gt;   Box2d       - &lt;inherits name='com.badlogic.gdx.physics.box2d.box2d-gwt' /&gt;   Box2dLights - &lt;inherits name='Box2DLights' /&gt;   Controllers - &lt;inherits name='com.badlogic.gdx.controllers.controllers-gwt' /&gt;   Ashley      - &lt;inherits name='com.badlogic.ashley_gwt' /&gt;   AI          - &lt;inherits name='com.badlogic.gdx.ai' /&gt;   Dependency management for libraries  If you’re creating a library that people can include in their projects via gradle, you might need to replace the implementation keyword by api. Any dependency of your library that you declare with api will be visible and usable by others that depend on your library while implementation makes it only accessible for you.  ",
        
        "url": "/wiki/articles/dependency-management-with-gradle" },{
        "title": "Deploying your application",
        "excerpt":
        
        "The mechanism to deploy your game differs between platforms. This article aims to articulate, what is necessary to deploy to each platform that libGDX officially supports:      Deploy to Windows/Linux/Mac   Deploy to Android   Deploy to iOS   Deploy Web   Deploy to Windows/Linux/Mac OS X  As JAR file  The easiest way to deploy to Windows/Linux/Mac is to create a runnable JAR file. This can be done via the following console command: ./gradlew desktop:dist   If you are getting an error like Unsupported class file major version 60, your Java version (see here for a list) is not supported by your Gradle version. To fix this, install an older JDK.   The generated JAR file will be located in the desktop/build/libs/ folder. It contains all necessary code as well as all your art assets from the android/assets folder and can be run either by double clicking or on the command line via java -jar jar-file-name.jar. Your audience must have a JVM installed for this to work. The JAR will work on Windows, Linux and Mac OS X!   Alternative (modern) ways of deployment  Distributing java applications as JAR file can be very unhandy and prone to issues, as not every user can be expected to have the right JRE (or even any JRE) installed. Other ways of deployment are for example:      A very convenient way to distribute java application is to just bundle an JRE. See this entry on how to do this. (This is the recommended way to distribute an application!)   Via electron, HTML5 applications can be deployed to desktop. See here.   GWT applications can also be bundled as UWP Apps, see here.   Deploy to Android  gradlew android:assembleRelease   This will create an unsigned APK file in the android/build/outputs/apk folder. Before you can install or publish this APK, you must sign it. The APK build by the above command is already in release mode, you only need to follow the steps for keytool and jarsigner. You can install this APK file on any Android device that allows installation from unknown sources.   Deploy to iOS  This section assumes you’re familiar with the basic deployment steps for iOS apps.   Prerequisites:  In order to upload the IPA to the app store, it must be signed with your distribution signature and linked to your provisioning profile. You can follow Apple’s guide on app store distribution to create provisioning profiles and certificates. Once you have done that, you must define them in your root build.gradle file, in your IOS Project   project(\":ios\") {     apply plugin: \"java\"     apply plugin: \"robovm\"      dependencies {         // ...     }      robovm {         iosSignIdentity = \"[Signing identity name]\"         iosProvisioningProfile = \"[provisioning profile name]\"         iosSkipSigning = false         archs = \"thumbv7:arm64\"     }  }      Your provisioning profile name is available in your developer portal (where you created your provisioning profile).   Your Signing identity name is available in your keychain, under “My Certificates”   Packaging:  To create your IPA, run   gradlew ios:createIPA   This will create an IPA in the ios/build/robovm folder that you distribute to the Apple App Store. To upload your app you will need to use the application loader within XCode (Xode-&gt;Open Developer Tool-&gt;Application loader)   Note: as of iOS 11 instead of simply adding your icons into your data folder within your iOS project you need to include an asset Catalog. If you do not include one, you can still submit your app but later you receive a message regarding Missing Info.plist value - A value for the Info.plist key CFBundleIconName is missing in the bundle '...'. Apps that provide icons in the asset catalog must also provide this Info.plist key. To fix this, follow these instructions to include an asset catalog.   Additional guides   Deploying to iOS is relatively straight forward, see here if you’re having difficulties. Take a look at this post, if you want to deploy your iOS application to TestFlight.   Deploy Web  gradlew html:dist   This will compile your app to Javascript and place the resulting Javascript, HTML and asset files in the html/build/dist/ folder. The contents of this folder have to be served up by a web server, e.g. Apache or Nginx. Just treat the contents like you’d treat any other static HTML/Javascript site. There is no Java or Java Applets involved!   When running the result, you might encounter errors like Couldn't find Type for class .... To fix this, please see our wiki page Reflection and include the needed classes/packages.   With Python installed, you can test your distribution by executing the following in the html/build/dist folder:   Python 2.x   python -m SimpleHTTPServer   Python 3.x   python -m http.server 8000   You can then open a browser to http://localhost:8000 and see your project in action.   With Node.js npm install http-server -g then http-server html/build/dist and browse at http://localhost:8080. docs   With PHP you may type php -S localhost:8000 and browse at http://localhost:8080. docs  ",
        
        "url": "/wiki/deployment/deploying-your-application" },{
        "title": "Distance field fonts",
        "excerpt":
        
        "rendering super-smooth scalable bitmap fonts   Signed distance field rendering is a technique used in Team Fortress 2, and documented by Chris Green of Valve in the SIGGRAPH 2007 paper Improved Alpha-Tested Magniﬁcation for Vector Textures and Special Effects. It allows you to render bitmap fonts without jagged edges even at high magnifications. This article describes how to implement the technique in libgdx.   Introduction   Traditional bitmap fonts work fine if the pixels in the font map 1:1 onto screen pixels. However, they look bad when rotated, and increasingly worse when scaled up. Either you end up seeing individual pixels, or you turn on linear interpolation and end up with a smudgy blur instead.   Using a distance field font lets you render text that remains crisp even under rotations and other arbitrary transforms, even blown up to a large magnification, without notable extra run-time cost. You can see the difference below:      The same technique can also be used to draw symbols, logos, anything. The major drawback is that it works only for monochrome images; it is not possible to use this technique for arbitrary color images.   There is an example of rendering in the libGDX source code. Check out com.badlogic.gdx.tests.BitmapFontDistanceFieldTest in the gdx-tests project. It was used to produce the above screenshot. There’s also com.badlogic.gdx.graphics.g2d.DistanceFieldFont.java class if you want to jump directly to usage.   How does it work?   The idea is pretty simple. Instead of providing a (possibly anti-aliased) black and white image of the font, we pre-process it to produce a signed distance field. The rightmost column in the screenshot above shows what our font image looks like after pre-processing.   The pre-processor takes a black and white image as input, with a black background and a white glyph. For each white pixel, it computes the distance to the closest black pixel, and vice versa. For black pixels, the distance for black pixels is then negated, and the result is normalized to the range 0-1. This gives us a smooth, continuous field, with 0.5 exactly on the edge of the original glyph, decreasing to 0.0 as we move farther away, and increasing to 1.0 when we move towards the inside.   We then set up alpha testing to output a pixel only when the alpha is greater than 0.5. With a texture that uses nearest-neighbour interpolation, this will look exactly the same as our input image. However, the distance field image is much better suited to linear interpolation than a traditional font image is: compare the third and fourth columns in the picture above.   Generating the font   This process is much the same as for regular bitmap fonts, but with different settings.      Fire up Hiero and choose your font and attributes as normal.   In the “Effects” list on the right, double-click “Distance field”. (If there is no filter called “Distance field”, your version of Hiero is too old. Try the nightly build as described on the Hiero page.)   Remove the default “Color” effect by clicking the X.   Set the color of the distance field if you like. It is best to leave this set to white, because you can change the color at rendering time.   Set the “Spread” to a suitable value. It should be about half the width of the thickest lines in your font, in pixels. At most, there should be small regions of bright white; don’t lose too much contrast.   In the bottom right corner, set the “Padding” on all four sides to be equal to the spread. You should see that your glyphs are no longer being clipped.   Set the “X” and “Y” to minus twice the spread. If you used a spread of 4, you’d set -8 for both X and Y. This is necessary because the padding increases the spacing between glyphs at rendering time.   Select the “Glyph cache” radio button and set the page size such that all glyphs fit on one page, with as little waste as possible. This makes loading easier.   Set the “Scale” to something larger than 1. We save this step for last because the higher the scale, the slower the font generation gets. 32 is a good value. You should now have something like this:         Save the font as usual to your assets directory.   Loading the font   There is no magic to loading the font into your game. You just need to make sure that you enable linear filtering on the texture:  Texture texture = new Texture(Gdx.files.internal(\"myfont.png\")); texture.setFilter(TextureFilter.Linear, TextureFilter.Linear);   To make your font look better when down-scaled (less than 1:1 size), you can also turn on mipmapping:  Texture texture = new Texture(Gdx.files.internal(\"myfont.png\"), true); // true enables mipmaps texture.setFilter(TextureFilter.MipMapLinearNearest, TextureFilter.Linear); // linear filtering in nearest mipmap image  Use either MipMapLinearNearest or the slower but smoother MipMapLinearLinear.   Then create the font:  BitmapFont font = new BitmapFont(Gdx.files.internal(\"myfont.fnt\"), new TextureRegion(texture), false);   Note: Before LibGDX 1.6.0 (May 2015), if you’re replacing a “regular” font by a distance field font, be aware that the font metrics are not the same. In particular, the extra padding causes the baseline to shift downwards, so you’ll need to compensate by drawing your text higher. As of commit c976f463, padding should be compensated for automatically.   Rendering with a shader   I’ll assume that you are familiar with shaders in libgdx; if not, read the page on shaders.   There is nothing special about the vertex shader; we can just duplicate the one that SpriteBatch uses by default. Just take care to name the variables in the way that SpriteBatch expects:  uniform mat4 u_projTrans;  attribute vec4 a_position; attribute vec2 a_texCoord0; attribute vec4 a_color;  varying vec4 v_color; varying vec2 v_texCoord;  void main() {     gl_Position = u_projTrans * a_position;     v_texCoord = a_texCoord0;     v_color = a_color; }   The secret sauce is in the fragment shader. But even here, there’s not much to it:  #ifdef GL_ES precision mediump float; #endif  uniform sampler2D u_texture;  varying vec4 v_color; varying vec2 v_texCoord;  const float smoothing = 1.0/16.0;  void main() {     float distance = texture2D(u_texture, v_texCoord).a;     float alpha = smoothstep(0.5 - smoothing, 0.5 + smoothing, distance);     gl_FragColor = vec4(v_color.rgb, v_color.a * alpha); }   Assuming you’ve saved these to your assets directory as font.vert and font.frag, you can load the shader as usual:  ShaderProgram fontShader = new ShaderProgram(Gdx.files.internal(\"font.vert\"), Gdx.files.internal(\"font.frag\")); if (!fontShader.isCompiled()) {     Gdx.app.error(\"fontShader\", \"compilation failed:\\n\" + fontShader.getLog()); }   To render the font using this shader, assuming that you already have a SpriteBatch and are between a begin() and end() call, is really straightforward:  spriteBatch.setShader(fontShader); font.draw(spriteBatch, \"Hello smooth world!\", 10, 10); spriteBatch.setShader(null);   Customizing the shader   Remember that distance is a value between 0 and 1, with 0 being far away from the letter, 0.5 being right on the edge, and 1 being well inside it. The smoothstep function in the shader above is mapping values well below 0.5 to 0, and values well above 0.5 to 1, but gives a smooth transition around 0.5 to provide antialiasing. The softness of this transition is configured by the smoothing constant, which you should tweak to be correct for your font and scale.   The right smoothing value for crisp fonts is 0.25f / (spread * scale), where spread is the value you used when generating the font, and scale is the scale you’re drawing it at (how pixels in the distance field font are mapped to screen pixels). If the scale is not constant, you can pass it in via a uniform variable.   There are all sorts of additional tricks you can do based on the distance variable in the shader. Here are some possibilities. I haven’t tested any of these; if you find bugs, please update this wiki page!   Adding an outline   The idea is that we output a different color when distance is between outlineDistance and 0.5.   ... const float outlineDistance; // Between 0 and 0.5, 0 = thick outline, 0.5 = no outline const vec4 outlineColor; ... void main() {     float distance = texture2D(u_texture, v_texCoord).a;     float outlineFactor = smoothstep(0.5 - smoothing, 0.5 + smoothing, distance);     vec4 color = mix(outlineColor, v_color, outlineFactor);     float alpha = smoothstep(outlineDistance - smoothing, outlineDistance + smoothing, distance);     gl_FragColor = vec4(color.rgb, color.a * alpha); }   Adding a drop shadow   Here, we sample the texture a second time, slightly offset from the first. The second application gets a lot more smoothing applied to it, and is rendered “behind” the actual text.   ... const vec2 shadowOffset; // Between 0 and spread / textureSize const float shadowSmoothing; // Between 0 and 0.5 const vec4 shadowColor; ... void main() {     float distance = texture2D(u_texture, v_texCoord).a;     float alpha = smoothstep(0.5 - smoothing, 0.5 + smoothing, distance);     vec4 text = vec4(v_color.rgb, v_color.a * alpha);      float shadowDistance = texture2D(u_texture, v_texCoord - shadowOffset).a;     float shadowAlpha = smoothstep(0.5 - shadowSmoothing, 0.5 + shadowSmoothing, shadowDistance);     vec4 shadow = vec4(shadowColor.rgb, shadowColor.a * shadowAlpha);      gl_FragColor = mix(shadow, text, text.a); }   Using distance fields for arbitrary images   The generator used by Hiero can also be used as a stand-alone command line tool, to process pre-existing black and white images. Run it from an unzipped libGDX distribution directory as follows:   Windows:  java -cp gdx.jar;gdx-natives.jar;gdx-backend-lwjgl.jar;gdx-backend-lwjgl-natives.jar;extensions\\gdx-tools\\gdx-tools.jar com.badlogic.gdx.tools.distancefield.DistanceFieldGenerator   Linux:  java -cp gdx.jar:gdx-natives.jar:gdx-backend-lwjgl.jar:gdx-backend-lwjgl-natives.jar:extensions/gdx-tools/gdx-tools.jar com.badlogic.gdx.tools.distancefield.DistanceFieldGenerator   This will print usage instructions:  Generates a distance field image from a black and white input image. The distance field image contains a solid color and stores the distance in the alpha channel.  The output file format is inferred from the file name.  Command line arguments: INFILE OUTFILE [OPTION...]  Possible options:   --color rrggbb    color of output image (default: ffffff)   --downscale n     downscale by factor of n (default: 1)   --spread n        edge scan distance (default: 1)   The options are similar to those in Hiero described above, except that spread is defined in terms of pixels on the input image, not the downscaled output image. To get the same result, multiply it by downscale. Also note that the defaults are probably not very helpful, and you’ll want to specify both --downscale and --spread:  java -cp extensions/gdx-tools.jar:gdx.jar com.badlogic.gdx.tools.distancefield.DistanceFieldGenerator      --downscale 32      --spread 128      logo.png logo-df.png  ",
        
        "url": "/wiki/graphics/2d/fonts/distance-field-fonts" },{
        "title": "Event handling",
        "excerpt":
        
        "Event handling allows you to get more granular and most of all chronological information about input from the user. Event handling provides a way to implement interactions with user interfaces, where specific input sequences are important, e.g. touch down, touch up on a button means the user clicked the button. Such interactions are hard to implement with polling.   Input Processor  Event handling is done using the common observer pattern. First we have to implement a listener interface called InputProcessor:   public class MyInputProcessor implements InputProcessor {    public boolean keyDown (int keycode) {       return false;    }     public boolean keyUp (int keycode) {       return false;    }     public boolean keyTyped (char character) {       return false;    }     public boolean touchDown (int x, int y, int pointer, int button) {       return false;    }     public boolean touchUp (int x, int y, int pointer, int button) {       return false;    }     public boolean touchDragged (int x, int y, int pointer) {       return false;    }     public boolean mouseMoved (int x, int y) {       return false;    }     public boolean scrolled (float amountX, float amountY) {       return false;    } }   The first three methods allow you to listen for keyboard events:      keyDown(): Called when a key was pressed down. Reports the key code, as found in Keys.   keyUp(): Called when a key was lifted. Reports the key code as above.   keyTyped(): Called when a Unicode character was generated by the keyboard input. This can be used to implement text fields and similar user interface elements.   The next five methods report mouse/touch events:      touchDown(): Called when a finger went down on the screen or a mouse button was pressed. Reports the coordinates as well as the pointer index and mouse button (always Buttons.LEFT for touch screens).   touchUp(): Called when a finger was lifted from the screen or a mouse button was released. Reports the last known coordinates as well as the pointer index and mouse button (always Buttons.Left for touch screens).   touchDragged(): Called when a finger is being dragged over the screen or the mouse is dragged while a button is pressed. Reports the coordinates and pointer index. The button is not reported as multiple buttons could be pressed while the mouse is being dragged. You can use Gdx.input.isButtonPressed() to check for a specific button.   mouseMoved(): Called when the mouse is moved over the screen without a mouse button being down. This event is only relevant on the desktop and will never occur on touch screen devices where you only get touchDragged() events.   scrolled(): Called when the scroll wheel of the mouse was turned. Reports either -1 or 1 depending on the direction of spin. This will never be called for touch screen devices.   Each of the methods returns a boolean. We’ll look into why that is in the InputMultiplexer section below.   Once you implement your InputProcessor you have to tell libGDX about it so it can be called when a new input event arrives:   MyInputProcessor inputProcessor = new MyInputProcessor(); Gdx.input.setInputProcessor(inputProcessor);   From this point on, all new input events will be pushed to the MyInputProcessor instance. Events are dispatched right before the call to ApplicationListener.render(), on the rendering thread.   InputAdapter   InputAdapter implements all the InputProcessor methods, returning false from each. You can extend InputAdapter so you only need to implement the methods you need. You can also use an anonymous inner class.   Gdx.input.setInputProcessor(new InputAdapter () {    @Override    public boolean touchDown (int x, int y, int pointer, int button) {       // your touch down code here       return true; // return true to indicate the event was handled    }     @Override    public boolean touchUp (int x, int y, int pointer, int button) {       // your touch up code here       return true; // return true to indicate the event was handled    } });   InputMultiplexer  Sometimes you want to chain InputProcessors, e.g. you have one processor for your UI which should be invoked first, and a second processor for input events that manipulate your game’s world. You can use the InputMultiplexer class to achieve this:   InputMultiplexer multiplexer = new InputMultiplexer(); multiplexer.addProcessor(new MyUiInputProcessor()); multiplexer.addProcessor(new MyGameInputProcessor()); Gdx.input.setInputProcessor(multiplexer);   The InputMultiplexer will hand any new events to the first InputProcessor that was added to it. If that processor returns false from the method invoked to handle the event, this indicates the event was not handled and the multiplexer will hand the event to the next processor in the chain. Through this mechanism, the MyUiInputProcessor can handle any events that fall inside one of its widgets and pass on any other events to the MyGameInputProcessor.   Example of continuous Input handle  If you want to move an actor using the Input Processor, you will notice that it will move only when the key is typed (or pressed with keydown). To continuosly handle input, or to move a sprite, you could add a flag to your actor like this:  public class Bob {     boolean leftMove;     boolean rightMove;     ...     updateMotion()     { \t    if (leftMove) \t    { \t\t    x -= 5 * Gdx.graphics.getDeltaTime(); \t    } \t    if (rightMove) \t    { \t\t    x += 5 * Gdx.graphics.getDeltaTime(); \t    }     }     ...     public void setLeftMove(boolean t)     { \t    if(rightMove &amp;&amp; t) rightMove = false; \t    leftMove = t;     }     public void setRightMove(boolean t)     { \t    if(leftMove &amp;&amp; t) leftMove = false; \t    rightMove = t;     }  Then, in your Input processor:   ...     @Override     public boolean keyDown(int keycode)     { \t    switch (keycode) \t    { \t\tcase Keys.LEFT: \t\t\tbob.setLeftMove(true); \t\t\tbreak; \t\tcase Keys.RIGHT: \t\t\tbob.setRightMove(true); \t\t\tbreak; \t    } \t    return true;     }     @Override     public boolean keyUp(int keycode)     { \t    switch (keycode) \t    { \t\tcase Keys.LEFT: \t\t\tbob.setLeftMove(false); \t\t\tbreak; \t\tcase Keys.RIGHT: \t\t\tbob.setRightMove(false); \t\t\tbreak; \t    } \t    return true;     }                  Prev       Next          ",
        
        "url": "/wiki/input/event-handling" },{
        "title": "External tutorials",
        "excerpt":
        
        "This page gives a list of some external tutorials, if our wiki here isn’t enough for you. If you’re looking for some representative games made with libGDX, have a look at our collection here. If you’re interested in some tools and frameworks to use with libGDX, this curated list may be of help. We also have a list of useful resources on our Discord server.   Preliminary note: Given the nature of this page’s content, it is likely that some of the links displayed here might become offline or even change. If you spot some broken link or wrong information please feel free to edit this page so we can keep our wiki as up-to-date as possible. Thank you.   General tutorial websites  These are webpages where individuals or companies write about their experience with libGDX from a technical perspective. Some of the tutorials present in these pages are also referenced below in more topic specific sections but if you’re just looking to some websites where people talk about their general experiences with libGDX you should start here.   Some one-page tutorials for beginners     Simple breakout game – focused on beginners with no prior game dev experience   Sprite Sheets &amp; Physics with Box2d – Tutorial by Code and Web   libGDX code samples (see the sidebar)   Some blog posts regarding libGDX – by Rotating Canvas   Complete tutorial series     GameDevelopment.blog – 17 parts (2017)   Flappy Bird Remake – 12 parts (2016?)   GameFromScratch.com – 17 parts (2014)   Video tutorials     By Brandon Grasely – 12 parts (2020)   By Coldwild Games – 27 parts (2017)   Asteroid game – by ForeignGuyMike (2014)   By dermetfan – 25 parts (2013)   Topic-specific tutorials  AdMob integration for libGDX Android projects     norakomi.com   Lighting     Normal Mapping   Shadow Mapping   UI with Scene2D     From the Ground Up Series by raeleus (recommended!)   javagamexyz   Video series by dermetfan   Performance-related     Image Formats: PNG, JPEG, or GIF?   A splash screen on application startup (example)   Showing a splash screen while doing heavy processing   Shaders     Matt DesLauriers has a very nice introduction to GLSL shaders (both libGDX specific and in general)   Video series by dermetfan   Online Shader Tools, which let you play with WebGL (provided your browser supports it) and see the code behind it:            https://www.shadertoy.com       https://glslsandbox.com           Tile-based games     Video tutorial series by dermetfan            gdx-tiled-preprocessor video tutorial (Warning: the gdx-tiled-preprocessor was merged into gdx-tools)           Some simple open-source projects for reference  These are some open-source projects found on the web that use libGDX and can be used as reference. This list is intentionally kept as clean and interesting as possible (you won’t see repeated projects nor Hello-World kind-of-games here).     Fire and Ice (uses tile maps)   On Guard (uses tile maps)   Lendigastel (3D; uses gdx-gltf)   libGDX vs Ray3K (local multiplayer Smash Bros. clone)   Super Spineboy (a platformer using Spine)   Klooni 1010! (puzzle game based on the original 1010!)   TDGalaxy (modular 3D tower defense game)   bombergame   martianrun (2D running game; uses AdMob and Google Play Game Services)   GdxCombat (2D Mortal Kombat-like game)   rpgboss-editor  ",
        
        "url": "/wiki/articles/external-tutorials" },{
        "title": "File handling",
        "excerpt":
        
        "   Introduction   Platform Filesystems   File (Storage) Types   Checking Storage availability and paths   Obtaining FileHandles   Listing and Checking Properties of Files   Error Handling   Reading from a File   Writing to a File   Deleting, Copying, Renaming and Moving Files/Directories   Introduction  libGDX applications run on four different platforms: desktop systems (Windows, Linux, macOS, headless), Android, iOS, and JavaScript/WebGL capable browsers. Each of these platforms handles file I/O a little differently. libGDX’s Files (code) module provides a common interface for all these platforms with the ability to:      Read from a file   Write to a file   Copy a file   Move a file   Delete a file   List files and directories   Check whether a file/directory exists   Before diving into the specifics of libGDX’s file handling, users should be aware of certain differences between the filesystems for all supported platforms:   Platform Filesystems  Desktop (Windows, Linux, Mac OS X, Headless)  On a desktop OS, the filesystem is one big chunk of memory. Files can be referenced with paths relative to the current working directory (the directory the application was executed in) or absolute paths. Ignoring file permissions, files and directories are usually readable and writable by all applications.   Android  On Android the situation is a little bit more complex. Files can be stored inside the application’s APK either as resources or as assets. These files are read-only. libGDX only uses the assets mechanism, as it provides raw access to the byte streams and more closely resembles a traditional filesystem. Resources better lend themselves to normal Android applications but introduce problems when used in games. Android manipulates them at load time, e.g. it automatically resizes images.   Assets are stored in your project’s assets directory and will be packaged with your APK automatically when you deploy your application. They are accessible via Gdx.files.internal, a read-only directory not to be confused with what the Android documentation refers to as “internal”. No other application on the Android system can access these files.   Files can also be stored on what the Android documentation refers to as internal storage (accessible via Gdx.files.local in LibGDX), where they are readable and writable. Each installed application has a dedicated internal storage directory. This directory is again only accessible by that application. One can think of this storage as a private working area for the application.   Finally, files can be stored on the external storage, accessible via Gdx.files.external in LibGDX. The behaviour regarding external files was changed in Android over the times, hence in LibGDX:      libGDX up to 1.9.11 uses the Android external storage directory. That is up to Android 4.3 the sd card directory, which might not always be available, and a virtual emulated sd card directory on later versions. For accessing these files, you need to add a permission to your AndroidManifest.xml file, see Permissions. From Android 6 on, even a runtime permission is needed to use the directory and starting from Android 11, access is forbidden completely for normal apps (if you want to publish on the Play Store).   libGDX 1.9.12 or later uses the App external storage directory. This directory (located at Android/data/data/your_package_id/) is readable and writable from your app without any further permission and changes. Other apps (like file managers) can access the files up to Android 10, from Android 11 on the directory is only accessible via USB access. Note: If the user uninstalls the app, the data saved here will be deleted if not copied to another location before by the user.   The App external storage is initialized at game start for you to use, therefore Android creates an empty directory. If you don’t use external files and want to suppress this behaviour, you can do so by overriding the instantiation of AndroidFiles in AndroidApplication#createFiles (1.9.14 and up):   \tprotected AndroidFiles createFiles() { \t\tthis.getFilesDir(); // workaround for Android bug #10515463 \t\treturn new DefaultAndroidFiles(this.getAssets(), this, false); \t}   iOS  On iOS all file types are available.   Javascript/WebGL  A raw Javascript/WebGL application doesn’t have a traditional filesystem concept. Instead, assets like images are referenced by URLs pointing to files on one or more servers. Modern browsers also support Local Storage which comes close to a traditional read/write filesystem. The problem with local storage is that the storage amount available by default is fairly small, not standardized, and there no (good) way to accurately query the quota. For this reason, the preferences API is currently the only way to write local data persistently on the JS platform.   libGDX does some magic under the hood to provide you with a read-only filesystem abstraction.   File (Storage) Types  A file in libGDX is represented by an instance of the FileHandle class. A FileHandle has a type which defines where the file is located. The following table illustrates the availability and location of each file type for each platform.                  Type       Description, file path and features       Desktop       Android       HTML5       iOS                       Classpath       Classpath files are directly stored in your source folders. These get packaged with your jars and are always read-only. They have their purpose, but should be avoided if possible.       Yes       Yes       No       Yes                 Internal       Internal files are relative to the application’s root or working directory on desktops, relative to the assets directory on Android, and relative to the core/assets/ directory of your GWT project. These files are read-only. If a file can’t be found on the internal storage, the file module falls back to searching the file on the classpath. This is necessary if one uses the asset folder linking mechanism of Eclipse, see Project Setup. Relative paths (./ or ../) are not always supported and thus shouldn’t be used.       Yes       Yes       Yes       Yes                 Local       Local files are stored relative to the application’s root or working directory on desktops and relative to the internal (private) storage of the application on Android. Note that Local and internal are mostly the same on the desktop.       Yes       Yes       No       Yes                 External       External files paths are relative to the home directory of the current user on desktop systems. On Android, the app-specific external storage is used.       Yes       Yes       No       Yes                 Absolute       Absolute files need to have their fully qualified paths specified.  Note: For the sake of portability, this option must be used only when absolutely necessary       Yes       Yes       No       Yes           Absolute and classpath files are mostly used for tools such as desktop editors, that have more complex file i/o requirements. For games these can be safely ignored. The order in which you should use the types is as follows:      Internal Files: all the assets (images, audio files, etc.) that are packaged with your application are internal files. If you use the Setup UI, just drop them in your Android project’s assets folder.   Local Files: if you need to write small files, e.g. save a game state, use local files. These are in general private to your application. If you want a key/value store instead, you can also look into Preferences. Note that Android’s app-specific cache can be accessed using ‘../cache’. Files stored there can be cleared by the user via the ‘clear cache’ button found in the app’s settings.   External Files: if you need to write big files, e.g. screenshots, or download files from the web, they could go on the external storage. Note that the external storage is volatile, a user can remove it or delete the files you wrote. Because they are not cleaned up and volatile, it is usually simpler to use local file storage.   Checking Storage availability and paths  The different storage types might not be available depending on the platform your application runs on. You can query this kind of information via the Files module:   boolean isExtAvailable = Gdx.files.isExternalStorageAvailable(); boolean isLocAvailable = Gdx.files.isLocalStorageAvailable();   You can also query the root paths for external and local storage:   String extRoot = Gdx.files.getExternalStoragePath(); String locRoot = Gdx.files.getLocalStoragePath();   Obtaining FileHandles  A FileHandle is obtained by using one of the aforementioned types directly from the Files module. The following code obtains a handle for the internal myfile.txt file.   FileHandle handle = Gdx.files.internal(\"data/myfile.txt\");   If you used the gdx-setup tool, this file will be contained in your project’s assets folder, /assets/data to be specific. Your desktop and html projects link to this folder in Eclipse, and will pick it up automatically when executed from within Eclipse.   FileHandle handle = Gdx.files.classpath(\"myfile.txt\");   The myfile.txt file is located in the directory where the compiled classes reside or the included jar files.   FileHandle handle = Gdx.files.external(\"myfile.txt\");   In this case, myfile.txt needs to be in the users’ home directory (/home/&lt;user&gt;/myfile.txt on Linux, /Users/&lt;user&gt;/myfile.txt on macOS and C:\\Users\\&lt;user&gt;\\myfile.txt on Windows) on desktop, and in the root of the SD card on Android.   FileHandle handle = Gdx.files.absolute(\"/some_dir/subdir/myfile.txt\");   In the case of absolute file handle, the file has to be exactly where the full path points. In /some_dir/subdir/ of the current drive on Windows or the exact path on linux, macOS and Android.   FileHandle instances are passed to methods of classes they are responsible for reading and writing data. E.g. a FileHandle needs to be specified when loading an image via the Texture class, or when loading an audio file via the Audio module.   Listing and Checking Properties of Files  Sometimes it is necessary to check for the existence of a specific file or list the contents of a directory. FileHandle provides methods to do just that in a concise way.   Here’s an example that checks whether a specific file exists and whether a file is actually a directory or not.   boolean exists = Gdx.files.external(\"doitexist.txt\").exists(); boolean isDirectory = Gdx.files.external(\"test/\").isDirectory();   Listing a directory is equally simple:   FileHandle[] files = Gdx.files.local(\"mylocaldir/\").list(); for(FileHandle file: files) {    // do something interesting here }   WARNING: If you don’t specify a folder the list will be empty.   Note: Listing of internal directories is not supported on Desktop. To work around this problem, you can generate a list of files before deployment.   We can also ask for the parent directory of a file or create a FileHandle for a file in a directory (aka “child”).   FileHandle parent = Gdx.files.internal(\"data/graphics/myimage.png\").parent(); FileHandle child = Gdx.files.internal(\"data/sounds/\").child(\"myaudiofile.mp3\");   parent would point to \"data/graphics/\", child would point to data/sounds/myaudiofile.mp3\".   There are many more methods in FileHandle that let you check for specific attributes of a file. Please refer to the Javadocs for detail.   Note: These functions are mostly unimplemented in the HTML5 back-end at the moment. Try not to rely on them too much if HTML5 will be a target of your application.   Error Handling  Some operations on FileHandles can fail. We adopted RuntimeExceptions to signal errors instead of checked Exceptions. Our reasoning goes like this: 90% of the time we will access files that we know exist and are readable (e.g. internal files packaged with our application).   Reading from a File  After obtaining a FileHandle, we can either pass it to a class that knows how to load content from the file (e.g. an image), or read it ourselves. The latter is done through any of the input methods in the FileHandle class. The following example illustrates how to load text from an internal file:   FileHandle file = Gdx.files.internal(\"myfile.txt\"); String text = file.readString();   If you have binary data, you can easily load the file into a byte array:   FileHandle file = Gdx.files.internal(\"myblob.bin\"); byte[] bytes = file.readBytes();   The FileHandle class has many more read methods. Check the Javadocs for more information.   Writing to a File  Similarly to reading files, FileHandle also provides methods to write to a file. Note that only the local, external and absolute file types support writing to a file. Writing a string to a file works as follows:   FileHandle file = Gdx.files.local(\"myfile.txt\"); file.writeString(\"My god, it's full of stars\", false);   The second parameter of FileHandle#writeString specifies if the content should be appended to the file. If set to false, the current content of the file will be overwritten.   One can of course also write binary data to a file:   FileHandle file = Gdx.files.local(\"myblob.bin\"); file.writeBytes(new byte[] { 20, 3, -2, 10 }, false);   There are many more methods in FileHandle that facilitate writing in different ways, e.g. using OutputStream. Again, refer to the Javadocs for details.   Deleting, Copying, Renaming and Moving Files/Directories  These operations are again only possible for writable file types (local, external, absolute). Note however, that the source for a copying operation can also be a read only FileHandle. A few examples:   FileHandle from = Gdx.files.internal(\"myresource.txt\"); from.copyTo(Gdx.files.external(\"myexternalcopy.txt\"));  Gdx.files.external(\"myexternalcopy.txt\").rename(\"mycopy.txt\"); Gdx.files.external(\"mycopy.txt\").moveTo(Gdx.files.local(\"mylocalcopy.txt\"));  Gdx.files.local(\"mylocalcopy.txt\").delete();   Note that source and target can be files or directories.   For more information on available methods, check the FileHandle Javadocs.  ",
        
        "url": "/wiki/file-handling" },{
        "title": "Firebase in libGDX",
        "excerpt":
        
        "If you are interested in using Firebase with libGDX, take a look at mk-5’s gdx-fireapp.   Setup  Add the following dependencies to the corresponding places in your build.gradle files. For further information check out the project’s extensive wiki: Android guide, iOS guide, GWT guide.   Core:  implementation \"pl.mk5.gdx-fireapp:gdx-fireapp-core:$gdxFireappVersion\"   Android:  implementation \"pl.mk5.gdx-fireapp:gdx-fireapp-android:$gdxFireappVersion\"   iOS:  implementation \"pl.mk5.gdx-fireapp:gdx-fireapp-ios:$gdxFireappVersion\"   GWT:  implementation \"pl.mk5.gdx-fireapp:gdx-fireapp-html:$gdxFireappVersion\"   Basics  Gdx-firebase is a bridge between a libGDX app and the Firebase SDK. It covers Firebase functionality, so if you have some knowledge of the Firebase SDK, using gdx-firebase’s API should be intuitive.   To initialize the library, just put this line somewhere in your app’s initialization code:   GdxFIRApp.inst().configure();   Firebase Analytics should start working just after this step.   Examples  The project’s wiki provides various examples. Check them out here.  ",
        
        "url": "/wiki/third-party/firebase-in-libgdx" },{
        "title": "Frame buffer objects",
        "excerpt":
        
        "Framebuffers allow users to render stuff to a texture instead of to the screen (= the backbuffer). This can prove useful in various ways, for example to perform post processing effects.   How to use framebuffers   Let’s create a FrameBuffer:   FrameBuffer fbo = new FrameBuffer(Format.RGBA8888, 1024, 720, false);   Now, to render anything to the framebuffer, it has to be bound (via begin() and end()):   fbo.begin(); Gdx.gl.glClearColor(1, 1, 1, 1); Gdx.gl.glClear(GL20.GL_COLOR_BUFFER_BIT);  batch.begin(); batch.draw(anyCoolSprite, 0, 0); batch.end();  fbo.end();   To retrieve the stuff rendered to the framebuffer, one has to call getColorBufferTexture():   Texture texture = fbo.getColorBufferTexture(); TextureRegion textureRegion = new TextureRegion(texture); textureRegion.flip(false, true);   As you can see above, the texture is flipped via use of a TextureRegion. This is done, because the framebuffer textures are generally upside-down.   If you just want to draw the framebuffer’s texture on screen, you can also use this to flip the texture:  batch.draw(fbo.getColorTexture(), x, y, w, h, 0, 0, 1, 1)   Common Issues  Nesting  Please note, that the default framebuffers of libGDX cannot be nested (i.e. used inside of each other). This is due to FrameBuffer.end() always binding the back buffer, even if it wasn’t the previously bound buffer. A detailed description of this can be found here. To work around this problem, FrameBuffer can be extended and the end() method overridden. If you don’t want to implement this yourself, there are some community options.   Hdpi  If you’re having an hdpi display, rendering scene2d stuff inside of a framebuffer causes problems with clipping, which is used, for example, in dialogs. To fix this, either set the size of your framebuffer to the real pixel size (instead of the logical size):   FrameBuffer fbo = new FrameBuffer(Format.RGBA8888, HdpiUtils.toBackBufferX(currentWidth), HdpiUtils.toBackBufferY(currentHeight), false);   or temporarily switch the hdpi mode:   HdpiUtils.setMode(HdpiMode.Pixels); fbo.begin(); stage.draw(); fbo.end(); HdpiUtils.setMode(HdpiMode.Logical);   Further resources     A LWJGL tutorial regarding framebuffers (has a libGDX version of the used code as well)  ",
        
        "url": "/wiki/graphics/opengl-utils/frame-buffer-objects" },{
        "title": "Gdx freetype",
        "excerpt":
        
        "Introduction   If you want to draw text in your game, you usually use a BitmapFont. However, there is a downside:      BitmapFonts rely on an image, so you have to scale them if you want a different size, which may look ugly.   You could just save a BitmapFont of the biggest size needed in your game then and you never have to scale up, just down, right? Well, that’s true, but downscaling by large amounts can either look aliased or slightly blurry, depending if mipmapping is used. Distance field fonts aim to solve this, but that’s not what this page is about!   A BitmapFont can also take up more storage space than the corresponding TrueType Font (.ttf), though whether these fonts would require more space than gdx-freetype itself depends on your game and target platform.   The solution to your problem is the gdx-freetype extension:     ship only lightweight .ttf files with your game   generate a BitmapFont of your desired size on the fly   user might put their own fonts into your game   Tutorial available on LibGDX.info   Details   Since this is an extension, it is not included in your libGDX project by default. How you add the extension differs based on the setup of your project.   How to put gdx-freetype in your project   For projects using Gradle   For new projects, simply select the Freetype option under extensions in the setup UI.   To add to an existing Gradle project, see Dependency management with Gradle.   HTML5   gdx-freetype is not compatible with HTML5. However, you may use the gdx-freetype-gwt library by Intrigus to enable HTML5 functionality. Version 1.9.10.1 remains compatible with never versions of libGDX, including 1.10.0.   How to use gdx-freetype in code   Using the gdx-freetype extension in your code is really simple.   FreeTypeFontGenerator generator = new FreeTypeFontGenerator(Gdx.files.internal(\"fonts/myfont.ttf\")); FreeTypeFontParameter parameter = new FreeTypeFontParameter(); parameter.size = 12; BitmapFont font12 = generator.generateFont(parameter); // font size 12 pixels generator.dispose(); // don't forget to dispose to avoid memory leaks!  A much simpler way to display your font is by placing your font file in project’s assets folder. You will have to modify the first line of the above code and mention just your font file name in the parameters.   The defaults for the FreeTypeFontParameter:  /** The size in pixels */ public int size = 16; /** Foreground color (required for non-black borders) */ public Color color = Color.WHITE; /** Border width in pixels, 0 to disable */ public float borderWidth = 0; /** Border color; only used if borderWidth &gt; 0 */ public Color borderColor = Color.BLACK; /** true for straight (mitered), false for rounded borders */ public boolean borderStraight = false; /** Offset of text shadow on X axis in pixels, 0 to disable */ public int shadowOffsetX = 0; /** Offset of text shadow on Y axis in pixels, 0 to disable */ public int shadowOffsetY = 0; /** Shadow color; only used if shadowOffset &gt; 0 */ public Color shadowColor = new Color(0, 0, 0, 0.75f); /** The characters the font should contain */ public String characters = DEFAULT_CHARS; /** Whether the font should include kerning */ public boolean kerning = true; /** The optional PixmapPacker to use */ public PixmapPacker packer = null; /** Whether to flip the font vertically */ public boolean flip = false; /** Whether or not to generate mip maps for the resulting texture */ public boolean genMipMaps = false; /** Minification filter */ public TextureFilter minFilter = TextureFilter.Nearest; /** Magnification filter */ public TextureFilter magFilter = TextureFilter.Nearest;   If rendering large fonts, the default PixmapPacker page size may be too small. You can provide your own PixmapPacker or use FreeTypeFontGenerator.setMaxTextureSize to set the default page size.   You may want to generate your font in your game’s resize () event to handle different window resolutions without scaling, being sure to dispose of old BitmapFonts. Your font size should be limited to what your page size can handle, especially on gdx-freetype-gwt, to avoid glitched fonts and crashes.   Examples   parameter.borderColor = Color.BLACK; parameter.borderWidth = 3;     parameter.shadowColor = Color.BLACK; parameter.shadowOffsetX = 3; parameter.shadowOffsetY = 3;     You can also load BitmapFonts generated via the FreeType extension using AssetManager. See FreeTypeFontLoaderTest   Caveats   Quoting from https://web.archive.org/web/20201128081723/https://www.badlogicgames.com/wordpress/?p=2300:     Asian scripts “might” work, see caveat above though. They contain just too many glyphs. I’m thinking about ways to fix this.   Right-to-left scripts like Arabic are a no-go. The layout “algorithms” in BitmapFont and BitmapFontCache have no idea how to handle that.   Throwing just any font at FreeType is not a super awesome idea. Some fonts in the wild are just terrible, with bad or no hinting information and will look like poopoo.     Download an example  ",
        
        "url": "/wiki/extensions/gdx-freetype" },{
        "title": "gdx pay",
        "excerpt":
        
        "Gdx-Pay: cross-platform In-App purchasing  Gdx-Pay aims to provide a cross-platform API for InApp purchasing. The gdx-pay project is a libGDX extension. Please refer to:     the subproject gdx-pay   the gdx-pay-Wiki.  ",
        
        "url": "/wiki/extensions/gdx-pay" },{
        "title": "Gesture detection",
        "excerpt":
        
        "Gesture Detection  Touch screens lend themselves well to gesture based input. A gesture could be a pinch with two fingers to indicate the desire to zoom, a tap or double tap, a long press and so on.   libGDX provides a GestureDetector (source) that lets you detect the following gestures:      touchDown: A user touches the screen.   longPress: A user touches the screen for some time.   tap: A user touches the screen and lifts the finger again. The finger must not move outside a specified square area around the initial touch position for a tap to be registered. Multiple consecutive taps will be detected if the user performs taps within a specified time interval.   pan: A user drags a finger across the screen. The detector will report the current touch coordinates as well as the delta between the current and previous touch positions. Useful to implement camera panning in 2D.   panStop: Called when no longer panning.   fling: A user dragged the finger across the screen, then lifted it. Useful to implement swipe gestures.   zoom: A user places two fingers on the screen and moves them together/apart. The detector will report both the initial and current distance between fingers in pixels. Useful to implement camera zooming.   pinch: Similar to zoom. The detector will report the initial and current finger positions instead of the distance. Useful to implement camera zooming and more sophisticated gestures such as rotation.   A GestureDetector is an event handler in disguise. To listen for gestures, one has to implement the GestureListener interface and pass it to the constructor of the GestureDetector. The detector is then set as an InputProcessor, either on an InputMultiplexer or as the main InputProcessor:   public class MyGestureListener implements GestureListener{     \t@Override    \tpublic boolean touchDown(float x, float y, int pointer, int button) { \t\t   \t \t   \treturn false;    \t} \t   \t \t@Override \tpublic boolean tap(float x, float y, int count, int button) { \t\t\t \t\treturn false; \t} \t\t \t@Override \tpublic boolean longPress(float x, float y) { \t\t\t \t\treturn false; \t} \t\t \t@Override \tpublic boolean fling(float velocityX, float velocityY, int button) { \t\t\t \t\treturn false; \t} \t\t \t@Override \tpublic boolean pan(float x, float y, float deltaX, float deltaY) { \t\t\t \t\treturn false; \t} \t\t \t@Override \tpublic boolean panStop(float x, float y, int pointer, int button) { \t\t\t \t\treturn false; \t} \t\t    \t@Override    \tpublic boolean zoom (float originalDistance, float currentDistance){ \t   \t\t \t   return false;    \t}     \t@Override    \tpublic boolean pinch (Vector2 initialFirstPointer, Vector2 initialSecondPointer, Vector2 firstPointer, Vector2 secondPointer){ \t   \t\t \t   return false;    \t}    \t@Override \tpublic void pinchStop () { \t} }   Gdx.input.setInputProcessor(new GestureDetector(new MyGestureListener()));   The GestureListener can signal whether it consumed the event or wants it to be passed on to the next InputProcessor by returning either true or false respectively from its methods.   As with the events reported to a normal InputProcessor, the respective methods will be called right before the call to ApplicationListener.render() on the rendering thread.   The GestureDetector also has a second constructor that allows it to specify various parameters for gesture detection. Please refer to the Javadocs for more information.                  Prev       Next          ",
        
        "url": "/wiki/input/gesture-detection" },{
        "title": "Getting Help",
        "excerpt":
        
        "The libGDX community is glad to help you when you get stuck or encounter a bug, but we need your help to make it as easy to help you as possible.   Contents      Helping Yourself   Help Us Help You   Title   Context   Relevance   Problem Statement   Exceptions   Code Snippets   Executable Example Code            Example Resources       Barebones Application       Barebones SpriteBatch       Barebones Stage           Actually Executable   Attitude   Formatting   Helping Yourself   Please go through this short checklist to be sure you haven’t missed an easy to find solution.      Are you using the latest nightly build? You can do so by changing gdxVersion in build.gradle to the snapshot version listed on GitHub. Please try that first, as issues are being fixed every single day.   Have you read the documentation on the Table of Contents? It can also be very helpful to look at the Javadocs and source code (don’t be shy!). Search the tests for a specific class to find example code.   Have you searched on our Discord server for your problem?   Have you searched the issue tracker for your problem? Be sure to search “All issues”, not just “Open issues”.   If you still have a problem, the way to get help is the libGDX Discord. There are a lot of active users, any one of which could answer your question right now.   Otherwise, if you wish to post a new issue on the tracker, keep reading.   Help Us Help You  If you believe your issue, error, or suspected bug is related to a specific backend, please present the following information with your issue. If you are on the Discord/IRC have the following information on hand.   For Android backend issues          Note: Android issues can sometimes be more difficult due to device manufactures breaking things or buggy drivers.            Note 2: libGDX only works completely on the Android Runtime (ART) on devices running the Android L developer preview and higher. Some functions will not work on the ART builds in 4.4.x due to an issue in ART that was fixed and included in the L developer preview.            Please have the device name and Android version in the bug report. Providing things won’t be broken, we will make an attempt to fix the issue or implement a workaround for the device.       For Desktop backend issues (LWJGL 2 &amp; 3)     Please list the operating system and version, architecture, and if necessary OpenGL version.   Also mark specifically which of those backends have this issue.   For iOS (RoboVM) backend issues     Please list the iOS version, and device the issue occurs on   For GWT (WebGL) backend issues     Please list the operating system and version, and architecture.   Please list the browser and browser version Listing this information can greatly reduce the workload on us and can greatly increase the chances your issue will be resolved or an available fix or workaround implemented.   Title   Write a clear and short title. Titles that do not describe the issue (such as “please help”) or contain all caps, exclamation marks, etc. make it much less likely that your issue will be read.   Context   Describe what you are trying to achieve. If it might help your question get answered, also explain the reasons why. If specific solutions are unacceptable, list them and why.   Try to keep the information relevant. If you aren’t sure, include the extra information but if your text gets very long, provide an executive summary separate from the rest.   Problem Statement   Concisely describe the problem. Describe each approach you have tried and, for each of those, explain what you expected and what actually happened.   If you fail to do this, likely you will be ignored. No one wants to guess what your problem is and often they don’t have the time or patience to ask for the information you should have included from the start.   Exceptions   If an exception occurred, include the full exception message and stack-trace.   Often the first line number just after the deepest (nearest to the bottom) exception message is most relevant. If this line is in your code, along with the full exception message and stack-trace you should include your code for this line and 1-2 surrounding lines.   Exception in thread \"LWJGL Application\" com.badlogic.gdx.utils.GdxRuntimeException: java.lang.NullPointerException \tat com.badlogic.gdx.backends.lwjgl.LwjglApplication$1.run(LwjglApplication.java:111) Caused by: java.lang.NullPointerException \tat com.badlogic.gdx.scenes.scene2d.ui.Button.setStyle(Button.java:155) &lt;- **MOST IMPORTANT LINE** \tat com.badlogic.gdx.scenes.scene2d.ui.TextButton.setStyle(TextButton.java:55) \tat com.badlogic.gdx.scenes.scene2d.ui.Button.&lt;init&gt;(Button.java:74) \tat com.badlogic.gdx.scenes.scene2d.ui.TextButton.&lt;init&gt;(TextButton.java:34) \tat com.badlogic.gdx.backends.lwjgl.LwjglApplication.mainLoop(LwjglApplication.java:125) \tat com.badlogic.gdx.backends.lwjgl.LwjglApplication$1.run(LwjglApplication.java:108)   Code Snippets   Code snippets are most often not very useful. Unless you are blatantly misusing the API, most problems cannot be solved just by looking at a code snippet. Code snippets mostly lead to vague guesses at what might be wrong instead of a real answer to your question. Instead, include executable example code.   Executable Example Code   Example code that can be copied, pasted, and run is the best way to get help. It saves those helping you time because they can see the problem right away. They can quickly fix your code or fix the bug, verify the fix, and show you the result. No matter what, an executable example has to be written to properly test, fix, and verify the fix. If you can’t debug and fix the problem yourself, you can still help by providing the executable example.   Creating executable example code does take some time. You need to take apart your application and reconstruct the relevant parts in a new, barebones application that shows the problem. Quite often just by doing this you will figure out the problem. If not, you will get help very quickly and the people helping you will have more time to help more people.   Example code should be contained entirely in a single class (use static member classes if needed) and executable, meaning it has a main method and can simply be copied, pasted, and run. Do not use a GdxTest, as that cannot be copy, pasted, and run.   For more about how to make executable example code, please see SSCCE and MCVE.   Example Resources   Often executable examples need some resources, such as an image or sound file. It is extra work for those trying to help if they must download your specific resources. Instead, it is ideal to use resources from the libgdx tests. This enables your example code to be simply pasted into the gdx-tests-lwjgl project and run.   The easiest way to write an executable example is to paste one of the barebones applications below into the gdx-tests-lwjgl project and then modify it to show your problem, using only the test resources. Note the test resources are pulled in by gdx-tests-lwjgl from the gdx-tests-android project.   Barebones Application   Below is a simple, barebones, executable application. This can be used as a base for creating your own executable example code.   import com.badlogic.gdx.*; import com.badlogic.gdx.backends.lwjgl.LwjglApplication; import com.badlogic.gdx.graphics.GL20;  public class Barebones extends ApplicationAdapter { \tpublic void create () { \t\t// your code here \t}  \tpublic void render () { \t\tGdx.gl.glClear(GL20.GL_COLOR_BUFFER_BIT); \t\t// your code here \t}  \tpublic static void main (String[] args) throws Exception { \t\tnew LwjglApplication(new Barebones()); \t} }   Barebones SpriteBatch   This barebones application uses SpriteBatch to draw an image from the gdx-tests-lwjgl project.   import com.badlogic.gdx.*; import com.badlogic.gdx.backends.lwjgl.LwjglApplication; import com.badlogic.gdx.graphics.*; import com.badlogic.gdx.graphics.g2d.SpriteBatch;  public class BarebonesBatch extends ApplicationAdapter { \tSpriteBatch batch; \tTexture texture;  \tpublic void create () { \t\tbatch = new SpriteBatch(); \t\ttexture = new Texture(Gdx.files.internal(\"data/badlogic.jpg\")); \t}  \tpublic void render () { \t\tGdx.gl.glClear(GL20.GL_COLOR_BUFFER_BIT); \t\tbatch.begin(); \t\tbatch.draw(texture, 100, 100); \t\tbatch.end(); \t}  \tpublic static void main (String[] args) throws Exception { \t\tnew LwjglApplication(new BarebonesBatch()); \t} }   Barebones Stage   This barebones application has a scene2d Stage and uses scene2d.ui to draw a label and a button. It uses the Skin from the gdx-tests-lwjgl project.   import com.badlogic.gdx.*; import com.badlogic.gdx.backends.lwjgl.LwjglApplication; import com.badlogic.gdx.graphics.GL20; import com.badlogic.gdx.scenes.scene2d.Stage; import com.badlogic.gdx.scenes.scene2d.ui.*; import com.badlogic.gdx.utils.viewport.*;  public class BarebonesStage extends ApplicationAdapter { \tStage stage;  \tpublic void create () { \t\tstage = new Stage(new ScreenViewport()); \t\tGdx.input.setInputProcessor(stage);  \t\tSkin skin = new Skin(Gdx.files.internal(\"skin.json\")); \t\tLabel label = new Label(\"Some Label\", skin); \t\tTextButton button = new TextButton(\"Some Button\", skin);  \t\tTable table = new Table(); \t\tstage.addActor(table); \t\ttable.setFillParent(true);  \t\ttable.debug(); \t\ttable.defaults().space(6); \t\ttable.add(label); \t\ttable.add(button); \t}  \tpublic void render () { \t\tGdx.gl.glClear(GL20.GL_COLOR_BUFFER_BIT); \t\tstage.draw(); \t}  \tpublic void resize (int width, int height) { \t\t// Pass false to not modify the camera position. \t\tstage.getViewport().update(width, height, true); \t}  \tpublic static void main (String[] args) throws Exception { \t\tnew LwjglApplication(new BarebonesStage()); \t} }   Actually Executable   If your executable example cannot be pasted into the gdx-tests-lwjgl project and run, then it is not actually an executable example. Others should not have to fix up your code to run it, not even to add a main method.   Attitude   Begging for help or a quick answer tends to turn people off and makes it less likely you will receive help at all. Just be polite and your question will get answered politely as time allows. If you are rude, you will be ignored or met with rudeness in return. The people helping you are busy and providing you help for free simply because they are nice. They don’t owe you anything and they don’t have to care about you or your problems.   Formatting   If you spend a little bit of your time to format your post nicely, it is more likely others will spend their time responding to your post. This means capital letters where appropriate, paragraphs to separate ideas, use actual words (rather than “u”, “bcoz”, etc), put code in code blocks, etc. If English is not your first language, we understand. No need to apologize, just do your best to make an effort.  ",
        
        "url": "/wiki/articles/getting-help" },{
        "title": "Getting ready for #libGDXJAM",
        "excerpt":
        
        "The #libGDXJAM was organized by the libGDX community and took place in December 2015. You can find all the entries here.   This article will take you through the rules and help you get ready to jam!   The 10 Rules of Jamming   The jam will be held from December 18th to January 18th. Here are the rules:      You must use libGDX to create a game that fits the theme.   You may work alone or in a team. Only one submission per person/team is allowed.   You may use pre-existing code, e.g. libraries like Ashley, or your own code libraries.   You may use pre-existing art, e.g. assets from OpenGameArt, or your own art.   You may use external tools like Tiled or Overlap2D.   You must not re-skin an already existing game or prototype!   You must submit your game before the end of the 18th of January via the jam’s site on itch.io (to be made public).   You must publish the source of your game, e.g. to GitHub.   You must submit your game to the itch.io libGDX Jam page before the end of day January 18th, UTC-12!   If you want to win one of the sponsored prizes, you must tweet about your game and document its development, using the hashtag #libGDXJam and the handles @robovm and @robotality.   First of all, you can participate in the jam without following these rules! In that case, you will not qualify for the prizes though.   Documenting your progress is a great way of sharing your experience, and an invaluable tool for others to learn. Making a bit of noise on Twitter is also a great way to give back to our sponsors. Chaining those 2 things together via rule 9 is my evil overlord plan to make everyone happy.   Here are a few examples of tweets:      Progress screenshot of my #libGDXJam entry  @robovm @robotality       New dev log entry for my #libGDXJam game  @robovm @robotality    For the dev logs, we want quality first and foremost! Progress screenshots, descriptions of problems you ran into and their solutions, streaming and so on is what we want to see! Just mindless spamming will not get you anywhere.   Prizes &amp; Judging   We are happy to have RoboVM and Robotality as sponsors for the following prizes:      Grand Prize: Mac Mini, sponsored by RoboVM.   Silver: iPad, sponsored by RoboVM.   Bronze: iPod Touch, sponsored by RoboVM.   For 20 random submissions: Steam keys for Halfway, sponsored by Robotality.   For another 5 random submissions: libGDX Jam t-shirt.   To qualify for any of the prizes, you’ll need to follow rule 10 as outlined above. Judging works as follows:      The community can vote on itch.io from the 19th of January to the 2nd of February.   The Grand Prize will be awarded to the entry with the highest community votes on itch.io. This way the highest quality entry will win!   The Silver and Bronze prizes will be awarded to the entries with the best mixture of dev logs and tweets and community votes. * Our sponsors and the libGDX core team will pick these entries. This should motivate people to make some noise on the web and document their progress for the greater good of the community!   The random awards guarantee that everyone has a chance to win a prize!   The winners will be announced on the 3rd of February!   Timetable      Theme Voting round 1: Nov. 22nd – Dec. 11th   Final Theme Voting: Dec. 11th – Dec. 18th   Jam: Dec. 18th – Jan. 18th   Judging: Jan 19th – Feb. 2nd   Survival guide   All hail ye who are brave enough to take up on the #libGDXJam challenge, have you done all the preparations prior the competition?      What preparation?    Make sure you have decided on:      Libraries: obviously Libgdx, but… Are you using any extensions or third party libraries?   Programming language and IDE: Java, Kotlin, Clojure or Scala? Eclipse, Intellij, Netbeans, Sublime, Vim, Emacs?   Graphic editors   Level editors   Sound generators and editors   Golden advice: use what you already know. If you experiment too much, you will most likely not finish a game on time!   Tools &amp; Resources   Audio   BFXR: web based sound effect generator.      Audiotool: web based music synthesizer.      Audacity: open source audio editor.      Free sound effects and music:      FreeSound   SoundCloud   Open Music Archive   CC Mixter   Graphics   Gimp: open source image editor.      Paint.NET: open source image editor.      Inkscape: open source vector editor.      Spine: skeletal 2D animation tool.      Blender: open source 3D editor.      Free game art:      Open Game Art   Kenney   Pixabay   Game Art 2D   Map editors   Tiled Map Editor: open source tile based editor.      Overlap2D: open source game editor. Make sure to check the Overlap2D survival guide for #libGDXJAM.      BDX: Open source 3D game engine integrated with Blender.      Team structure   Gather your quest party and put your specialist hats on!   Programmers      Do the programmy bits   Split tasks among them: graphics, controls, physics, UI   The less code overlap, the easier!   Need to tell the artists what formats they need   Need to define how level designers create content   Graphics and audio artists      Do the artsy bits   Need to spit up tasks among them: UI, background, characters, effects…   Agree on a consistent style   May need to create placeholder art early on   Game and Level designer      Does the content bits   Needs to define the game mechanics   Needs to define the game progression   Needs to create levels   Needs to playtest and give feedback to programmers and artists   Coordinator      Makes sure everyone knows what to do   Keeps track of things to be done   Keeps track of dependencies between members   Keeps track of time   Keeps track of human needs (food, sleep)   Note: depending on your actual team, some roles could be blurred or shared.     No artists? Use pre-existing art.   No designers? Everybody becomes a games designer.   No coordinator? Pick one person.   Alone? You do everything!   The 5 phases of Jamming   Now let’s get onto it!   Brainstorming   Goals:     Consider the set theme.   Get a high-level understanding of your game: genre, mechanics, setting, story, art style…   Take time limits into account. FPS, MMO and RTS games might not be your best choices.   Think outside the box.   To-do:     Gather ideas from everyone.   Pick the most promising ones via vote.   Define genre and mechanics using pen and paper.   Define setting and story.   Define art style, artist could draw quick mockups.   Setup   Goals:     Get a detailed understanding of your game:            What will programmers have to do?       What will artists have to do?       What will game designers have to do?           Define interfaces between all team members            How do programmers work with each other?       How do artists get their art into the game?       How do game designers create game content?           Define tasks and their order for every team member!            The coordinator is responsible for keeping track of tasks           To-do     Programmers agree on platform and tools   Artists agree on style   Programmers and artists agree on how to get art into the game   Programmers and game designers agree on how to create content   Each subteam defines their initial task   Coodinator keeps track of things   A super lightweight Kanban-like board can help, such as Trello.   Implementation   Goals:     Get the damned game done!   Ensure to have a playable prototype early            Prioritize tasks accordingly       Game mechanics first to see if they are fun!           Realize you’ll likely not get everything done!            Which is why you should have something playable at all times       Cut corners, kill features, focus on the core of your game           To-do     Every sub-team works on their task   Coordinator keeps track of progress   Sub-teams talk whenever they need to re-define or prioritize new tasks   Goto 1   Have something playable early on   Tips for programmers:     Use source control (git, SVN…), do not use shared drives, zip files nor e-mail   Don’t code for re-use   Don’t optimize   Try to create a modular-design so people don’t depend on each other too much   Make sure the game designer can create content as early as possible   Make sure artists export to easy to use formats   Make sure artists and game designers understand limitations   Tips for artists:     Make it easy to export your art into the proper format   Make sure everyone uses the same coordinate system/resolution   Use descriptive names for files   Have one shared folder (Dropbox, Google Drive) containing assets ready to integrate into the game            Don’t put multiple versions of the same thing there       Have whatever local folder structure for work in progress assets           Tips for game designers     Talk to the developers about what’s possible and what not   Focus on simple mechanics but try to put in a twist   Favor simple level-design over brainy or complex levels, they take too long to design!   If you have down-time, try to help or be the coordinator   Tips for coordinators     Ensure that everyone can stay busy, gather them to discuss/re-prioritize current tasks   Check on progress regularly, if something takes too long, ask the team to kill the feature   Make sure everybody is reminded they are human. They need to take breaks, sleep and eat   If you have down time, pick a task you can do!   Finishing up!   Goals:     Submit a playable game before the deadline!   To-do:     Feature freeze 2-3 hours before the deadline   Create a build for submission   Get the team together and decide what to polish in the remaining hours   If polish works out, create a new build for submission  ",
        
        "url": "/wiki/misc/getting-ready-for-libgdxjam" },{
        "title": "Google Mobile Ads in libGDX (replaces deprecated AdMob)",
        "excerpt":
        
        "The number one ad service being used by Android and libGDX developers at the moment is Google AdMob.   If you’ve not updated your app recently you should consider doing so soon. Google says:      Android (6.4.1 and earlier SDKs) Deprecated. On August 1, 2014, Google Play will stop accepting new or updated apps that use the old standalone Google Mobile Ads SDK v6.4.1 or lower. You must upgrade to the Google Play version of the Mobile Ads SDK by then.    Ok, so we want to migrate to the new Google Play Services way of doing things - this wiki post walks you through the process :)     Barebones Sample App   I created a new libGDX project using gdx-setup-ui.jar, added a .gitignore file, and made my initial commit.   Eclipse Setup   In eclipse, import the barebones sample app (file &gt; import &gt; existing projects into workspace) - you should now have at least three projects in package explorer (core, android, and desktop).   Open the Android SDK Manager, download the latest SDK Platform and Google APIs (at time of writing: 4.4.2/API19), the 2.3.1/API9 SDK Platform, and from Extras - Google Play Services.   Locate the /extras/google/google_play_services/libproject/google-play-services_lib/ directory on your machine (on my windows machine - C:\\Program Files (x86)\\Android\\android-sdk\\extras\\google\\google_play_services\\libproject\\google-play-services_lib) and copy into your working directory alongside the existing libGDX projects.   File &gt; Import &gt; Android &gt; Existing Android Code, Next, Browse, navigate to the local copy of google-play-services_lib in your working directory, Ok, Finish.   Right-click your android project, select Properties, Android, scroll down and click Add, select the google-play-services_lib project, Ok.   A refresh and clean in eclipse probably wouldn’t hurt at this point, so go ahead and do that.   AndroidManifest.xml   Ensure that the target in android project’s project.properties file is at least 13, and the android:minSdkVersion in your AndroidManifest.xml is at least 9. Sadly this does mean users running ancient versions of Android will be excluded, but there’s nothing we can do about this. There are very very VERY few devices still running versions below 2.3/API9, so at least you won’t be excluding many users…   Add these two lines as children of the ‘application’ element:   &lt;meta-data android:name=\"com.google.android.gms.version\" android:value=\"@integer/google_play_services_version\"/&gt;   &lt;activity android:name=\"com.google.android.gms.ads.AdActivity\" android:configChanges=\"keyboard|keyboardHidden|orientation|screenLayout|uiMode|screenSize|smallestScreenSize\"/&gt;   Add these two permissions as children of the ‘manifest’ element:   &lt;uses-permission android:name=\"android.permission.INTERNET\"/&gt;   &lt;uses-permission android:name=\"android.permission.ACCESS_NETWORK_STATE\"/&gt;   Save changes, then refresh and clean in eclipse for good luck…   Banner Ad   See this version of the android project’s MainActivity class for a reasonably straightforward banner ad implementation.   Interstitial Ad   This diff shows an interstitial ad implementation (ActionResolver interface lets us trigger interstitial actions from the core project while retaining the invaluable libGDX cross-platform functionality).     That’s all there is to it!   One final note if cloning from https://github.com/TheInvader360/tutorial-libgdx-google-ads, pay attention to the problems view in eclipse! You will need to create an empty ‘gen’ directory in both the google-play-services_lib and tutorial-libgdx-google-ads-android projects, and ensure you have the required android sdks installed. As is often the case with eclipse, a liberal amount of refreshing and cleaning will do no harm…  ",
        
        "url": "/wiki/third-party/google-mobile-ads-in-libgdx" },{
        "title": "Google Play Games Services in libGDX",
        "excerpt":
        
        "Google Play Games Services offers social leaderboards, achievements, and much more (realtime multiplayer, cloud saves, anti-piracy…)   Cross-platform features are deprecated, for new projects it is recommended to use on Android only.   Using open-source libGDX Extension  Google Play Games support for libGDX games on Android and Desktop is provided by gdx-gamesvcs libGDX extension. The extension also provides implementations for other game services (Apple Game Center, GameJolt and others).   Follow the readme and the project’s wiki to integrate GPGS and other game services in your project.   Manual integration in your project   The following article describes the manual integration in your project without using the library.   A Super Jumper based example that makes use of Leaderboards and Achievements is available to download from Google Play.   The project is freely available on GitHub, and a companion tutorial is available here.   Another in-depth LibGDX-based tutorial for adding Google Play Game Services can be found here.   Latest tutorial using Android Studio can be found here   Intellij and Android Studio Setup      Install Google Play Service and Google Play Repository using and Android SDK   To do that on Android Studio, Open up SDK Manager, ( Click the button next to the AVD manager in the top toolbar ) click Launch Standalone SDK Manager Scroll down to the Extras section and make sure these 2 packages are installed and updated to the latest :     Google Play services   Google Repository           Download BaseGameUtils sample project here, copy folder BaseGameUtils, located in folder BasicSamples into your project root folder.       Edit settings.gradle     include 'desktop', 'android', 'ios', 'html', 'core', \"BaseGameUtils\"           Edit root build.gradle and add the below as android dependency:     compile project(\":BaseGameUtils\")           in your AndroidManifest.xml file      add two permissions: ```       * add to your application tag     6. string.xml  in your android project, under 'res'-&gt;'values', in file `strings.xml` add app_id as follow, where 123456789 is your app ID in as declared in the Google Play Developer Console.   &lt;?xml version=”1.0” encoding=”utf-8”?&gt;     sample_ios_google_signin   123456789   6. build.gradle in Android project  Synchronize with Gradle. you will get the following message:   Error:Execution failed for task ‘:android:processDebugManifest’.     Manifest merger failed : uses-sdk:minSdkVersion 9 cannot be smaller than version 15 declared in library [libgdx-GPGS:BaseGameUtils:1.0] ```    edit and set minSdkVersion to the version number in the message above (in this case ‘15’)   iOS integration   Google Play Games’ iOS support is deprecated and shouldn’t be implemented in new games.   There are two ways (called Backend) to integrate Google Play Games Services with iOS depending whether your are using the open source, community supported Mobidevelop’s RoboVM and its Robopods or Intel’s Multi-OS Engine   Mobidevelop’s RoboVM and its Robopods   Please read the specific page for more information on Mobidevelop’s RoboVM and its Robopods   Warning:You can no longer create new Google Play Game Services accounts with iOS. There is a Simple LIBDX Google Play Games Services integration for iOS but it explains why the new version of Google SDK does not allow iOS users to create a GPGS account from iOS (not a libGDX issue).   note: Until early 2016, libGDX iOS integration was achieved using RoboVM.com. This has been deprecated. Be careful when you check examples on the Internet as older examples may be based on this version. The easy way to find out which version is being referred to:     Supported version will have com.mobidevelop.robovm in the buid.gradle file   Deprecated examples will have org.robovm:robovm in the buid.gradle file   Intel’s Multi-OS Engine   Checkout the specific page for more information on Intel’s Multi-OS Engine.   In the meantime, you can check out the following sample to get you started:   Splinter Sweets is a Kotlin based example that makes use of Leaderboards. It is available on Android and iOS (Gamecenter and Google Play Services integration).   ",
        
        "url": "/wiki/third-party/google-play-games-services-in-libgdx" },{
        "title": "Google Summer of Code 2014",
        "excerpt":
        
        "This page was a RFC for the GSoC 2014.   Introduction  We assume you already know what Google Summer of Code is, if not, please check out the Google Summer of Code site for general information.   For students, we recommend to read up on what libGDX is, then proceed to the Information For Students section.   Mentors must read the Information for Mentors  section, add themselves to existing ideas in the list below, and optionally create and add new ideas.   What is libGDX?   libGDX is a cross-platform game development framework. You write your game in Java, and have it working instantly on Windows, Linux, Mac, Steam, Android, iOS, Facebook and HTML/WebGL. The entire Java code base is shared across all these platforms. You can find more information on libGDX’s feature set here.   libGDX was born 3 years ago, out of a desire to target multiple platforms with a single code base. It has since grown into a big OSS project with over 100 professional and amateur contributors, and is used by thousands of developers to create the games of their dreams. From entries for the 48 hour Ludum Dare challenge, to top grossing mobile games to augmented reality experiments like Google’s Ingress.   libGDX has achieved wide adoption, especially on Android where it is powering 3.2% of all installed applications, with numerous top-grossing games. On that platform it can already compete with Unity3D.   Our Community as well as the entire libGDX development team are looking forward to welcome you among us. Contribute to powering thousands of applications and bringing joy to millions of users!   Information for Students  As a student you may ask yourself why you should take part in libGDX’s Google Summer of Code effort. Here we try to give you some food for thought that may help you decide whether you want to apply for our project.   libGDX offers students to explore a wide range of technologies:      Different operating systems, platforms  and hardware (desktop, Android, iOS, HTML5/WebGL)   Different languages (C, C++, Java, C#, Javascript)   Different fields of software development (data structures, 2D &amp; 3D graphics, audio, peripheral  communication, etc.)   Such broad environments are usually rare to find in many university settings.   You will work in a globally distributed team, a setup that is becoming more common. You’ll learn how to deal with the difficulties that arise in communication in such environments. Different time zones, different language skills (English is our lingua franca, but not everyone is a native speaker), miscommunication arising from limited expressiveness of text and so shapes how the team functions.   libGDX is an industrial strength framework used in production. It is employed in thousands of games that are played by millions of users every day. Modifying libgdx’s code means taking responsibility, and considering the impact it has on all the applications that are build on top of it. Big changes need to be discussed with the team, learning how to put forward a solid argument is crucial.   Finally, and maybe most importantly, libGDX means games. Many of the libGDX team members got into programming due to a desire to write games. If you want to improve your skillset while working within the game development field, then libGDX is for you.   Steps  The ideas below have been compiled by the libGDX contributors and community. If you want to pick any of those for your proposal, or come up with your own idea, it’s best to get to know libGDX and the backing technologies before starting with your proposal. That let’s you better estimate what is needed to finish a specific project. If you plan on applying for libGDX for your GSoC project you should follow these steps:      Fork the project on Github.   Get familiar on how to work with libgdx’s source.   Register on the forums and introduce yourself. Drop by on IRC (#libgdx, irc.freenode.org), get to know the community. This forum thread is where most of the GSoC students introduced themselves so far, make sure to subscribe to it to receive notifications for new messages. The subscribe button is at the bottom of the page.   Pick one of the project ideas below or come up with your own. Prepare your application using our [student application template] (FIXME (todo)). You’ll have to submit this application via the Google Summer of Code 2013 site, from April 22 - May 3, 2013. You’ll also need to sign the Student Participation Agreement. For more information on the process, read the GSoC FAQ. Reading it is mandatory!   Get in contact with the mentors listed below, on IRC (irc.freenode.org, #libgdx) or the forums.   Send a pull request for some issue or improvement you made to libGDX on Github, note that in your application. When selecting students, we’ll favor those that already demonstrated that they can interact with our community and read the docs on how to contribute   Information for Mentors  Since mentors have quite a bit of responsibility, I (badlogic) would want to limit potential mentors to the group of contributors. Exceptions to this rule may be made of course. Nex and I take on the project administrator role, which is different to that of a mentor.   Mentoring means commitment, once you agree to be a mentor, there’s no way back after we submit the application to GSoC.   Steps     Drop me (badlogic) an e-mail   Improve or add new ideas to the list below   Read the GSoC FAQ, which explains all the obligations of a mentor.   Read the GSoC Mentor Handbook which provides guidance on how to approach mentoring   We organize our mentor activity on this thread. Subscribe to it, the subscription button is at the bottom of the page.   Look out for students on the forum and on IRC, welcome them to the community and show them around   Idea List Guidelines  Please use the following format for ideas:      Title use ### Project: Title   Goals: keep it short, can be vague   Required Skills: language, platforms, etc.   Links: further information, forum threads, etc.   Potential Mentors: contributors only   If you are not a contributor, post your idea in this thread (FIXME (todo))   Please add ---- after each idea as a separator.   Ideas   Project: In-app Purchase Extension  Goals: create a libGDX extension that allows to incorporate in-app purchases across as many platforms as possible. iOS and Android are a must, GWT/desktop would be a nice to have.   Required Skills: Java, C#, basic iOS knowledge (bonus if you know Xamarin.iOS), basic Android knowledge (FIXME (xamarin -&gt; robovm))   Links: https://github.com/libgdx/libgdx/pull/146 API draft by a core contributor   Mentors: [noblemaster](https://code.google.com/p/libgdx/wiki/GoogleSummerOfCode2013Mentors, [tamas](https://code.google.com/p/libgdx/wiki/GoogleSummerOfCode2013Mentors, [badlogic](https://code.google.com/p/libgdx/wiki/GoogleSummerOfCode2013Mentors     Project: Social Network Extension  Goals: create a libGDX extension that allows to incorporate social network interactions, e.g. get friend lists. Ideally this should work across all platforms (desktop, GWT, Android, IOS).   Required Skills: Java, C#, basic iOS knowledge (bonus if you know Xamarin.iOS), basic Android knowledge (FIXME (xamarin -&gt; robovm))   Mentors: nex, tamas, badlogic     Project: RoboVM/iOS backend   (FIXME done?) Goals: create a libGDX backend based on RoboVM for iOS.   Required Skills: Java, C/C++, OpenGL ES, iOS, maybe LLVM   Links: https://web.archive.org/web/20200426122040/https://www.badlogicgames.com/forum/viewtopic.php?f=23&amp;t=7883 instructions on how to possibly approach this.   Mentors: noblemaster, badlogic     Project: Avian VM/iOS backend  Goals: create a libGDX backend based on Avian VM for iOS.   Required Skills: Java, C/C++, ObjectiveC, OpenGL ES, iOS   Links: https://web.archive.org/web/20200426122040/https://www.badlogicgames.com/forum/viewtopic.php?f=23&amp;t=7883 instructions on how to possibly approach this for RoboVM, the same approach is applicable to Avian VM.   Mentors: noblemaster, badlogic     Project: Generic 2D Level Editor   Goals: create a 2D level editor, using libgdx’s scene2d for portability, that allows creating arbitrary level maps that can be loaded via the maps API. As a bonus point, make it extensible through plugins.   Required Skills: Java, basic graphics programming, bonus if you know scene2d, maybe OSGi   Links: https://docs.google.com/document/d/1iNo5yB39-iXV10I2bxPHpuk84EoEpU7_L9HSuPKN5tE quick draft of features/requirements we’d like to include.   Mentors: nex, nate, bach, badlogic     Project: Oculus Rift backend  Goals: create a backend so libGDX supports Oculus Rift, a virtual reality headset with insanely low latency. We’ll probably have to provide the headset to the student. We’ll either buy it or ask the good folks at Oculus if they’d contribute one. We’d want to either extend an existing backend or write a new one specifically for the Oculus rift. This extension allows to expand libgdx’s domain to non-game related areas, like art installations, scientific visualizations and so on.   Required Skills: Java, C/C++, 3D programming, including GLSL shaders, linear algebra   Links: Oculus Rift first reactions, just because it’s funny :)   Mentors: [xoppa](https://code.google.com/p/libgdx/wiki/GoogleSummerOfCode2013Mentors, [bach](https://code.google.com/p/libgdx/wiki/GoogleSummerOfCode2013Mentors, [badlogic](https://code.google.com/p/libgdx/wiki/GoogleSummerOfCode2013Mentors     Project: XBox Kinect extension   Goals: Create an extension that allows people to use the Kinect in their libGDX desktop projects. Kinect may have to be provided by us or sponsored in some way. The extension should also provide one or more demos that demonstrate how the extension works and what potential it has. This extension allows to expand libgdx’s domain to non-game related areas, like art installations, scientific visualizations and so on.   Required Skills: Java, C/C++, 3D programing, including GLSL shaders, linear algebra   Links: OpenKinect, library we could wrap and expose   Mentors: [xoppa](https://code.google.com/p/libgdx/wiki/GoogleSummerOfCode2013Mentors, [badlogic](https://code.google.com/p/libgdx/wiki/GoogleSummerOfCode2013Mentors     Project: Leap Motion extension   Goals: Leap Motion is a new kind of controller that tracks your hand and finger movement and allows you to interact with digital media in a more natural way (think Minority Report). Creating an extension that gives libGDX users access to this device would be the goal for this project. On top of the extension, a set of demos should be created that demonstrate how to use the extension as well as its overall potential. This extension allows to expand libgdx’s domain to non-game related areas, like art installations, scientific visualizations and so on.   Devices would either have to be sponsored or bought by students/mentors. The devices are meant to ship in May, that may be too late.   This idea was orginally submitted by Fisherman on the forums.   Required Skills: Java, game programming, potentially a scripting language like Lua, JavaScript, Squirrel etc.   Links: Block54 Leap Motion demo Information for Leap Motion developers   Mentors: [badlogic](https://code.google.com/p/libgdx/wiki/GoogleSummerOfCode2013Mentors, [Xoppa](https://code.google.com/p/libgdx/wiki/GoogleSummerOfCode2013Mentors     Project: Experimental Features for new 3D API   Goals: libgdx’s 3D API is currently being worked on by a few of the core contributors. We’d like to explore some experimental features, including but not limited to: deferred rendering, different shadow mapping techniques, especially those suited for mobiles, CPU-side skinning (may require modifications to the API), mobile GPU specific optimizations for the default über-shader, using tools by GPU manufacturers to profile and improve the shader, dynamic lighting ala God of War 3. We are open for other more experimental features. Most of the above will not take a full 3 months, students are advised to pick at least two.   Required Skills: Java, 3D programming, including GLSL shaders, linear algebra   Links: Discussion by core developers on the new 3D API, for topic specific links see goals text above.   Mentors: [xoppa](https://code.google.com/p/libgdx/wiki/GoogleSummerOfCode2013Mentors, [bach](https://code.google.com/p/libgdx/wiki/GoogleSummerOfCode2013Mentors, [badlogic](https://code.google.com/p/libgdx/wiki/GoogleSummerOfCode2013Mentors     Project: Improve GDX Remote   Goals: Gdx remote allows you to run your application on the desktop, to which you can connect a device that sends input events to the application. This allows testing things like accelerometer controls in the desktop environment without having to deploy the application to the device. The current implementation is very primitive. Ideally, we’d like to also stream video from the desktop to the device, with minimal latency.   Required Skills: Java, network programming   Links: blog post with video of current implementation   Mentors: [nate](https://code.google.com/p/libgdx/wiki/GoogleSummerOfCode2013Mentors, [nex](https://code.google.com/p/libgdx/wiki/GoogleSummerOfCode2013Mentors, [badlogic](https://code.google.com/p/libgdx/wiki/GoogleSummerOfCode2013Mentors     Project: Websocket Support   Goals: libGDX currently only has rudimentary networking support (HTTP, TCP). We’d like to add Websocket support, allowing network programming with the HTML5 backend. This would allow us to do things like Chrome World Wide Maze or Browser Quest. Adding Websocket support would require a change in our current networking APIs, namely Socket#getInputStream needs to be changed.   Required Skills: Java, C#, network programming, GWT   Links: Entry-point to libGDX networking API   Mentors: [tamas](https://code.google.com/p/libgdx/wiki/GoogleSummerOfCode2013Mentors, [nex](https://code.google.com/p/libgdx/wiki/GoogleSummerOfCode2013Mentors, [badlogic](https://code.google.com/p/libgdx/wiki/GoogleSummerOfCode2013Mentors     Project: Simplified 3D Physics API and HTML5 implementation  Goals: libGDX wraps bullet physics, an OSS 3D physics library. Currently, the bullet extension is only supported on the desktop and Android (iOS pending). We’d like to create a simplified 3D physics API, maybe modelled after Bullet’s C-API. This would allow us to also create an implementation for the HTML5 backend based on ammo.js or other Javascript 3D physics libraries.   Note that we already have Box2D fully working on all platforms, including HTML5. This was achieved by using JBox2D for the HTML5 backend. Doing this for bullet seems less feasible.   Required Skills: Java, GWT   Links: See links in goals above.   Mentors: [xoppa](https://code.google.com/p/libgdx/wiki/GoogleSummerOfCode2013Mentors, [badlogic](https://code.google.com/p/libgdx/wiki/GoogleSummerOfCode2013Mentors     Project: Scripting Support   Goals: libGDX is currently tighly coupled to JVM languages. Scripting can help game developers to quickly prototype ideas and also have technical artists interact with the game mechanics more easily. A scripting extension that is cross-platform could be created. It does not need to provide specific bindings to libgdx, but should be easy to integrate and customize on all supported platforms. Javascript seems to be the best candidate (through e.g. Nashorn on the desktop, plain Javascript in GWT, V8 on Android and JavascriptCore on iOS, Lua may be an option as well. The end result would be a libGDX extension that lets users integrate scripts in their games more easily.   This project could be done in collaboration with the High-Level Game Engine project below.   Required Skills: Java, C/C++, familiarity with at least one potential scripting language   Links: see links in goals description   Mentors: [bach](https://code.google.com/p/libgdx/wiki/GoogleSummerOfCode2013Mentors, [tamas](https://code.google.com/p/libgdx/wiki/GoogleSummerOfCode2013Mentors, [badlogic](https://code.google.com/p/libgdx/wiki/GoogleSummerOfCode2013Mentors     Project: High-level Game Engine   Goals: libGDX is a framework which can be overwhelming for people who do not have previous experience with game programming. A high-level game engine on top of libGDX could lower the barrier of entry. We propose to build a high-level 2D game engine on top of libGDX. Good API design, documentation and examples are part of the challenge. Inspiration can be found in projects such as Corona, Flixel, HaxePunk or Cocos2D. We do not rule out a 3D engine either, but think 2D would have more chances of success given the time frame of Google Summer of Code. Ideally, the engine would be based on the component/entity system pattern.   This project could be done in collaboration with the Scripting Support project above.   Required Skills: Java, game programming, potentially a scripting language like Lua, JavaScript, Squirrel etc.   Links: see links in description above.   Mentors: [bach](https://code.google.com/p/libgdx/wiki/GoogleSummerOfCode2013Mentors, [tamas](https://code.google.com/p/libgdx/wiki/GoogleSummerOfCode2013Mentors, [nate](https://code.google.com/p/libgdx/wiki/GoogleSummerOfCode2013Mentors, [badlogic](https://code.google.com/p/libgdx/wiki/GoogleSummerOfCode2013Mentors     Project: Command Line Project Management   (FIXME done with Gradle?)   Goals: libGDX projects are usually done with Eclipse in mind. While we support Maven and other IDEs and development environments, integration isn’t as straight forward. Maven is problematic due to GWT and Android being somewhat second class citizens in that area. Also, IDE integration of Maven Android and GWT projects is lacking. In addition to IDE woes, packaging and deploying projects can be cumbersome. We’d like to develop a command line application that allows the creation of new libGDX projects, create project files for various IDEs, create POM and Gradle build files, can update dependencies automatically, can remove and add extensions, and package and deploy the application to connected devices. Loom Engine and Haxe NME provide similar tools.   Required Skills: Java, Maven, Gradle, Eclipse, Intellij, Netbeans Links: Google Doc with requirements   Mentors: tamas,bach, nate, badlogic     Project: Increase Robustness through Static Code Analysis Tools   Goals: libGDX is used in production, and as such requires high robustness. One way of increasing robustness is to use static analysis tools that uncover bad practices and bugs. Applying such analysis to a code base as big as libGDX’s is a challenge. Not only is the size an issue, but a gaming framework may have to rely on patterns for performance that are wrongly identified as issues by static analysis tools. For this idea, we’d like to see how static analysis tools can be exploited to increase libGDX’s quality and how we could integrate them into our everyday workflow. It is expected that whatever tool is used for analysis needs to be adapted with new rules that fit for the requirements of a gaming framework.   Required Skills: Java, Maven, Gradle, Eclipse, Intellij, Netbeans   Links: List of static analysis tools on Wikipedia   Mentors: nate, [badlogic](https://code.google.com/p/libgdx/wiki/GoogleSummerOfCode2013Mentors   (FIXME, maybe add the mentor list here… idk)  ",
        
        "url": "/wiki/misc/google-summer-of-code-2014" },{
        "title": "Graphics",
        "excerpt":
        
        "REWRITE THIS  This page needs to be rewritten.   Introduction   The Graphics module provides information about the current device display and application window as well as information about and access to the current OpenGL context. Specifically, information regarding screen size, pixel density, and frame-buffer properties such as color-depth, depth/stencil buffers, and anti-aliasing capabilities can all be found within this class. As with other common modules, access is provided via static fields of the Gdx class.   OpenGL Context   A particular use of this module concerns more direct access to the current OpenGL context for lower-level commands and queries.   The following example accesses the context in an OpenGL ES2 application to set the viewport and clear the frame and depth buffers:   Gdx.gl20.glViewport( 0, 0, Gdx.graphics.getWidth(), Gdx.graphics.getHeight() ); Gdx.gl20.glClearColor( 0, 0, 0, 1 ); Gdx.gl20.glClear( GL20.GL_COLOR_BUFFER_BIT | GL20.GL_DEPTH_BUFFER_BIT );   Note the use of getWidth() / getHeight() to access the current application window dimensions in setting the viewport as well as the use of constants from the GL20 class just as one would do in a regular OpenGL program. A key advantage in libGDX is the ability to access low level functionality whenever higher level abstraction does not suffice.   Each version of OpenGL ES is available through its own respective interface as well as a GLCommon ( that doesn’t exist, this article needs to be rewritten! ) interface for version agnostic commands. Note that use of GL20 requires instructing the application to use OpenGL ES2 upon start-up.   Access to the OpenGL Utility class ( that doesn’t exist, this article needs to be rewritten! ) is also provided, although this functionality may be better handled through Libgdx’s own Orthographic and Perspective camera classes. There is also a simple method for querying support for named extensions in supportsExtension(). Just supply the name of the extension to determine support on the current device.   Frame Time   One particularly useful method in the Graphics class is getDeltaTime(), which provides the time elapsed since the last rendered frame. This can be useful for time-based animation when frame independence is not necessary. For instance Actor or UI animation in a Stage instance might be controlled by a call such as the following in the application’s render method:   stage.act( Math.min( Gdx.graphics.getDeltaTime(), 1/30 ) );   Notice the use of a maximum time step of 1/30 seconds. This is to avoid potentially large jerks in the resulting animation. This illustrates the fact that while getDeltaTime() can be useful for simple animations, it is still frame dependent and more sensitive actions such as game logic or physics simulation may benefit from other timing strategies.   Another useful method is getFramesPerSecond(), which returns a running average of the current frame-rate for simple diagnostic purposes. However for more serious profiling efforts, the use of FPSLogger is recommended.   Platform Differences   On the desktop, the Graphics class also provides the ability to set a window’s icon and title values. Obviously these methods have no effect on platforms which lack an icon or title.   The methods setDisplayMode() and setVSync() set the display mode to full-screen/windowed and enable/disable vertical display sync respectively. Keep in mind these methods have effects only on certain platforms.  ",
        
        "url": "/wiki/graphics/graphics" },{
        "title": "Gyroscope",
        "excerpt":
        
        "Some Android devices have a gyroscope sensor that provides information about the rate of rotation in rad/s around a device’s x, y, and z axis.   NOTE: The gyroscope is currently not available on iOS devices since there is no implementation in the RoboVM - backend yet.   You must first enable the gyroscope in your android config. (Typically in your AndroidLauncher.java file)   config = new AndroidApplicationConfiguration(); config.useGyroscope = true;  //default is false  //you may want to switch off sensors that are on by default if they are no longer needed. config.useAccelerometer = false; config.useCompass = false;  Querying whether the gyroscope is available works as follows:   boolean gyroscopeAvail = Gdx.input.isPeripheralAvailable(Peripheral.Gyroscope);   Once you determined that the gyroscope is indeed available, you can poll its state:    if(gyroscopeAvail){ \tfloat gyroX = Gdx.input.getGyroscopeX(); \tfloat gyroY = Gdx.input.getGyroscopeY(); \tfloat gyroZ = Gdx.input.getGyroscopeZ(); }    ",
        
        "url": "/wiki/input/gyroscope" },{
        "title": "Hiero",
        "excerpt":
        
        "Hiero is a bitmap font packing tool. It saves in the Angel Code font format, which can be used by BitmapFont in libGDX applications.      Running Hiero   You can download the .jar file here. Make it executable (if you are on a UNIX-like operating system) and then run it.   As Hiero still runs on our LWJGL 2 backend, it may have trouble with newer Java versions. If you want to be on the safe side, use Adoptium’s OpenJDK 8.   If you are using Gradle and you added “Tools” extension to your project, you can easily run Hiero from your IDE, otherwise look at Downloading Hiero.   Example for IntelliJ IDEA: Go to the Hiero class, right click and select Run Hiero.main(). On the Run configurations popup that will appear, select the Desktop module, and click Run.   Rasterization   Hiero has multiple options for rasterizing fonts:      FreeType is typically the highest quality. It makes good use of hinting, which means that small fonts are rendered nicely. The gamma setting controls how much antialiasing is done. The mono setting disables all font smooth. No other effects are supported, though glyphs can be rendered with padding and effects applied via Photoshop or other tools. Hiero uses gdx-freetype, so generated bitmap fonts will exactly match those rendered on the fly by gdx-freetype.   Java’s font rendering provides the vector outline for the glyphs which allows various effects to be applied, such as a drop shadow, outline, etc. Output is often blurry at small sizes, but larger sizes are good quality.   OS native rendering is the most simplistic. It does not provide tightly fitting bounds, so glyphs take up more atlas space.   Hiero will output kerning information for fonts with kerning entries.   Command line arguments   Hiero supports 4 command line arguments:      --input &lt;file&gt; or -i &lt;file&gt; loads the specified .hiero configuration file when launching.   --output &lt;file&gt; or -o &lt;file&gt; sets the output .fnt file to the specified value.   --batch or -b makes hiero  automatically generate it’s output and close to be used without human intervention.   --scale &lt;scale&gt; or -s &lt;scale&gt; scales the font by the specified amount.   Alternatives   BitmapFontWriter   BitmapFontWriter is a class in gdx-tools which can write BMFont files from a BitmapFontData instance. This allows a font to be generated using FreeTypeFontGenerator, then written to a font file and PNG files. BitmapFontWriter has the benefit that it can be more easily run from scripts and can make use of FreeTypeFontGenerator’s shadows and borders. Otherwise, the output is very similar to Hiero, though Hiero avoids writing a glyph image multiple times if different character codes render the same glyph.   Usage can look like this:   new LwjglApplication(new ApplicationAdapter() { \tpublic void create () { \t\tFontInfo info = new FontInfo(); \t\tinfo.padding = new Padding(1, 1, 1, 1);  \t\tFreeTypeFontParameter param = new FreeTypeFontParameter(); \t\tparam.size = 13; \t\tparam.gamma = 2f; \t\tparam.shadowOffsetY = 1; \t\tparam.renderCount = 3; \t\tparam.shadowColor = new Color(0, 0, 0, 0.45f); \t\tparam.characters = Hiero.EXTENDED_CHARS; \t\tparam.packer = new PixmapPacker(512, 512, Format.RGBA8888, 2, false, new SkylineStrategy());  \t\tFreeTypeFontGenerator generator = new FreeTypeFontGenerator(Gdx.files.absolute(\"some-font.ttf\")); \t\tFreeTypeBitmapFontData data = generator.generateData(param);  \t\tBitmapFontWriter.writeFont(data, new String[] {\"font.png\"}, \t\t\tGdx.files.absolute(\"font.fnt\"), info, 512, 512); \t\tBitmapFontWriter.writePixmaps(param.packer.getPages(), Gdx.files.absolute(\"imageDir\"), name);  \t\tSystem.exit(0); \t} });   Glyph Designer   Glyph Designer is a commercial bitmap font tool designed specifically for Mac. It allows you to create beautifully styled text with custom backgrounds, gradient fills, gradient strokes &amp; shadows. The command line interface allows you to export multi-lingual character sets and target multiple device profiles. At time of writing Glyph Designer is priced at $39.99.   BMFont   The BMFont tool uses FreeType. It has additional supersampling features for smoother glyphs. BMFont does not support effects like drop-shadows or outlines, but glyphs can be output with padding and effects applied with Paint.NET, Photoshop, etc.   BMFont is Windows only but can be run using Wine. There are reports that it hangs if the space character is exported. The space character can be added manually, eg:  char id=32   x=0   y=0    width=0     height=0     xoffset=0    yoffset=0    xadvance=3     page=0  chnl=15  Change the xadvance as needed, this is the number of pixels for a space character.   TWL Theme Editor   The TWL Theme Editor has a font tool that also uses FreeType. It doesn’t support the supersampling. Theme Editor JWS.   gdx-fontpack   The gdx-fontpack tool also uses FreeType. It doesn’t yet support supersampling.   FontPacker   The FontPacker tool is written in C# and uses .NET’s TextRenderer, FontFamily, and Graphics classes to render.   ShoeBox   ShoeBox has a tool for creating Angel Code fonts.   JME   jMonkeyEngine has an Angel Code font tool, though it looks simplistic and most likely uses Java’s font rendering.   bmglyph   The bmglyph tool is for OS X only and hasn’t been evaluated.   Littera   https://kvazars.com/littera/littera.swf (requires Adobe Flash Player)  ",
        
        "url": "/wiki/tools/hiero" },{
        "title": "HTML5 Backend and GWT Specifics",
        "excerpt":
        
        "Welcome to a place of magic and wonder, the World Wide Web! Even though some folks say this Internet thing is “just a fad,” and we should keep using usenet and gopher, there’s at least one thing the WWW has that those technologies don’t:   libGDX Games   That’s right! You can make your very own libGDX games that run in an HTML5-capable web browser, which I assume is some advanced form of Netscape Navigator. This is possible by GWT, or Google Web Toolkit! I know what you’re thinking, Google? The guys who are trying to let people search the Internet with a form? What are they doing with libGDX? I have no idea either. If you want to make your own libGDX game deploy to the web using GWT, well, just make a project in the latest setup jar and make sure to check the Html checkbox. The rest should be straightforward!   BUT IT SOMETIMES ISN’T, AT FIRST   So there are a few things that are fundamentally different about developing using GWT as opposed to running a desktop project. You’ll want to get familiar with two Gradle tasks in particular; you can launch these tasks from your IDE if you aren’t comfortable on the command line, but command-line Gradle tends to avoid problems when the IDE isn’t working as well as we would like. gradlew html:superDev will be your main tool during development; it allows for a much-improved debugging experience and allows quickly reloading changes to the Java code. gradlew html:dist produces a fully-functioning web page that can be uploaded to a static web host (such as itch.io or GitHub Pages, both free); it also optimizes the web page so the game in it will perform better, which makes dist take a little longer than superDev.   superDev   superDev allows you to debug your HTML5 application. This is not necessary in most cases: if there are problems in your core game, you can debug the desktop application. But sometimes, there are bugs only appearing when running on HTML5. You can debug the application with the following steps:      Run the html:superdev Gradle task. It compiles the game and sets up a local HTTP server. When it is done, it will idle to keep the server running.   Your game is available here: http://localhost:8080/index.html (current config) or http://localhost:8080/html/ (older Gradle configuration with Jetty plugin) - open the page with Chrome to debug   Hit the big reload button and hit compile. The game will recompile and source maps will be set up.   After the game restarted, open Chrome’s dev console with F12 and navigate to the sources tab. Hit Ctrl-P and enter the name of the Java file you want to debug. The Java file will open within Chrome’s dev console and you can set a break point. You are able to step through the Java code lines. However, debug variables will be generated JS names but you’ll be able to make sense of it.   When you are done, you can stop the Gradle task with Ctrl-C.   If your bug does not show up on Chrome, but only on Firefox or Safari, you are in bad luck. No debugging is available. But you can work with debug logging and, to avoid unreadable stack traces, you can turn off the obfuscation by adding this line to HTML project’s build.gradle:   gwt {   // right below compiler.strict = true   compiler.style = org.wisepersist.gradle.plugins.gwt.Style.DETAILED }   dist Information   Should be pretty straightforward; the dist is generated in html/build/dist/. You can delete the sourcemap files if you feel you won’t be debugging the dist; they’re usually a few MB in size and are in html/build/dist/WEB-INF/deploy/html/symbolMaps.   Fullscreen Functionality   Surprisingly, fullscreen functionality actually works on the HTML backend. To enable fullscreen, call the following method from within your core project:   Gdx.graphics.setFullscreenMode(Gdx.graphics.getDisplayMode());   The user will be prompted to press “ESC” to exit fullscreen. And it even works on mobile. Great! It does have some caveats though. Turns out you can’t activate full screen on iOS. Also, if you choose to use the “Resizable Application” option in the HTML Launcher, you’ll need to rewrite the ResizeListener to the following:   class ResizeListener implements ResizeHandler {     @Override     public void onResize(ResizeEvent event) {         if (Gdx.graphics.isFullscreen()) {             Gdx.graphics.setFullscreenMode(Gdx.graphics.getDisplayMode());         } else {             int width = event.getWidth() - PADDING;             int height = event.getHeight() - PADDING;             getRootPanel().setWidth(\"\" + width + \"px\");             getRootPanel().setHeight(\"\" + height + \"px\");             getApplicationListener().resize(width, height);             Gdx.graphics.setWindowedMode(width, height);         }     } }   Don’t forget to also set the fullscreen orientation for mobile in the getConfig():   cfg.fullscreenOrientation = GwtGraphics.OrientationLockType.LANDSCAPE;   Resolution on mobiles   On mobile, if your game is run in an iframe, or if you switch to full screen, you will notice that your game looks pixelated. That is because the reported screen size of mobiles is not the real screen size. You can enable using the real screen size with config.usePhysicalPixels = true;. This will also affect HDPI and Retina screens on desktop, so maybe you want to use usePhysicalPixels = GwtApplication.isMobileDevice(). Check out this PR for detailed information.   Changing the Load Screen Progress Bar   As much as we love libGDX, the default loading progress bar when preparing the HTML game screams “newbie”. Impress your friends and bring honor to your family name by making a custom progress bar! Add the following to your HtmlLauncher class in your HTML project:   @Override public Preloader.PreloaderCallback getPreloaderCallback() {     return createPreloaderPanel(GWT.getHostPageBaseURL() + \"preloadlogo.png\"); }  @Override protected void adjustMeterPanel(Panel meterPanel, Style meterStyle) {     meterPanel.setStyleName(\"gdx-meter\");     meterPanel.addStyleName(\"nostripes\");     meterStyle.setProperty(\"backgroundColor\", \"#ffffff\");     meterStyle.setProperty(\"backgroundImage\", \"none\"); }   “preloadlogo.png” is an image you place in the “webapp” folder in the HTML project for DIST builds. Place the image in your “war” folder as well for your SUPERDEV builds. Adjust your color to fit the theme of your game. Enjoy yourself.   Please note that you can only use pure GWT facilities to display the loading screen. libGDX APIs will only be available after the preloading is complete.   Speeding up preload process   Speaking of the preloader: The HTML5 preloader is necessary, because usual gdx games rely on all assets being ready to access when needed. It prefetches every file in your asset directory. This may take some time and is not necessary if your game is a game that does not need all assets for presenting the startup screen. Think of all the people out there not having high speed internet connections.   From 1.9.12 on, you can decrease your preload time a lot if you use asset manager to load your assets later. You can override a predefined AssetFilter with your own AssetFilter on GWT and return false for all asset files that are not needed before game start. Make sure these files are only loaded via AssetManager, otherwise your game will freeze when using such assets.   public class AssetFilter extends DefaultAssetFilter {     @Override     public boolean preload(String file) {         return !file.endsWith(\".png\") || file.startsWith(\"data/hud/\");     } }   For compile process to pick up this asset filter instead of your own, add the following configuration to your GdxDefinition.gwt.xml file:     &lt;set-configuration-property name=\"gdx.assetfilterclass\" value=\"your.package.AssetFilter\"/&gt;   (See game source commit using the feature)   Prior 1.9.12, you can use an alternative backend.   Preventing Keys From Triggering Scrolling and Other Browser Functions   On a normal web page, if you press the down arrow on your keyboard, it will scroll the page up. That’s nice and all, but maybe you don’t want that to happen when players are trying to move the character in your game. To prevent this, you have to set libGDX to prevent the default actions of special keys by catching them:   Gdx.input.setCatchKey(Input.Keys.SPACE, true);   Preventing Right Click Context Menu   Similarly to keyboard keys, the right click context menu can be prevented from interrupting your game. You’ll notice that there are already functions to prevent left click from doing anything unexpected. You just need to add an additional line to apply the fix to right click as well. The following must be added to the script block of your index.html in the “html/webapp” folder (dist) and “html/war” folder (superDev):   // prevent right click document.getElementById('embed-html').addEventListener('contextmenu', handleMouseDown, false);   Sound and Music   You will probably face some problems with sounds and music, especially on mobile platforms. It is not recommended to play sounds immediately on startup of the game as browsers probably will block this.   The implementation the official HTML5 backend uses has some other restrictions, too. Pitch will not work and you will experience a lag on playing the sounds the first time. If you want to improve the situation, check out this PR   Differences Between GWT and Desktop Java   Numbers      When some number is very important and you want to make sure it is treated identically on desktop/Android and GWT, use a long.   When you know a number will never be especially large (specifically, that it won’t encounter numeric overflow by exceeding roughly 2 billion or negative 2 billion), feel free to use an int.            Math with ints is much faster than math with longs on GWT, because any int is represented by a JavaScript Number and web browsers are used to working with Numbers all the time. On the other hand, any long is represented by a specific type of JavaScript Object that stores three Numbers to help ensure precision.       A JavaScript Number, so an int, is almost the same as a double in Java, but it also allows bitwise operations to be used on it.                    Because Numbers act like doubles, they don’t overflow, and can go higher than Integer.MAX_VALUE (2147483647) and lower than Integer.MIN_VALUE (-2147483648). Using any bitwise operation on them will bring any numbers that got too big back into the normal int range. If you encounter fishy numeric results that seem way too large for an int, try using this simple trick: int fishy = Integer.MAX_VALUE * 5; int fixed = (Integer.MAX_VALUE * 5) | 0; On desktop, adding | 0 won’t change anything, but it can correct numbers that got weird on GWT. Or, you can use a long.                           The problem with long values on GWT is that they aren’t visible to reflection, so libGDX’s Json class won’t automatically write them or read them. You can work around this with Json’s handy custom serializer behavior, so it isn’t a huge issue.   Floats can have more equality check problems than usual. Make sure you make all equality checks for floats by using MathUtils.isEqual().   Other Known Limitations      Some java classes/features that are not supported:            System.nanoTime       Java reflection. You must only use libGDX reflection utils, see this wiki page for more details.       Multithreading is not supported.           Audio:            Sound pitch is not implemented prior 1.9.12. You can use an alternative backend which is based on WebAudioAPI and supports it.       Your game needs a user interaction (eg. click on a button) before playing any music or sounds. This is a limitation for any games played in a browser.           TiledMaps should be saved with Base64 encoding.   Pixmap            Some Pixmap methods are not supported (eg. loading from binary data).       Some drawings (eg. lines) are antialiased which is not always wanted. If you need non-antialiased lines, you can draw it pixel by pixel or use FrameBuffer with a ShapeRenderer to achieve it.           WebGL 1.0 is used and has its own limitations compared with OpenGL or GLES, among them:            NPOT (non power of two) textures are not supported with MipMap filters and/or Repeat wrapping.       Gdx.graphics.supportsExtension(…) should be called for each extension prior to enabling it in shaders.           Some libGDX extensions are not supported or require additional libraries:            Bullet       Freetype requires gdx-freetype-gwt           Further Reading   The original Super Dev Instructions from Mario   How to speed up GWT compilation   Building libGDX from source and adding new files to gdx.gwt.xml   HTML5 - GWT Explained on YouTube  ",
        
        "url": "/wiki/html5-backend-and-gwt-specifics" },{
        "title": "ImGui",
        "excerpt":
        
        "An alternative GUI for libGDX is Dear ImGui, a bloat-free graphical user interface library in C++. It outputs optimized vertex buffers that you can render anytime in your 3D-pipeline enabled application. It is fast, portable, renderer agnostic and self-contained (few dependencies).   There are a couple of different JVM bindings you can choose from: kotlin-graphics/imgui, ice1000/jimgui and SpaiR/imgui-java.   General Information   Dear ImGui is designed to enable fast iteration and empower programmers to create content creation tools and visualization/ debug tools (as opposed to UI for the average end-user). It favors simplicity and productivity toward this goal, and thus lacks certain features normally found in more high-level libraries.   A common misunderstanding is to think that immediate mode gui == immediate mode rendering, which usually implies hammering your driver/GPU with a bunch of inefficient draw calls and state changes, as the gui functions are called by the user. This is NOT what Dear ImGui does. Dear ImGui outputs vertex buffers and a small list of draw calls batches. It never touches your GPU directly. The draw call batches are decently optimal and you can render them later, in your app or even remotely.   Dear ImGui is particularly suited to integration in realtime 3D applications, fullscreen applications, embedded applications, games, or any applications on consoles platforms where operating system features are non-standard.   This is an example demonstrating what ImGui is capable of:        Option 1: kotlin-graphics’ Bindings   There is an elaborate wiki entry over in the Kotlin-graphics’s repo, detailing how ImGui can be used together with libGDX: https://github.com/kotlin-graphics/imgui/wiki/Using-libGDX   These are some very simple examples, how its usage may look like:   Kotlin  var f = 0f with(ImGui) {     text(\"Hello, world %d\", 123)     button(\"OK\"){         // react     }     inputText(\"string\", buf)     sliderFloat(\"float\", ::f, 0f, 1f) }   Java  ImGui imgui = ImGui.INSTANCE; float[] f = {0f}; imgui.text(\"Hello, world %d\", 123); if(imgui.button(\"OK\")) {     // react } imgui.inputText(\"string\", buf); imgui.sliderFloat(\"float\", f, 0f, 1f);      ImGui supports also other languages, such as Japanese, initiliazed here as:   IO.fonts.addFontFromFileTTF(\"extraFonts/ArialUni.ttf\", 18f, glyphRanges = IO.fonts.glyphRangesJapanese)!!   Option 2: SpaiR’s Bindings  Required Dependencies for the LWJGL 3 Subproject  api \"io.github.spair:imgui-java-binding:&lt;version&gt;\" api \"io.github.spair:imgui-java-lwjgl3:&lt;version&gt;\" api \"io.github.spair:imgui-java-natives-linux:&lt;version&gt;\" api \"io.github.spair:imgui-java-natives-macos:&lt;version&gt;\" api \"io.github.spair:imgui-java-natives-windows:&lt;version&gt;\"   Example Minimal Usage  The following instructions detail how ImGui can be used on top of a 3D scene in libGDX.           Initialize ImGui at the beginning of the program:       ImGuiImplGlfw imGuiGlfw = new ImGuiImplGlfw(); ImGuiImplGl3 imGuiGl3 = new ImGuiImplGl3();                In create():       // create 3D scene GLFWErrorCallback.createPrint(System.err).set(); if (!glfwInit()) {    throw new IllegalStateException(\"Unable to initialize GLFW\"); } ImGui.createContext(); final ImGuiIO io = ImGui.getIO(); io.setIniFilename(null); ImGuiTools.setupFonts(io);     windowHandle = ((Lwjgl3Graphics) Gdx.graphics).getWindow().getWindowHandle();     imGuiGlfw.init(windowHandle, true); imGuiGl3.init(glslVersion);                In render():       // render 3D scene imGuiGlfw.newFrame(); ImGui.newFrame(); ImGui.button(\"I'm a Button!\"); ImGui.render(); imGuiGl3.renderDrawData(ImGui.getDrawData()); glfwSwapBuffers(windowHandle); glfwPollEvents();                In dispose():       imGuiGl3.dispose(); imGuiGlfw.dispose(); ImGui.destroyContext();                The result:             ",
        
        "url": "/wiki/graphics/2d/imgui" },{
        "title": "Importing & Running a Project",
        "excerpt":
        
        "Next up, you need to import your project into your IDE.                                                                                                   Set Up a Dev Environment                                                                                           Generate a Project                                                                                         Importing &amp; Running                                                                                         A Simple Game                                       Importing the Project          In IntelliJ IDEA or Android Studio, you can choose to open the build.gradle file and select “Open as Project” to get started.       In Eclipse, choose File -&gt; Import... -&gt; Gradle -&gt; Existing Gradle Project (make sure that your freshly generated project is not located inside of your workspace).       In NetBeans it is File -&gt; Open Project.            You may need to refresh the Gradle project after the initial import if some dependencies weren’t downloaded yet.       In IntelliJ IDEA/Android Studio, the Reimport all Gradle projects button is a pair of circling arrows at the top left in the Gradle tool window, which can be opened with View -&gt; Tool Windows -&gt; Gradle.       In Eclipse right click on your project Gradle -&gt; Refresh Gradle Project.           Getting it Running  If you want to execute your freshly imported project, you have to follow different steps, depending on your IDE and the platform you are targeting.  Desktop  In IDEA/Android Studio:     Extend the Gradle tab on the right sight of your window:             Expand the tasks of your project and then select: desktop -&gt; other -&gt; run.       In Android Studio 4.2, tasks are no longer shown by default. Go to Settings -&gt; Experimental and uncheck Do not build Gradle task list during Gradle sync. Then sync the project via File -&gt; Sync Project with Gradle Files       Alternatively, you can create a run configuration:     Right click your DesktopLauncher class   Select ‘Run DesktopLauncher.main()’. This should fail with missing assets, because we need to hook up the assets folder first.   Open up Run Configurations             Edit the Run Configuration that was just created by running the desktop project and set the working directory to point to your assets folder            On macOS, LWJGL3 projects require one extra step: Either add the com.badlogicgames.gdx:gdx-lwjgl3-glfw-awt-macos dependency to your desktop project, or, in your Run Configuration, set the VM Options to -XstartOnFirstThread. Additional information can be found here.       Run your application using the run button   In Eclipse:     Right click your desktop project -&gt; Run as -&gt; Run Configurations…   On the right side, select Java Application:        At the top left, click the icon to create a new run configuration:      As Main class select your DesktopLauncher class   After that, click on the Arguments tab        At the bottom, under ‘Working directory’ select ‘Other’ -&gt; Workspace…          On macOS, LWJGL3 projects require one extra step: Either add the com.badlogicgames.gdx:gdx-lwjgl3-glfw-awt-macos dependency to your desktop project, or, in your Run Configuration, set the VM Options to -XstartOnFirstThread. Additional information can be found here.       Then select your asset folder located in assets   In NetBeans:  Right click the desktop project -&gt; Run       Android     IDEA/Android Studio: Right click AndroidLauncher -&gt; Run AndroidLauncher   Eclipse: Right click Android project -&gt; Run As -&gt; AndroidApplication   NetBeans: Right click Android project -&gt; Run As -&gt; AndroidApplication       iOS  In IDEA/Android Studio     Open Run/Debug Configurations   Create a new run configuration for a RoboVM iOS application              Select the provisioning profile and simulator/device target       Note: arm64 simulators are not working by default. Either use x86_64 or use the MetalANGLE RoboVM backend instead (“com.badlogicgames.gdx:gdx-backend-robovm-metalangle:$gdxVersion”)            Run the created run configuration       For more information on using and configuring the RoboVM IntelliJ plugin please see the documentation.   In Eclipse     Right click the iOS RoboVM project &gt; Run As &gt; RoboVM runner of your choice      For more information on using and configuring the RoboVM IntelliJ plugin please see the documentation.       HTML  HTML is best suited to be run on command line. You are welcome to manually setup GWT in the IDE of your choice if you are familiar with it, but the recommended way is to drop down to terminal or command prompt.   The HTML target can be run in Super Dev mode, which allows you to recompile on the fly, and debug your application in browser.   To do so, open up your favourite shell or terminal, change directory to the project directory and invoke the respective gradle task:   ./gradlew html:superDev   On Unix: If you get a permission denied error, set the execution flag on the gradlew file: chmod +x gradlew   You should see lots of text wizzing by, and if all goes well you should see the following line at the end:      You can then go to http://localhost:8080/index.html, to see your application running, with a recompile button.   For further info on configuring and debugging with SuperDev check the GWT documentation.       Command Line  All the targets can be run and deployed to via the command line interface.   Desktop:  ./gradlew desktop:run   Android:  ./gradlew android:installDebug android:run   The ANDROID_HOME environment variable needs to be pointing to a valid android SDK before you can do any command line wizardry for Android. On Windows, use: set ANDROID_HOME=​C:/Path/To/Your/Android/Sdk; on Linux and macOS: export ANDROID_HOME=​/Path/To/Your/Android/Sdk. Alternatively you can create a file called “local.properties” with the following content: sdk.dir /Path/To/Your/Android/Sdk.   iOS:  ./gradlew ios:launchIPhoneSimulator   HTML:  ./gradlew html:superDev   Then go to http://localhost:8080/index.html.   Gradle tasks are failing?  If whenever you invoke Gradle, the build or refresh fails to get more information, run the same command again and add the --debug parameter to the command, e.g.:   ./gradlew desktop:run --debug   This will provide you with a stacktrace and give you a better idea of why gradle is failing.       What to do next?  Now that you’re done with the set up, you can get to do some real coding. Take a look at our post A Simple Game for a step-by-step guide.  ",
        
        "url": "/wiki/start/import-and-running" },{
        "title": "Importing Blender models in LibGDX",
        "excerpt":
        
        "LibGDX provides its own 3D format out of the box called G3D (g3dj and g3db files), this article describes steps from Blender to your game using this format. OBJ format is partially supported and not recommended for production. Alternatively, you can use glTF format via third party library gdx-gltf which also provides advanced features like PBR rendering.   Note: while this page uses Blender for practical examples, most of it applies to other modeling applications as well.   Blender is an open-source modeling application you can use to create 3D models, scenes and animations. You can get Blender at blender.org. If you are new to creating 3D models using Blender, you can checkout the blender tutorials. This page provides practical tips on preparing and converting your Blender model for use in libGDX.   Blender considerations  As Blender is a multi-purpose tool, there are certain pitfalls you can stumble into that will make your model ill suited for game development. One such mistake is using the Rigify plugin to animate your model: it will add so much stuff that your model will grow in size to at the very least 3mb (per animated model), and possibly even more, so use that with great care.   Another size consideration may be (depending on type and amount of animations) to set your key frame interpolation to linear (from the default bezier interpolation). This may drastically improve your g3db file size (but it may also change how your animations look, so check before you hit save). To change keyframe interpolation in blender, switch to the animation perspective, select all your keyframe nodes with the A key in the Dope Sheet, press T and select “Linear”.   Blender Animation  Make sure to use the Action Editor for you animation of your models. The name you provide for the animation dropsheet in blender is the animation ID you can use in your code. In the below snapshot, CubeAction would be the name. Don’t forget to hit that little F to ensure the action is saved!    Exporting to FBX and converting to G3DB  Note: see this project here) which converts directly from .blend files. *For Older Blender versions &lt;2.8, check this project: here.   The default (preferred) method is to export to FBX. Make sure you select all and only those options (e.g. nodes and animations) you want to actually include. Don’t include your camera, lights, etc. Next download the latest version of fbx-conv and convert the FBX file to G3DB. You’ll need to flip texture coordinates by using the -f commandline option. fbx-conv -f file.fbx   Optionally, you may also convert your file to the G3DJ format, which is a JSON format which is readily viewable with a simple text editor. fbx-conv -f -o G3DJ file.fbx Please note that G3DJ will take longer to load when you run your application, as it is not a binary format.   Please note that there is a known limitation to using the FBX export in Blender. The current exporter only supports texface textures (i.e. textures assigned to an UV map).   Also note that Blender exports at 1 unit = 1 meter, while libGDX imports at a scale of 1 unit = 1 cm, making imported models 100x bigger. Change the export options from the default 1.00 to 0.01 to fix.    Setting the coordinate system (up-axis)  The coordinate system Blender uses (z-up) is different compared to the most common system used for games (y-up). The Blender FBX exporter contains the option to change the coordinate system to y-up (which might be even the default in the FBX exporter), do not use this option, instead set it to Blender’s default (z-up).   Fbx-conv will compensate the coordinate system by rotating the model (to y-up). However it will only be able to do this, if the fbx file itself contains the correct information. The Blender FBX exporter option will not modify the model, instead it will simply act like its y-up (causing fbx-conv unable to compensate).   When fbx-conv needs to compensate the coordinate system, it will rotate all root nodes of the model 90 degrees along the X-axis. It will also modify any animations accordingly. The geometry (vertices) itself however, remains unchanged.   However, if you want to use z-up in your application, then you can set Blender’s FBX exporter coordinate system option to y-up. This will cause fbx-conv not to rotate your model and animations, so it will be z-up.   Troubleshooting missing textures   If your faces are not drawn, please check try disabling back face culling. Your faces may be missing because they are facing away from the camera. DefaultShader.defaultCullFace = 0;  Also, it is quite common that the materials from Blender export with opacity set to Zero. If you notice your model is not being rendered. Go to the Material in Blender, and below “Transparency” set its Alpha to the desired one (usually 1, for full opacity).   Troubleshooting black textures   Please ensure you limit the size of your texture files to POT (power of two dimensions) which are square shaped meaning of equal width and height (e.g. 32x32, 64x64 etc). A maximum recommended size would be 1024x1024 for widespread support, however larger sizes may still render, depending on device. Often non-POT textures render correctly on a desktop, but not on mobiles. This is a limitation specific to the GPU being used, and non-POT support will vary from device to device.   Additionally test that your lighting/color is configured in a way which will illuminate your model instance. A good test is to pass a null environment pointer to your Model Batch, which will disable lighting effects.   If you have used Blender’s FBX export script, please ensure your textures were assigned to a UV map, as the export script only supports texface textures, go to edit mode, in UV/Texture editor select the texture/image that you want to export with the object.   RrSs warning  When using the Blender FBX exporter, you might receive a RrSs warning when converting the FBX file. This is due to the Blender FBX exporter wrongfully exporting the transformations. The fbx-conv utility will correct this and you can safely ignore the warning.   Maximum vertices  A model (g3dj or g3db file) can contain multiple meshes. These meshes are indexed. The indices used by libGDX are short values. Java’s maximum short value is 32,767. In other words: you practically can’t use more than 32767 vertices within a single mesh. Therefore you should make sure that your meshes never exceeds this limitation.   By default, fbx-conv warns you when you try to convert a mesh that contains more than 32767 indices. While this doesn’t have to mean that it will also result in more than 32767 vertices, it is a good indication that your mesh it is too “high-poly” and might cause issues. In this case you should consider lowering the polygon count or splitting the mesh into multiple parts.   If your model contains multiple similar meshes with less than 32767 vertices, then fbx-conv will try to combine these into a single mesh. When it combines meshes it will never exceed the maximum number of vertices of 32767. You can change this maximum using the -m command line option.   Warning: increasing the maximum number of vertices used by fbx-conv will not increase java’s maximum short value.   fbx-conv will never split your mesh, unless your mesh is skinned and exceeds the maximum number amount of bones specified. Simply because it doesn’t have enough information to do so.      Note that in contrast to java, both fbx-conv and opengl support unsigned indices. Therefore you might not notice issues between 32767 and 65535 vertices in some cases and devices. You should not rely on this though.    Using the Model Preview Utility   There is a model preview utility at https://github.com/ASneakyFox/libgdx-fbxconv-gui which you can use to preview your models. Download a precompiled release from the releases section.      Loading a G3DJ file into libGDX and instantiating it   The simplest way to load a G3DJ file into libGDX is the following:   Model model = new G3dModelLoader(new JsonReader()).loadModel(Gdx.files.internal(modelFileName));  This will import the G3DJ file. To actually create a run-time instance of it, we can use a ModelBuilder in combination with a ModelBatch:  ModelBuilder modelBuilder = new ModelBuilder(); Model model = new G3dModelLoader(new JsonReader()).loadModel(Gdx.files.internal(modelFileName)); ModelInstance instance = new ModelInstance(model);   Loading and rendering a G3DJ file example  import com.badlogic.gdx.ApplicationListener; import com.badlogic.gdx.Gdx; import com.badlogic.gdx.graphics.Color; import com.badlogic.gdx.graphics.GL20; import com.badlogic.gdx.graphics.PerspectiveCamera; import com.badlogic.gdx.graphics.VertexAttributes.Usage; import com.badlogic.gdx.graphics.g3d.Environment; import com.badlogic.gdx.graphics.g3d.Model; import com.badlogic.gdx.graphics.g3d.ModelBatch; import com.badlogic.gdx.graphics.g3d.ModelInstance; import com.badlogic.gdx.graphics.g3d.attributes.ColorAttribute; import com.badlogic.gdx.graphics.g3d.environment.DirectionalLight; import com.badlogic.gdx.graphics.g3d.Material; import com.badlogic.gdx.graphics.g3d.loader.G3dModelLoader; import com.badlogic.gdx.graphics.g3d.utils.CameraInputController; import com.badlogic.gdx.graphics.g3d.utils.ModelBuilder; import com.badlogic.gdx.utils.JsonReader;  /**  * Example program that imports \"myModel.g3dj\" from the assets folder and renders it onto the screen.  */ public class ImportG3DJ implements ApplicationListener {     private Environment environment;     private PerspectiveCamera camera;     private CameraInputController cameraController;     private ModelBatch modelBatch;     private Model model;     private ModelInstance instance;      @Override     public void create() {         // Create an environment so we have some lighting         environment = new Environment();         environment.set(new ColorAttribute(ColorAttribute.AmbientLight, 0.4f, 0.4f, 0.4f, 1f));         environment.add(new DirectionalLight().set(0.8f, 0.8f, 0.8f, -1f, -0.8f, -0.2f));          modelBatch = new ModelBatch();          // Create a perspective camera with some sensible defaults         camera = new PerspectiveCamera(67, Gdx.graphics.getWidth(), Gdx.graphics.getHeight());         camera.position.set(10f, 10f, 10f);         camera.lookAt(0, 0, 0);         camera.near = 1f;         camera.far = 300f;         camera.update();          // Import and instantiate our model (called \"myModel.g3dj\")         ModelBuilder modelBuilder = new ModelBuilder();         model = new G3dModelLoader(new JsonReader()).loadModel(Gdx.files.internal(\"myModel.g3dj\"));         instance = new ModelInstance(model);          cameraController = new CameraInputController(camera);         Gdx.input.setInputProcessor(cameraController);     }      @Override     public void render() {         cameraController.update();          // Clear the stuff that is left over from the previous render cycle         Gdx.gl.glViewport(0, 0, Gdx.graphics.getWidth(), Gdx.graphics.getHeight());         Gdx.gl.glClear(GL20.GL_COLOR_BUFFER_BIT | GL20.GL_DEPTH_BUFFER_BIT);          // Let our ModelBatch take care of efficient rendering of our ModelInstance         modelBatch.begin(camera);         modelBatch.render(instance, environment);         modelBatch.end();     }      @Override     public void dispose() {         modelBatch.dispose();         model.dispose();     }      @Override     public void resize(int width, int height) { }      @Override     public void pause() { }      @Override     public void resume() { } }  ",
        
        "url": "/wiki/graphics/3d/importing-blender-models-in-libgdx" },{
        "title": "Improving workflow with Gradle",
        "excerpt":
        
        "Contents      Introduction   Optimizing Gradle integration in your IDE and on the command line   Gradle Daemon   Configuration on Demand   Removing Gradle integration from your IDE   Intellij IDEA   Eclipse   libGDX without Gradle   Why does libGDX recommend Gradle   Introduction  Gradle is very powerful, and once you get used to it is a great tool to be versed in and to have in your arsenal, but when mixed with your IDE it can cause workflow problems. This varies from project to project, and from person to person; here is an example of such an issue:   Having a multi-project, multi-flavour project, where you want to be running your desktop build very often to check your latest changes. Due to how Gradle works (it allows the flexibility of accessing and changing any part of the build from any part of the project), it must configure all projects in a multi-project setup before any task is executed. When you have a project with desktop only, this is usually very speedy, but when you add in an android project and a html project the setup time starts to rack up. This is especially noticeable in Intellij IDEA.   Tips to speed up Gradle if you still want IDE integration   You can try a few things to get running more efficiently:   Gradle daemon  The Gradle daemon aims to lower execution time and startup time of tasks especially where tasks are executed frequently.   You can run tasks with the daemon by adding the --daemon flag.   You can also add the option to run on all tasks by editing a gradle.properties file that resides in the root directory of your project. Add the line: org.gradle.daemon=true   Further reading:     Daemon - https://docs.gradle.org/current/userguide/gradle_daemon.html   Configuration on Demand  This setting helps to solve the issue of Gradle configuring the world before execution. Using this setting, the root project is always configured, the project in the directory where the build is executed is configured, and only dependent projects are configured, resulting in much faster build times.   To use this setting add the following to the gradle.properties file in the root directory of your project: Add the line; org.gradle.configureondemand=true   How to remove Gradle IDE integration from your project   Note: This only applies for Eclipse and Intellij IDEA users   With the project that is generated from gdx-setup.jar, the Gradle IDEA, and Gradle Eclipse plugins are applied to all projects. These plugins allow you to generate IDE specific files that you can then work from.   Doing this allows you to keep Gradle out of your IDE, so that when you want to launch any task, it won’t configure your whole project and will be as quick as if you had a regular IDE generated project. This does mean that you need to sort out support for the sub modules in the IDE yourself, android support, robovm ect. (It is still recommended that you do all GWT/html stuff from the command line, as well as using the great packaging tasks that are supplied)   It also allows you to keep the power of Gradle at your disposal, to handle all your dependencies, packaging tasks, signing tasks, deployment and anything else you may want to implement by running from the command line.   Creating your IDEA project  From the command line, run the following command from your project root directory: (If on a UNIX based OS, use ./gradlew to invoke gradle)  gradlew idea   In IDEA: File &gt; OPEN &gt; Locate the .ipr that the task above generates  Creating your Eclipse Project  From the command line, run the following command from your project root directory: (If on a UNIX based OS, use ./gradlew to invoke gradle)  gradlew eclipse   In Eclipse: File &gt; Import &gt; Existing project into workspace &gt; Locate the .project file and import   libGDX without Gradle   You are never forced to use Gradle, it is just recommended. If you want clarification on why it is recommended, check here.   If you don’t want to use Gradle, you have the following choices.      You can use the setup to create your project still and never touch Gradle again. Follow the guide from here and never touch Gradle again.   You can setup your project manually.   You can maintain the old setup and use that to generate your projects.   Doing any of the above will have the following consequences:     You no longer have access to packaging tasks, you will have to package and deploy your projects yourself   You will lose support for IDE’s other than IDEA and Eclipse   You install all the plugins required for your IDE to interpret each of the sub-projects   You manually update your dependencies from now on, don’t forget the platform specific natives too!   Why does libGDX recommend Gradle   Support     Using Gradle, libGDX is supported regardless of what IDE you use, even if you don’t use one. Your development environment doesn’t support Android/RoboVM/HTML/Java? Don’t worry, Gradle can compile/build/run/package your project no matter what you use to code with.   Having one unified system of managing your project makes it easy to get support from other users. The more people that are using the same approach, the larger the support user base grows.   Dependency Management  One of the biggest pros of any build/dependency management system.     Easy to understand   Insanely quick and simple to stay up to date with the latest LibGDX, update one line-refresh-done. No more jar hunting.   Transitive dependencies (Inherit dependencies from projects you depend on)   Custom dependency definitions, depend on anything!   Full compatibility with all repository types (Maven, Ivy, etc)   Flexibility  Gradle is built with Ant and Maven very much in mind, and aims to combine the two to make a very powerful and flexible build automation tool.     Structure your build however you want, add custom tasks, use the rich API to completely customize your build to suit your needs   Want to run your project on a CI server? Just throw it up there install the jenkins Gradle plugin and you’re good to go   The Gradle Wrapper allows you to be free of installing Gradle!   Maintainability  As libGDX grows, extensions are created from scratch or by removing them from core where appropriate, making libGDX more modular to avoid bloat when you don’t need those extensions. (This happened with Box2D). This means more dependencies for you to add and therefore more jars to add. If you are using a minimalist project with no extensions, if you were to manually manage your dependencies that would mean 1 core dependency, 1 desktop dependency, 4 android dependencies, 4 ios dependencies and 3 gwt dependencies.   That is 13 jars you need to track down every time you want to update to the latest libGDX with all the greatest new features. If you use Box2D, Box2D lights, FreeType, etc. you are looking at a lot of dependencies which will quickly become very annoying to update and you will probably put off doing it. The setup has been built with this in mind, and makes it very quick to add these to your project and painless to update by changing one line with the power of Gradle.   The setup itself is very simple, very transparent and maintainable by developers who didn’t create it. This lowers the bus factor for libGDX and reduces strain when the setup requires an update/alteration.   As previously mentioned, the Gradle project that is created by gdx-setup.jar includes tasks to run and package/distribute each project. This allows libGDX to have a unified way of packaging your project, rather than x number of ways for y number of IDE’s. This saves a lot of time in terms of writing documentation, following what every IDE is doing, and providing support on the forums; this gives developers more time to work on great features for libGDX to help make your project awesome.   It revolutionizes everything  Look down. Now look back up. Look down again. You’re sitting on a chair. Look back up. You aren’t using Gradle, your life is OK but it could be better. You read this article and you decide you want to use Gradle. You use the gdx-setup.jar to generate your project and you get tinkering with it. You use the power of Gradle to create 500 flappy clones from the same source code. Look down again. Your chair is now made of gold and there are an infinite number of monkeys with keyboards coding for you. Your life is now better and you can concentrate on the things that matter.  ",
        
        "url": "/wiki/articles/improving-workflow-with-gradle" },{
        "title": "Input handling",
        "excerpt":
        
        "Different platforms have different input facilities. On the desktop users can talk to your application via the keyboard and a mouse. The same is true for browser based games. On Android, the mouse is replaced with a (capacitive) touch screen, and a hardware keyboard is often missing. All (compatible) Android devices also feature an accelerometer and sometimes even a compass (magnetic field sensor).   libGDX abstract all these different input devices. Mouse and touch screens are treated as being the same, with mice lacking multi-touch support (they will only report a single “finger”) and touch-screens lacking button support (they will only report “left button” presses).   Depending on the input device, one can either poll the state of a device periodically, or register a listener that will receive input events in chronological order. The former is sufficient for many arcade games, e.g. analogue stick controls, the latter is necessary if UI elements such as buttons are involved, as these rely on event sequences such as touch down/touch up.   All of the input facilities are accessed via the Input module.      // Check if the A key is pressed    boolean isPressed = Gdx.input.isKeyPressed(Keys.A);   Next  ",
        
        "url": "/wiki/input/input-handling" },{
        "title": "Integrating libgdx and the device camera",
        "excerpt":
        
        "This article shows how to integrate the Android device camera with your libGDX application. This functionality can be used in order to let the user see what going on behind the device while playing and walking on the street, or even have some interaction between  the game and the real world (FPS game using the face detection mechanism?)   A Quick Outline     Create the libGDX view with alpha channel (this is not the default), so that the camera preview will be shown behind it.   Create the DeviceCameraController object responsible for the Camera surface where the preview is drawn.   Interact with the DeviceCameraController from your application code, most of the camera functionality is called asynchronously according to: https://code.google.com/p/libgdx-users/wiki/IntegratingAndroidNativeUiElements3TierProjectSetup   Take a picture            For taking a picture the Camera has a specific state machine that should be followed.       After getting the picture date from the camera, merge it with the libGDX screen-shot and write the result, this process is quite slow and can be done in a separate thread.           Continue with the preview (The picture taken is frozen on the screen until the preview is explicitly restarted) or hide the CameraSurface all together.   The sample application does the following:     draws a cube with the libGDX splash screen as its faces’ texture   upon the user touching the screen the Camera preview is started and drawn behind the cube   when the user untouch the screen:            the Camera starts auto-focusing ;       take a picture if succeeded to focus ;       take a screen-shot of the libGDX scene according to: https://libgdx.com/wiki/graphics/taking-a-screenshot ;       saves the merged picture to the storage ;       and remove the Preview.           The Gory Details   Creating the libGDX application  Initialize your Application in the !MainActivity class in the Android project, but modify the AndroidApplicationConfiguration object before passing it to the initialize method:          AndroidApplicationConfiguration cfg = new AndroidApplicationConfiguration();         cfg.useGL20 = false;         // we need to change the default pixel format - since it does not include an alpha channel          // we need the alpha channel so the camera preview will be seen behind the GL scene         cfg.r = 8;         cfg.g = 8;         cfg.b = 8;         cfg.a = 8;  After the initialization make sure the OpenGL surface format is defined as TRANSLUCENT          if (graphics.getView() instanceof SurfaceView) {         \tSurfaceView glView = (SurfaceView) graphics.getView(); \t\t\t// force alpha channel - I'm not sure we need this as the GL surface is already using alpha channel \t\t\tglView.getHolder().setFormat(PixelFormat.TRANSLUCENT); \t\t} \t}  We also create a new method in the !MainActivity class to help us call asynchronous functions:  \tpublic void post(Runnable r) { \t\thandler.post(r); \t}   Draw frame in the render() method  The important thing in this part is to clear the screen using the glClearColor() with a color with its alpha channel set to 0 when you want the Camera preview to be shown behind your scene.  \tGdx.gl10.glClearColor(0.0f, 0.0f, 0.0f, 0.0f);   After clearing the screen you can draw everything else as usual. note that object drawn with a transparent color will show through to the Camera preview (even if there is an object behind it that has a non transparent color, but is not drawn due to the objects Z-order).   Camera State Machine  The android camera has a specific state machine that must be followed. This state machine can be managed by the application using callbacks. This state machine is managed by the AndroidDeviceCameraController (This class implements an abstract interface defined in the base project, In the Desktop application this interface is implemented by an empty class just for compilation compatibility).   The Camera State machine is: Ready -&gt; Preview -&gt; autoFocusing -&gt; ShutterCalled -&gt; Raw PictureData -&gt; Postview PictureData -&gt; Jpeg PictureData -&gt; Ready   From this state machine the sample code only implements: Ready -&gt; Preview -&gt; autoFocusing -&gt; Jpeg PictureData -&gt; Ready   So The AndroidDeviceCameraController implements the additional two Camera interfaces: Camera.PictureCallback &amp; Camera.AutoFocusCallback  public class AndroidDeviceCameraController implements DeviceCameraControl, Camera.PictureCallback, Camera.AutoFocusCallback { . . . }   Preparing the Camera  We create a CameraSurface object which holds the Camera object and manages the Surface on which the Camera draws the preview images  \tpublic class CameraSurface extends SurfaceView implements SurfaceHolder.Callback { \t\tprivate Camera camera;  \t\tpublic CameraSurface( Context context ) { \t\t\tsuper( context ); \t\t\t// We're implementing the Callback interface and want to get notified \t\t\t// about certain surface events. \t\t\tgetHolder().addCallback( this ); \t\t\t// We're changing the surface to a PUSH surface, meaning we're receiving \t\t\t// all buffer data from another component - the camera, in this case. \t\t\tgetHolder().setType( SurfaceHolder.SURFACE_TYPE_PUSH_BUFFERS ); \t\t}  \t\tpublic void surfaceCreated( SurfaceHolder holder ) { \t\t\t// Once the surface is created, simply open a handle to the camera hardware. \t\t\tcamera = Camera.open(); \t\t}  \t\tpublic void surfaceChanged( SurfaceHolder holder, int format, int width, int height ) { \t\t\t// This method is called when the surface changes, e.g. when it's size is set. \t\t\t// We use the opportunity to initialize the camera preview display dimensions. \t\t\tCamera.Parameters p = camera.getParameters(); \t\t\tp.setPreviewSize( width, height ); \t\t\tcamera.setParameters( p );  \t\t\t// We also assign the preview display to this surface... \t\t\ttry { \t\t\t    camera.setPreviewDisplay( holder ); \t\t\t} catch( IOException e ) { \t\t\t    e.printStackTrace(); \t\t\t} \t\t}  \t\tpublic void surfaceDestroyed( SurfaceHolder holder ) { \t\t\t// Once the surface gets destroyed, we stop the preview mode and release \t\t\t// the whole camera since we no longer need it. \t\t\tcamera.stopPreview(); \t\t\tcamera.release(); \t\t\tcamera = null; \t\t}  \t\tpublic Camera getCamera() { \t\t\treturn camera; \t\t} \t}   The camera object is only created by calling the static Camera.open() method after the surfaceCreated() callback is being called. Until then the camera is not ready and cannot be used.   Showing the Camera preview  When the surfaceChanged() callback is called, we set the camera preview size and sets our CameraSurface object as the Camera preview display.   Now back to the AndroidDeviceCameraController Class. In the next method we prepare the Camera by creating the CameraSurface object (only if needed) and add it as !ContentView to the activity:  \t@Override \tpublic void prepareCamera() { \t\tif (cameraSurface == null) { \t\t\tcameraSurface = new CameraSurface(activity); \t\t} \t\tactivity.addContentView( cameraSurface, new LayoutParams( LayoutParams.WRAP_CONTENT, LayoutParams.WRAP_CONTENT ) ); \t}  This method should be called asynchronous from the libGDX rendering thread so we actually call the prepareCameraAsync() method  \t@Override \tpublic void prepareCameraAsync() { \t\tRunnable r = new Runnable() { \t\t\tpublic void run() { \t\t\t\tprepareCamera(); \t\t\t} \t\t}; \t\tactivity.post(r); \t}   We know that we can pass from the prepare state to the preview state, when the !CameraSurface and the camera objects are ready (by checking that the !CameraSurface &amp; Camera objects exists):  \t@Override \tpublic boolean isReady() { \t\tif (cameraSurface!=null &amp;&amp; cameraSurface.getCamera() != null) { \t\t\treturn true; \t\t} \t\treturn false; \t}   We do this by calling startPreview method via its Async sibling  \t@Override \tpublic synchronized void startPreviewAsync() { \t\tRunnable r = new Runnable() { \t\t\tpublic void run() { \t\t\t\tstartPreview(); \t\t\t} \t\t}; \t\tactivity.post(r); \t} \t@Override \tpublic synchronized void startPreview() { \t\t// ...and start previewing. From now on, the camera keeps pushing preview \t\t// images to the surface. \t\tif (cameraSurface != null &amp;&amp; cameraSurface.getCamera() != null) { \t\t\tcameraSurface.getCamera().startPreview(); \t\t} \t}   In this state the user should see the Camera preview screen (assuming that we cleared the screen with a color having its alpha component sets to 0).   Taking a picture  When we would like to take the picture, we set the suitable Camera parameters (we can do it in any stage before actually taking the picture)  \tpublic void setCameraParametersForPicture(Camera camera) { \t\t// Before we take the picture - we make sure all camera parameters are as we like them \t\t// Use max resolution and auto focus \t\tCamera.Parameters p = camera.getParameters(); \t\tList&lt;Camera.Size&gt; supportedSizes = p.getSupportedPictureSizes(); \t\tint maxSupportedWidth = -1; \t\tint maxSupportedHeight = -1; \t\tfor (Camera.Size size : supportedSizes) { \t\t\tif (size.width &gt; maxSupportedWidth) { \t\t\t\tmaxSupportedWidth = size.width; \t\t\t\tmaxSupportedHeight = size.height; \t\t\t} \t\t} \t\tp.setPictureSize(maxSupportedWidth, maxSupportedHeight); \t\tp.setFocusMode(Camera.Parameters.FOCUS_MODE_AUTO); \t\tcamera.setParameters( p );    \t \t}   In this example we take a picture with the Camera maximal available resolution, and sets the focus mode to AutoFocus. After setting the camera parameters we call the camera autoFocus method, with the proper callback. this callback will be called when the camera is Focused, or after a timeout.  \t@Override \tpublic synchronized void takePicture() { \t\t// the user request to take a picture - start the process by requesting focus \t    \tsetCameraParametersForPicture(cameraSurface.getCamera()); \t    \tcameraSurface.getCamera().autoFocus(this); \t}  When reaching a Focus, we call the actual Camera takePicture() method, with only onJpegPicture callback set.  \t@Override \tpublic synchronized void onAutoFocus(boolean success, Camera camera) { \t\t// Focus process finished, we now have focus (or not) \t\tif (success) { \t\t\tif (camera != null) { \t\t\t\tcamera.stopPreview(); \t\t\t\t// We now have focus take the actual picture \t\t\t\tcamera.takePicture(null, null, null, this); \t\t\t} \t\t} \t}  \t@Override \tpublic synchronized void onPictureTaken(byte[] pictureData, Camera camera) { \t\tthis.pictureData = pictureData; \t}   Taking the libGDX screenshot  In the render() method we wait until the pictureData object contains the Jpeg image and then create a Pixmap object out of this data:  \tif (deviceCameraControl.getPictureData() != null) { // camera picture was actually taken \t\tPixmap cameraPixmap = new Pixmap(deviceCameraControl.getPictureData(), 0, deviceCameraControl.getPictureData().length); \t}  and take the screenshot:  \tpublic Pixmap getScreenshot(int x, int y, int w, int h, boolean flipY) { \t\tGdx.gl.glPixelStorei(GL10.GL_PACK_ALIGNMENT, 1);  \t\tfinal Pixmap pixmap = new Pixmap(w, h, Format.RGBA8888); \t\tByteBuffer pixels = pixmap.getPixels(); \t\tGdx.gl.glReadPixels(x, y, w, h, GL10.GL_RGBA, GL10.GL_UNSIGNED_BYTE, pixels);  \t\tfinal int numBytes = w * h * 4; \t\tbyte[] lines = new byte[numBytes]; \t\tif (flipY) { \t\t\tfinal int numBytesPerLine = w * 4; \t\t\tfor (int i = 0; i &lt; h; i++) { \t\t\t\tpixels.position((h - i - 1) * numBytesPerLine); \t\t\t\tpixels.get(lines, i * numBytesPerLine, numBytesPerLine); \t\t\t} \t\t\tpixels.clear(); \t\t\tpixels.put(lines); \t\t} else { \t\t\tpixels.clear(); \t\t\tpixels.get(lines); \t\t}  \t\treturn pixmap; \t}   The next two operations are CPU and time consuming tasks so they should probably done in a separate thread with some kind of progress bar. In the code sample they are done directly in the rendering thread, so the screen is frozen during this processing.   Merging the screenshot and the Camera picture together  We now have to merge the two Pixmap object. The libGDX Pixmap object can do this for us, but since the camera picture may have different aspect ratio we first need to fix it manually.  \tprivate void merge2Pixmaps(Pixmap mainPixmap, Pixmap overlayedPixmap) { \t\t// merge to data and Gdx screen shot - but fix Aspect Ratio issues between the screen and the camera \t\tPixmap.setFilter(Filter.BiLinear); \t\tfloat mainPixmapAR = (float)mainPixmap.getWidth() / mainPixmap.getHeight(); \t\tfloat overlayedPixmapAR = (float)overlayedPixmap.getWidth() / overlayedPixmap.getHeight(); \t\tif (overlayedPixmapAR &lt; mainPixmapAR) { \t\t\tint overlayNewWidth = (int)(((float)mainPixmap.getHeight() / overlayedPixmap.getHeight()) * overlayedPixmap.getWidth()); \t\t\tint overlayStartX = (mainPixmap.getWidth() - overlayNewWidth)/2; \t\t\tmainPixmap.drawPixmap(overlayedPixmap,  \t\t\t\t\t\t0,  \t\t\t\t\t\t0,  \t\t\t\t\t\toverlayedPixmap.getWidth(),  \t\t\t\t\t\toverlayedPixmap.getHeight(),  \t\t\t\t\t\toverlayStartX,  \t\t\t\t\t\t0,  \t\t\t\t\t\toverlayNewWidth,  \t\t\t\t\t\tmainPixmap.getHeight()); \t\t} else { \t\t\tint overlayNewHeight = (int)(((float)mainPixmap.getWidth() / overlayedPixmap.getWidth()) * overlayedPixmap.getHeight()); \t\t\tint overlayStartY = (mainPixmap.getHeight() - overlayNewHeight)/2; \t\t\tmainPixmap.drawPixmap(overlayedPixmap,  \t\t\t\t\t\t0,  \t\t\t\t\t\t0,  \t\t\t\t\t\toverlayedPixmap.getWidth(),  \t\t\t\t\t\toverlayedPixmap.getHeight(),  \t\t\t\t\t\t0,  \t\t\t\t\t\toverlayStartY,  \t\t\t\t\t\tmainPixmap.getWidth(),  \t\t\t\t\t\toverlayNewHeight);\t\t\t\t\t \t\t} \t}   Saving the resulting image as a Jpeg  In order to save the resulting image we can save it to the storage using the PixmapIO class. however, the CIM format is not interoperable, and the PNG format will probably result in huge files. One way is to save the resulting image as a Jpeg using the Android Bitmap class. This function is implemented inside the AndroidDeviceCameraController, since it is Android specific. However, the libGDX pixel format is RGBA and the Bitmap pixel format is ARGB so we need to shuffle some bits around to get the colors right.  \t@Override \tpublic void saveAsJpeg(FileHandle jpgfile, Pixmap pixmap) { \t\tFileOutputStream fos; \t\tint x=0,y=0; \t\tint xl=0,yl=0; \t\ttry { \t\t\tBitmap bmp = Bitmap.createBitmap(pixmap.getWidth(), pixmap.getHeight(), Bitmap.Config.ARGB_8888); \t\t\t// we need to switch between libGDX RGBA format to Android ARGB format \t\t\tfor (x=0,xl=pixmap.getWidth(); x&lt;xl;x++) { \t\t\t\tfor (y=0,yl=pixmap.getHeight(); y&lt;yl;y++) { \t\t\t\t\tint color = pixmap.getPixel(x, y); \t\t\t\t\t// RGBA =&gt; ARGB \t\t\t\t\tint RGB = color &gt;&gt; 8; \t\t\t\t\tint A = (color &amp; 0x000000ff) &lt;&lt; 24; \t\t\t\t\tint ARGB = A | RGB; \t\t\t\t\tbmp.setPixel(x, y, ARGB); \t\t\t\t} \t\t\t} \t\t\t// Finished Color format conversion \t\t\tfos = new FileOutputStream(jpgfile.file()); \t\t\tbmp.compress(CompressFormat.JPEG, 90, fos); \t\t\t// Finished Comression to JPEG file \t\t\tfos.close(); \t\t} catch (FileNotFoundException e) { \t\t\te.printStackTrace(); \t\t} catch (IOException e) { \t\t\te.printStackTrace(); \t\t} catch (IllegalArgumentException e) { \t\t\te.printStackTrace();\t\t\t \t\t} \t}   Stoping the preview  After finishing to save the picture, we stop the preview and remove the !CameraSurface from the Activity views, and we also stop the camera from sending the preview to the camera surface. Again we need to do this asynchronously.  \t@Override \tpublic synchronized void stopPreviewAsync() { \t\tRunnable r = new Runnable() { \t\t\tpublic void run() { \t\t\t\tstopPreview(); \t\t\t} \t\t}; \t\tactivity.post(r); \t}  \t@Override \tpublic synchronized void stopPreview() {         // stop previewing. \t\tif (cameraSurface != null) { \t\t\tif (cameraSurface.getCamera() != null) { \t\t\t\tcameraSurface.getCamera().stopPreview(); \t\t\t} \t\t\tViewParent parentView = cameraSurface.getParent(); \t\t\tif (parentView instanceof ViewGroup) { \t\t\t\tViewGroup viewGroup = (ViewGroup) parentView; \t\t\t\tviewGroup.removeView(cameraSurface); \t\t\t} \t\t} \t}   Fixing Screen and Camera resolution discrepancies  There is still one problem in our Pixmaps merging process. The resolution of the Camera and our screenshot maybe very different (e.g. in my test on Sumsung Galaxy Ace, I was streaching a 480x320 screenshot to a 2560x1920 picture). one way around it is to enlarge the libGDX view size to a larger size than the actual physical device screen size. This is done using the setFixedSize() function. The actual screen size that can be defined depends on the memory allocated to the GPU and again your mileage may vary. I found that if I do it once during the initialization I can set the virtual screen size to 1920x1280, but this will results with a slower rendering. Other way to do it is to call the setFixedSize() function only during the takingPicture procedure and returning it to its orignal afterwards. However, in this case I managed to set the virtual screen size to 960x640 (probably because some of the GPU memory is already allocated for the screen with the original size)   \tpublic void setFixedSize(int width, int height) { \t\tif (graphics.getView() instanceof SurfaceView) { \t\t\tSurfaceView glView = (SurfaceView) graphics.getView(); \t\t\tglView.getHolder().setFixedSize(width, height); \t\t\tglView.getHolder().setFormat(PixelFormat.TRANSLUCENT); \t\t} \t}  \tpublic void restoreFixedSize() { \t\tif (graphics.getView() instanceof SurfaceView) { \t\t\tSurfaceView glView = (SurfaceView) graphics.getView(); \t\t\tglView.getHolder().setFixedSize(origWidth, origHeight); \t\t\tglView.getHolder().setFormat(PixelFormat.TRANSLUCENT); \t\t} \t}   Some Notes   Note  This code is Android specific, and will not work with the generic cross platform code, but I guess one can provide similar functionality, at least for the desktop application.   Another note  The process of merging and writing the merge image to the storage, takes quite a lot of time due to the format mismatch between libGDX color scheme (RGBA) and the Bitmap class used here (ARGB), if someone find a quicker way, I’d be happy to hear about it.   Last note  I tested this only with a handful of Android devices, and experienced different behaviors probably due to the different GPU implementations. The Samsung GSIII even managed to XOR white elements with the camera image instead of simply overlaying them (Other colors didn’t showed this effects). So your mileage may vary depending on the actual phone used.   The Code   Here’s the full code for the projects’ classes:   Android Project   MainActivity.java:  /*  * Copyright 2012 Johnny Lish (johnnyoneeyed@gmail.com)  *   * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the  * License. You may obtain a copy of the License at  *   * http://www.apache.org/licenses/LICENSE-2.0  *   * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\"  * BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language  * governing permissions and limitations under the License.  */  package com.johnny.camerademo;  import android.graphics.PixelFormat; import android.os.Bundle; import android.view.SurfaceView;  import com.badlogic.gdx.backends.android.AndroidApplication; import com.badlogic.gdx.backends.android.AndroidApplicationConfiguration;  public class MainActivity extends AndroidApplication {     private int origWidth; \tprivate int origHeight;  \t@Override     public void onCreate(Bundle savedInstanceState) {         super.onCreate(savedInstanceState);                  AndroidApplicationConfiguration cfg = new AndroidApplicationConfiguration();         cfg.useGL20 = false;         // we need to change the default pixel format - since it does not include an alpha channel          // we need the alpha channel so the camera preview will be seen behind the GL scene         cfg.r = 8;         cfg.g = 8;         cfg.b = 8;         cfg.a = 8;                  DeviceCameraControl cameraControl = new AndroidDeviceCameraController(this);         initialize(new CameraDemo(cameraControl), cfg);          if (graphics.getView() instanceof SurfaceView) {         \tSurfaceView glView = (SurfaceView) graphics.getView(); \t\t\t// force alpha channel - I'm not sure we need this as the GL surface is already using alpha channel \t\t\tglView.getHolder().setFormat(PixelFormat.TRANSLUCENT); \t\t}         // we don't want the screen to turn off during the long image saving process          graphics.getView().setKeepScreenOn(true);         // keep the original screen size       \torigWidth = graphics.getWidth();     \torigHeight = graphics.getHeight();     }          public void post(Runnable r) {     \thandler.post(r);     }          public void setFixedSize(int width, int height) {         if (graphics.getView() instanceof SurfaceView) {         \tSurfaceView glView = (SurfaceView) graphics.getView(); \t\t\tglView.getHolder().setFormat(PixelFormat.TRANSLUCENT);         \tglView.getHolder().setFixedSize(width, height);         }     }      public void restoreFixedSize() {         if (graphics.getView() instanceof SurfaceView) {         \tSurfaceView glView = (SurfaceView) graphics.getView(); \t\t\tglView.getHolder().setFormat(PixelFormat.TRANSLUCENT);         \tglView.getHolder().setFixedSize(origWidth, origHeight);         }     } }   CameraSurface.java:  /*  * Copyright 2012 Johnny Lish (johnnyoneeyed@gmail.com)  *   * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the  * License. You may obtain a copy of the License at  *   * http://www.apache.org/licenses/LICENSE-2.0  *   * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\"  * BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language  * governing permissions and limitations under the License.  */  package com.johnny.camerademo;  import java.io.IOException;  import android.content.Context; import android.hardware.Camera; import android.view.SurfaceHolder; import android.view.SurfaceView;  public class CameraSurface extends SurfaceView implements SurfaceHolder.Callback {     private Camera camera;       public CameraSurface( Context context ) {         super( context );         // We're implementing the Callback interface and want to get notified         // about certain surface events.         getHolder().addCallback( this );         // We're changing the surface to a PUSH surface, meaning we're receiving         // all buffer data from another component - the camera, in this case.         getHolder().setType( SurfaceHolder.SURFACE_TYPE_PUSH_BUFFERS );     }       public void surfaceCreated( SurfaceHolder holder ) {         // Once the surface is created, simply open a handle to the camera hardware.         camera = Camera.open();     }       public void surfaceChanged( SurfaceHolder holder, int format, int width, int height ) {         // This method is called when the surface changes, e.g. when it's size is set.         // We use the opportunity to initialize the camera preview display dimensions.         Camera.Parameters p = camera.getParameters();         p.setPreviewSize( width, height );         camera.setParameters( p );           // We also assign the preview display to this surface...         try {             camera.setPreviewDisplay( holder );         } catch( IOException e ) {             e.printStackTrace();         }     }       public void surfaceDestroyed( SurfaceHolder holder ) {         // Once the surface gets destroyed, we stop the preview mode and release         // the whole camera since we no longer need it.         camera.stopPreview();         camera.release();         camera = null;     }  \tpublic Camera getCamera() { \t\treturn camera; \t}  }   AndroidDeviceCameraController.java  /*  * Copyright 2012 Johnny Lish (johnnyoneeyed@gmail.com)  *   * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the  * License. You may obtain a copy of the License at  *   * http://www.apache.org/licenses/LICENSE-2.0  *   * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\"  * BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language  * governing permissions and limitations under the License.  */  package com.johnny.camerademo;  import java.io.FileNotFoundException; import java.io.FileOutputStream; import java.io.IOException; import java.util.List; import com.badlogic.gdx.files.FileHandle; import com.badlogic.gdx.graphics.Pixmap;  import android.graphics.Bitmap; import android.graphics.Bitmap.CompressFormat; import android.hardware.Camera; import android.view.ViewGroup; import android.view.ViewParent; import android.view.ViewGroup.LayoutParams;  public class AndroidDeviceCameraController implements DeviceCameraControl, Camera.PictureCallback, Camera.AutoFocusCallback {  \tprivate static final int ONE_SECOND_IN_MILI = 1000; \tprivate final MainActivity activity;     private CameraSurface cameraSurface; \tprivate byte[] pictureData;  \tpublic AndroidDeviceCameraController(MainActivity activity) { \t\tthis.activity = activity; \t}  \t@Override \tpublic synchronized void prepareCamera() { \t\tactivity.setFixedSize(960,640); \t\tif (cameraSurface == null) { \t\t\tcameraSurface = new CameraSurface(activity); \t\t} \t\tactivity.addContentView( cameraSurface, new LayoutParams( LayoutParams.WRAP_CONTENT, LayoutParams.WRAP_CONTENT ) ); \t}  \t@Override \tpublic synchronized void startPreview() { \t\t// ...and start previewing. From now on, the camera keeps pushing preview \t\t// images to the surface. \t\tif (cameraSurface != null &amp;&amp; cameraSurface.getCamera() != null) { \t\t\tcameraSurface.getCamera().startPreview(); \t\t} \t}  \t@Override \tpublic synchronized void stopPreview() {         // stop previewing.         if (cameraSurface != null) { \t\t\tViewParent parentView = cameraSurface.getParent(); \t\t\tif (parentView instanceof ViewGroup) { \t\t\t\tViewGroup viewGroup = (ViewGroup) parentView; \t\t\t\tviewGroup.removeView(cameraSurface); \t\t\t} \t\t\tif (cameraSurface.getCamera() != null) { \t\t\t\tcameraSurface.getCamera().stopPreview(); \t\t\t}         } \t\tactivity.restoreFixedSize(); \t}      public void setCameraParametersForPicture(Camera camera) {         // Before we take the picture - we make sure all camera parameters are as we like them     \t// Use max resolution and auto focus         Camera.Parameters p = camera.getParameters();         List&lt;Camera.Size&gt; supportedSizes = p.getSupportedPictureSizes();         int maxSupportedWidth = -1;         int maxSupportedHeight = -1;         for (Camera.Size size : supportedSizes) {         \tif (size.width &gt; maxSupportedWidth) {         \t\tmaxSupportedWidth = size.width;         \t\tmaxSupportedHeight = size.height;         \t}         }     \tp.setPictureSize(maxSupportedWidth, maxSupportedHeight);     \tp.setFocusMode(Camera.Parameters.FOCUS_MODE_AUTO);         camera.setParameters( p );    \t     }  \t@Override \tpublic synchronized void takePicture() { \t\t// the user request to take a picture - start the process by requesting focus     \t\tsetCameraParametersForPicture(cameraSurface.getCamera());     \t\tcameraSurface.getCamera().autoFocus(this); \t}  \t@Override \tpublic synchronized void onAutoFocus(boolean success, Camera camera) { \t\t// Focus process finished, we now have focus (or not) \t\tif (success) { \t\t\tif (camera != null) { \t\t\t\tcamera.stopPreview(); \t\t\t\t// We now have focus take the actual picture \t\t\t\tcamera.takePicture(null, null, null, this); \t\t\t} \t\t} \t}  \t@Override \tpublic synchronized void onPictureTaken(byte[] pictureData, Camera camera) { \t\t// We got the picture data - keep it \t\tthis.pictureData = pictureData; \t}  \t@Override \tpublic synchronized byte[] getPictureData() { \t\t// Give to picture data to whom ever requested it \t\treturn pictureData; \t}  \t@Override \tpublic void prepareCameraAsync() { \t\tRunnable r = new Runnable() { \t\t\tpublic void run() { \t\t\t\tprepareCamera(); \t\t\t} \t\t}; \t\tactivity.post(r); \t}  \t@Override \tpublic synchronized void startPreviewAsync() { \t\tRunnable r = new Runnable() { \t\t\tpublic void run() { \t\t\t\tstartPreview(); \t\t\t} \t\t}; \t\tactivity.post(r); \t}  \t@Override \tpublic synchronized void stopPreviewAsync() { \t\tRunnable r = new Runnable() { \t\t\tpublic void run() { \t\t\t\tstopPreview(); \t\t\t} \t\t}; \t\tactivity.post(r); \t}  \t@Override \tpublic synchronized byte[] takePictureAsync(long timeout) { \t\ttimeout *= ONE_SECOND_IN_MILI; \t\tpictureData = null; \t\tRunnable r = new Runnable() { \t\t\tpublic void run() { \t\t\t\ttakePicture(); \t\t\t} \t\t}; \t\tactivity.post(r); \t\twhile (pictureData == null &amp;&amp; timeout &gt; 0) { \t\t\ttry { \t\t\t\tThread.sleep(ONE_SECOND_IN_MILI); \t\t\t\ttimeout -= ONE_SECOND_IN_MILI; \t\t\t} catch (InterruptedException e) { \t\t\t\t// TODO Auto-generated catch block \t\t\t\te.printStackTrace(); \t\t\t} \t\t} \t\tif (pictureData == null) { \t\t\tcameraSurface.getCamera().cancelAutoFocus(); \t\t} \t\treturn pictureData; \t}  \t@Override \tpublic void saveAsJpeg(FileHandle jpgfile, Pixmap pixmap) { \t\tFileOutputStream fos; \t\tint x=0,y=0; \t\tint xl=0,yl=0; \t\ttry { \t\t\tBitmap bmp = Bitmap.createBitmap(pixmap.getWidth(), pixmap.getHeight(), Bitmap.Config.ARGB_8888); \t\t\t// we need to switch between libGDX RGBA format to Android ARGB format \t\t\tfor (x=0,xl=pixmap.getWidth(); x&lt;xl;x++) { \t\t\t\tfor (y=0,yl=pixmap.getHeight(); y&lt;yl;y++) { \t\t\t\t\tint color = pixmap.getPixel(x, y); \t\t\t\t\t// RGBA =&gt; ARGB \t\t\t\t\tint RGB = color &gt;&gt; 8; \t\t\t\t\tint A = (color &amp; 0x000000ff) &lt;&lt; 24; \t\t\t\t\tint ARGB = A | RGB; \t\t\t\t\tbmp.setPixel(x, y, ARGB); \t\t\t\t} \t\t\t} \t\t\tfos = new FileOutputStream(jpgfile.file()); \t\t\tbmp.compress(CompressFormat.JPEG, 90, fos); \t\t\tfos.close(); \t\t} catch (FileNotFoundException e) { \t\t\te.printStackTrace(); \t\t} catch (IOException e) { \t\t\te.printStackTrace(); \t\t} catch (IllegalArgumentException e) { \t\t\te.printStackTrace();\t\t\t \t\t} \t}  \t@Override \tpublic boolean isReady() { \t\tif (cameraSurface!=null &amp;&amp; cameraSurface.getCamera() != null) { \t\t\treturn true; \t\t} \t\treturn false; \t} }   Base Project  DeviceCameraControl.java  /*  * Copyright 2012 Johnny Lish (johnnyoneeyed@gmail.com)  *   * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the  * License. You may obtain a copy of the License at  *   * http://www.apache.org/licenses/LICENSE-2.0  *   * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\"  * BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language  * governing permissions and limitations under the License.  */  package com.johnny.camerademo;  import com.badlogic.gdx.files.FileHandle; import com.badlogic.gdx.graphics.Pixmap;  public interface DeviceCameraControl {  \t// Synchronous interface \tvoid prepareCamera();  \tvoid startPreview();  \tvoid stopPreview();  \tvoid takePicture();  \tbyte[] getPictureData();  \t// Asynchronous interface - need when called from a non platform thread (GDX OpenGl thread) \tvoid startPreviewAsync();  \tvoid stopPreviewAsync();  \tbyte[] takePictureAsync(long timeout);  \tvoid saveAsJpeg(FileHandle jpgfile, Pixmap cameraPixmap);  \tboolean isReady();  \tvoid prepareCameraAsync(); }   CameraDemo.java:   /*  * Copyright 2012 Johnny Lish (johnnyoneeyed@gmail.com)  *   * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the  * License. You may obtain a copy of the License at  *   * http://www.apache.org/licenses/LICENSE-2.0  *   * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\"  * BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language  * governing permissions and limitations under the License.  */  package com.johnny.camerademo;  import java.nio.ByteBuffer;  import com.badlogic.gdx.ApplicationListener; import com.badlogic.gdx.Gdx; import com.badlogic.gdx.files.FileHandle; import com.badlogic.gdx.graphics.Color; import com.badlogic.gdx.graphics.GL10; import com.badlogic.gdx.graphics.Mesh; import com.badlogic.gdx.graphics.PerspectiveCamera; import com.badlogic.gdx.graphics.Pixmap; import com.badlogic.gdx.graphics.Texture; import com.badlogic.gdx.graphics.Texture.TextureFilter; import com.badlogic.gdx.graphics.VertexAttribute; import com.badlogic.gdx.graphics.Pixmap.Filter; import com.badlogic.gdx.graphics.Pixmap.Format; import com.badlogic.gdx.graphics.VertexAttributes.Usage;  public class CameraDemo implements ApplicationListener {  \tpublic enum Mode { \t\tnormal, \t\tprepare, \t\tpreview, \t\ttakePicture,  \t\twaitForPictureReady,  \t}  \tpublic static final float vertexData[] = {    \t\t1.0f,  1.0f,  1.0f, Color.toFloatBits(255,255,255,255),  0.0f, 0.0f, // quad/face 0/Vertex 0  \t\t0.0f,  1.0f,  1.0f, Color.toFloatBits(255,255,255,255),  0.0f, 1.0f, // quad/face 0/Vertex 1 \t\t0.0f,  0.0f,  1.0f, Color.toFloatBits(255,255,255,255),  1.0f, 1.0f, // quad/face 0/Vertex 2 \t\t1.0f,  0.0f,  1.0f, Color.toFloatBits(255,255,255,255),  1.0f, 0.0f, // quad/face 0/Vertex 3  \t\t1.0f,  1.0f,  1.0f, Color.toFloatBits(255,255,255,255),  1.0f, 1.0f, // quad/face 1/Vertex 4 \t\t1.0f,  0.0f,  1.0f, Color.toFloatBits(255,255,255,255),  0.0f, 1.0f, // quad/face 1/Vertex 5 \t\t1.0f,  0.0f,  0.0f, Color.toFloatBits(255,255,255,255),  0.0f, 0.0f, // quad/face 1/Vertex 6 \t\t1.0f,  1.0f,  0.0f, Color.toFloatBits(255,255,255,255),  1.0f, 0.0f, // quad/face 1/Vertex 7 \t\t \t\t1.0f,  1.0f,  1.0f, Color.toFloatBits(255,255,255,255),  0.0f, 0.0f, // quad/face 2/Vertex 8 \t\t1.0f,  1.0f,  0.0f, Color.toFloatBits(255,255,255,255),  0.0f, 1.0f, // quad/face 2/Vertex 9 \t\t0.0f,  1.0f,  0.0f, Color.toFloatBits(255,255,255,255),  1.0f, 1.0f, // quad/face 2/Vertex 10 \t\t0.0f,  1.0f,  1.0f, Color.toFloatBits(255,255,255,255),  1.0f, 0.0f, // quad/face 2/Vertex 11 \t\t \t\t1.0f,  0.0f,  0.0f, Color.toFloatBits(255,255,255,255),  1.0f, 1.0f, // quad/face 3/Vertex 12 \t\t0.0f,  0.0f,  0.0f, Color.toFloatBits(255,255,255,255),  0.0f, 1.0f, // quad/face 3/Vertex 13 \t\t0.0f,  1.0f,  0.0f, Color.toFloatBits(255,255,255,255),  0.0f, 0.0f, // quad/face 3/Vertex 14 \t\t1.0f,  1.0f,  0.0f, Color.toFloatBits(255,255,255,255),  1.0f, 0.0f, // quad/face 3/Vertex 15  \t\t0.0f,  1.0f,  1.0f, Color.toFloatBits(255,255,255,255),  1.0f, 1.0f, // quad/face 4/Vertex 16 \t\t0.0f,  1.0f,  0.0f, Color.toFloatBits(255,255,255,255),  0.0f, 1.0f, // quad/face 4/Vertex 17 \t\t0.0f,  0.0f,  0.0f, Color.toFloatBits(255,255,255,255),  0.0f, 0.0f, // quad/face 4/Vertex 18 \t\t0.0f,  0.0f,  1.0f, Color.toFloatBits(255,255,255,255),  1.0f, 0.0f, // quad/face 4/Vertex 19 \t\t \t\t0.0f,  0.0f,  0.0f, Color.toFloatBits(255,255,255,255),  1.0f, 1.0f, // quad/face 5/Vertex 20 \t\t1.0f,  0.0f,  0.0f, Color.toFloatBits(255,255,255,255),  0.0f, 1.0f, // quad/face 5/Vertex 21 \t\t1.0f,  0.0f,  1.0f, Color.toFloatBits(255,255,255,255),  0.0f, 0.0f, // quad/face 5/Vertex 22 \t\t0.0f,  0.0f,  1.0f, Color.toFloatBits(255,255,255,255),  1.0f, 0.0f, // quad/face 5/Vertex 23 \t};   \tpublic static final short facesVerticesIndex[][] = { \t\t{ 0, 1, 2, 3 }, \t\t{ 4, 5, 6, 7 }, \t\t{ 8, 9, 10, 11 }, \t\t{ 12, 13, 14, 15 }, \t\t{ 16, 17, 18, 19 }, \t\t{ 20, 21, 22, 23 } \t};  \tprivate final static VertexAttribute verticesAttributes[] = new VertexAttribute[] {  \t\tnew VertexAttribute(Usage.Position, 3, \"a_position\"),  \t\tnew VertexAttribute(Usage.ColorPacked, 4, \"a_color\"), \t\tnew VertexAttribute(Usage.TextureCoordinates, 2, \"a_texCoords\"), \t};   \tprivate Texture texture;   \tprivate Mesh[] mesh = new Mesh[6];   \tprivate PerspectiveCamera camera;   \tprivate Mode mode = Mode.normal;   \tprivate final DeviceCameraControl deviceCameraControl;   \tpublic CameraDemo(DeviceCameraControl cameraControl) { \t\tthis.deviceCameraControl = cameraControl; \t}  \t@Override \tpublic void create() {\t\t \t\t \t\t// Load the libGDX splash screen texture \t\ttexture = new Texture(Gdx.files.internal(\"data/libgdx.png\")); \t\ttexture.setFilter(TextureFilter.Linear, TextureFilter.Linear); \t \t\t// Create the 6 faces of the Cube \t\tfor (int i=0;i&lt;6;i++) { \t\t\tmesh[i] = new Mesh(true, 24, 4, verticesAttributes); \t\t\tmesh[i].setVertices(vertexData); \t\t\tmesh[i].setIndices(facesVerticesIndex[i]); \t\t} \t\t \t\t// Create the OpenGL Camera \t\tcamera = new PerspectiveCamera(67.0f, 2.0f * Gdx.graphics.getWidth() / Gdx.graphics.getHeight(), 2.0f); \t\tcamera.far = 100.0f; \t\tcamera.near = 0.1f; \t\tcamera.position.set(2.0f,2.0f,2.0f);         camera.lookAt(0.0f, 0.0f, 0.0f);  \t}  \t@Override \tpublic void dispose() { \t\ttexture.dispose(); \t\tfor (int i=0;i&lt;6;i++) { \t\t\tmesh[i].dispose(); \t\t\tmesh[i] = null; \t\t} \t\ttexture = null; \t}  \t@Override \tpublic void render() {\t\t \t\tif (Gdx.input.isTouched()) { \t\t\tif (mode == Mode.normal) { \t\t\t\tmode = Mode.prepare; \t\t\t\tif (deviceCameraControl != null) { \t\t\t\t\tdeviceCameraControl.prepareCameraAsync(); \t\t\t\t} \t\t\t} \t\t} else { // touch removed \t\t\tif (mode == Mode.preview) { \t\t\t\tmode = Mode.takePicture; \t\t\t} \t\t}  \t\tGdx.gl10.glHint(GL10.GL_PERSPECTIVE_CORRECTION_HINT, GL10.GL_NICEST); \t\tif (mode == Mode.takePicture) { \t\t\tGdx.gl10.glClearColor(0.0f, 0.0f, 0.0f, 0.0f); \t\t\tif (deviceCameraControl != null) { \t\t\t\tdeviceCameraControl.takePicture(); \t\t\t} \t\t\tmode = Mode.waitForPictureReady; \t\t} else if (mode == Mode.waitForPictureReady) { \t\t\tGdx.gl10.glClearColor(0.0f, 0.0f, 0.0f, 0.0f);\t\t\t \t\t} else if (mode == Mode.prepare) { \t\t\tGdx.gl10.glClearColor(0.0f, 0.0f, 0.0f, 0.0f); \t\t\tif (deviceCameraControl != null) { \t\t\t\tif (deviceCameraControl.isReady()) { \t\t\t\t\tdeviceCameraControl.startPreviewAsync(); \t\t\t\t\tmode = Mode.preview; \t\t\t\t} \t\t\t} \t\t} else if (mode == Mode.preview) { \t\t\tGdx.gl10.glClearColor(0.0f, 0.0f, 0.0f, 0.0f); \t\t} else { // mode = normal \t\t\tGdx.gl10.glClearColor(0.0f, 0.0f, 0.0f, 1.0f); \t\t} \t\tGdx.gl10.glClear(GL10.GL_COLOR_BUFFER_BIT | GL10.GL_DEPTH_BUFFER_BIT); \t\tGdx.gl10.glEnable(GL10.GL_DEPTH_TEST); \t\tGdx.gl10.glEnable(GL10.GL_TEXTURE);\t\t\t \t\tGdx.gl10.glEnable(GL10.GL_TEXTURE_2D);\t\t\t \t\tGdx.gl10.glEnable(GL10.GL_LINE_SMOOTH); \t\tGdx.gl10.glDepthFunc(GL10.GL_LEQUAL); \t\tGdx.gl10.glClearDepthf(1.0F); \t\tcamera.update(true); \t\tcamera.apply(Gdx.gl10); \t\ttexture.bind(); \t\tfor (int i=0;i&lt;6;i++) { \t\t\tmesh[i].render(GL10.GL_TRIANGLE_FAN, 0 ,4); \t\t} \t\tif (mode == Mode.waitForPictureReady) { \t\t\tif (deviceCameraControl.getPictureData() != null) { // camera picture was actually taken \t\t\t\t// take Gdx Screenshot \t\t\t\tPixmap screenshotPixmap = getScreenshot(0, 0, Gdx.graphics.getWidth(), Gdx.graphics.getHeight(), true); \t\t\t\tPixmap cameraPixmap = new Pixmap(deviceCameraControl.getPictureData(), 0, deviceCameraControl.getPictureData().length); \t\t\t\tmerge2Pixmaps(cameraPixmap, screenshotPixmap); \t\t\t\t// we could call PixmapIO.writePNG(pngfile, cameraPixmap); \t\t\t\tFileHandle jpgfile = Gdx.files.external(\"libGdxSnapshot.jpg\"); \t\t\t\tdeviceCameraControl.saveAsJpeg(jpgfile, cameraPixmap); \t\t\t\tdeviceCameraControl.stopPreviewAsync(); \t\t\t\tmode = Mode.normal; \t\t\t} \t\t} \t} \t \tprivate Pixmap merge2Pixmaps(Pixmap mainPixmap, Pixmap overlayedPixmap) { \t\t// merge to data and Gdx screen shot - but fix Aspect Ratio issues between the screen and the camera \t\tPixmap.setFilter(Filter.BiLinear); \t\tfloat mainPixmapAR = (float)mainPixmap.getWidth() / mainPixmap.getHeight(); \t\tfloat overlayedPixmapAR = (float)overlayedPixmap.getWidth() / overlayedPixmap.getHeight(); \t\tif (overlayedPixmapAR &lt; mainPixmapAR) { \t\t\tint overlayNewWidth = (int)(((float)mainPixmap.getHeight() / overlayedPixmap.getHeight()) * overlayedPixmap.getWidth()); \t\t\tint overlayStartX = (mainPixmap.getWidth() - overlayNewWidth)/2; \t\t\t// Overlaying pixmaps \t\t\tmainPixmap.drawPixmap(overlayedPixmap, 0, 0, overlayedPixmap.getWidth(), overlayedPixmap.getHeight(), overlayStartX, 0, overlayNewWidth, mainPixmap.getHeight()); \t\t} else { \t\t\tint overlayNewHeight = (int)(((float)mainPixmap.getWidth() / overlayedPixmap.getWidth()) * overlayedPixmap.getHeight()); \t\t\tint overlayStartY = (mainPixmap.getHeight() - overlayNewHeight)/2; \t\t\t// Overlaying pixmaps \t\t\tmainPixmap.drawPixmap(overlayedPixmap, 0, 0, overlayedPixmap.getWidth(), overlayedPixmap.getHeight(), 0, overlayStartY, mainPixmap.getWidth(), overlayNewHeight);\t\t\t\t\t \t\t} \t\treturn mainPixmap; \t}  \tpublic Pixmap getScreenshot(int x, int y, int w, int h, boolean flipY) { \t\tGdx.gl.glPixelStorei(GL10.GL_PACK_ALIGNMENT, 1);  \t\tfinal Pixmap pixmap = new Pixmap(w, h, Format.RGBA8888); \t\tByteBuffer pixels = pixmap.getPixels(); \t\tGdx.gl.glReadPixels(x, y, w, h, GL10.GL_RGBA, GL10.GL_UNSIGNED_BYTE, pixels);  \t\tfinal int numBytes = w * h * 4; \t\tbyte[] lines = new byte[numBytes]; \t\tif (flipY) { \t\t\tfinal int numBytesPerLine = w * 4; \t\t\tfor (int i = 0; i &lt; h; i++) { \t\t\t\tpixels.position((h - i - 1) * numBytesPerLine); \t\t\t\tpixels.get(lines, i * numBytesPerLine, numBytesPerLine); \t\t\t} \t\t\tpixels.clear(); \t\t\tpixels.put(lines); \t\t} else { \t\t\tpixels.clear(); \t\t\tpixels.get(lines); \t\t}  \t\treturn pixmap; \t}  \t@Override \tpublic void resize(int width, int height) { \t\tcamera = new PerspectiveCamera(67.0f, 2.0f * width / height, 2.0f); \t\tcamera.far = 100.0f; \t\tcamera.near = 0.1f; \t\tcamera.position.set(2.0f,2.0f,2.0f);         camera.lookAt(0.0f, 0.0f, 0.0f);  \t}  \t@Override \tpublic void pause() { \t}  \t@Override \tpublic void resume() { \t} }  ",
        
        "url": "/wiki/graphics/integrating-libgdx-and-the-device-camera" },{
        "title": "Interfacing with platform specific code",
        "excerpt":
        
        "Oftentimes it can become necessary to access platform specific APIs, e.g., adding advertisement services or a leaderboard functionality. This can be achieved by allowing specific implementation to be defined through a common API interface.   Take the following example, which tries to use a very simple leaderboard API that is only available on Android. For other targets we simply want to log invocations or provide mock return values.   The Android API looks like this:   /** Let's assume this is the API provided by Swarm **/ public class LeaderboardServiceApi {    public void submitScore(String user, int score) { ... } }   The first step is to create an abstraction of the API in form of an interface.   The interface is put into the core project:   public interface Leaderboard {    public void submitScore(String user, int score); }   Next we create specific implementations for each platform and put these into their respective projects.   The following would go into the Android project:   /** Android implementation, can access LeaderboardServiceApi directly **/ public class AndroidLeaderboard implements Leaderboard {    private final LeaderboardServiceApi service;     public AndroidLeaderboard() {       // Assuming we can instantiate it like this       service = new LeaderboardServiceApi();    }     public void submitScore(String user, int score) {       service.submitScore(user, score);    } }   The following would go into the desktop project:   /** Desktop implementation, we simply log invocations **/ public class DesktopLeaderboard implements Leaderboard {    public void submitScore(String user, int score) {       Gdx.app.log(\"DesktopLeaderboard\", \"would have submitted score for user \" + user + \": \" + score);    } }   The following would go into the HTML5 project:   /** Html5 implementation, same as DesktopLeaderboard **/ public class Html5Leaderboard implements Leaderboard {    public void submitScore(String user, int score) {       Gdx.app.log(\"Html5Leaderboard\", \"would have submitted score for user \" + user + \": \" + score);    } }   Next, the ApplicationListener gets a constructor to which we can pass the concrete Leaderboard implementation:   public class MyGame implements ApplicationListener {    private final Leaderboard leaderboard;     public MyGame(Leaderboard leaderboard) {       this.leaderboard = leaderboard;    }     // rest omitted for clarity }   In each starter class we then simply instantiate MyGame, passing the corresponding Leaderboard implementation as an argument, e.g., on the desktop:   public static void main(String[] argv) {    LwjglApplicationConfiguration config = new LwjglApplicationConfiguration();    new LwjglApplication(new MyGame(new DesktopLeaderboard()), config); }   Further threading   A forum discussion on this matter, also mentioning iOS specific things.  ",
        
        "url": "/wiki/app/interfacing-with-platform-specific-code" },{
        "title": "Internationalization and Localization",
        "excerpt":
        
        "Overview   Technically speaking, internationalization is the process of designing a software so that it can potentially be adapted to various languages and regions without engineering changes.  Localization is the process of adapting internationalized software for a specific region or language by adding locale-specific components and translating text. Due to the length of the terms, both internationalization and localization are frequently abbreviated to i18n and l10n respectively, where 18 and 10 stand for the number of letters between the initial and the final letters of the respective words.   In LibGDX, the I18NBundle class is used to store and fetch strings that are locale sensitive. A bundle allows you to easily provide different translations for your application.   Creating Properties Files   Each bundle is a set of properties files that share the same base name, MyBundle in the following example. The characters following the base name indicate the language code, country code, and variant of a Locale, for example:      MyBundle.properties   MyBundle_de.properties   MyBundle_en_GB.properties   MyBundle_fr_CA_VAR1.properties   MyBundle_en_GB, for example, matches the Locale specified by the language code for English (en) and the country code for Great Britain (GB).   You should always create a default properties file, without any language code, country code or variant. In our example the contents of MyBundle.properties are as follows:  game=My Super Cool Game newMission={0}, you have a new mission. Reach level {1}. coveredPath=You covered {0,number}% of the path highScoreTime=High score achieved on {0,date} at {0,time}   To support an additional Locale, your localizers will create an additional properties file that contains the translated values. No changes to your source code are required, because your program references the keys, not the values.   For example, to add support for the Italian language, your localizers would translate the values in MyBundle.properties and place them in a file named MyBundle_it.properties. The contents of MyBundle_it.properties are as follows:   newMission={0}, hai una nuova missione. Raggiungi il livello {1}. coveredPath=Hai coperto il {0,number}% del percorso highScoreTime=High score ottenuto il {0,date} alle ore {0,time}  Notice that the key named game is missing from the Italian properties file since we want the game’s name to remain unchanged regardless of the locale. If a key is not found, the key will be looked up in decreasingly specific properties files until found; for instance, if a key is not found in MyBundle_en_GB.properties, it will be looked up in MyBundle_en.properties, and if not found there either, in the default MyBundle.properties.   Creating a Bundle   An instance of the I18NBundle class manages the named strings for a locale after loading into memory the appropriate properties files. Invoke the factory method createBundle to get a new instance of the desired bundle:  FileHandle baseFileHandle = Gdx.files.internal(\"i18n/MyBundle\"); Locale locale = new Locale(\"fr\", \"CA\", \"VAR1\"); I18NBundle myBundle = I18NBundle.createBundle(baseFileHandle, locale);   The path “i18n/MyBundle”, in this case, refers to files whose names begin with MyBundle that are located in the i18n folder, more precisely in “assets/i18n”. You do not need to create the MyBundle subfolder as well.   Or, if you’re using AssetManager:  assetManager.load(\"i18n/MyBundle\", I18NBundle.class); // ... after loading ... I18NBundle myBundle = assetManager.get(\"i18n/MyBundle\", I18NBundle.class);  If you don’t specify any locale when you invoke createBundle then the default locale is used. This is usually what you want.   Note that you have to leave out the .properties file extension in both cases. If a property file for the specified Locale does not exist, createBundle tries to find the closest match. For example, if you requested the locale fr_CA_VAR1 and the default Locale is en_US, createBundle will look for files in the following order:     MyBundle_fr_CA_VAR1.properties   MyBundle_fr_CA.properties   MyBundle_fr.properties   MyBundle_en_US.properties   MyBundle_en.properties   MyBundle.properties   Note that createBundle looks for files based on the default Locale before it selects the base file MyBundle.properties. If createBundle fails to find a match it throws a MissingResourceException. To avoid throwing this exception, you should always provide a base file with no suffixes.   Fetching Localized Strings   To retrieve a translated value from the bundle, invoke the get method as follows:  String value = myBundle.get(key);  The string returned by the get method corresponds to the specified key. The string is in the proper language, provided that a properties file exists for the specified locale. If no string for the given key can be found by the get method (or its parametric form format) a MissingResourceException is thrown.   Translated strings can contain formatting parameters. To substitute values for those, invoke the format method as follows:  String value = myBundle.format(key, arg1, arg2, ...);   For example, to retrieve the strings from the properties files above your code might look like this:  String game = myBundle.format(\"game\"); String mission = myBundle.format(\"newMission\", player.getName(), nextLevel.getName()); String coveredPath = myBundle.format(\"coveredPath\", path.getPerc()); String highScoreTime = myBundle.format(\"highScoreTime\", highScore.getDate());   When a string has no arguments, you might think that the methods get and format are equivalent. This is not entirely true. The get method returns the string exactly as it was specified in the properties file, while the format method might make replacements for escaping even if no arguments are present. As a result, get is slightly faster, but unless you are certain that none of your translations contain any escaping it’s best to always use format.   Message Format   As we have seen, the strings in a properties file can contain parameters. These strings are commonly called patterns and follow the syntax specified by the java.text.MessageFormat API. In short, a pattern can contain zero or more formats of the form {index, type, style} where the type and the style are optional. Refer to the official JavaDoc of the MessageFormat class to learn all its features.   Important: There is one change that libGDX makes to MessageFormat’s syntax, because the default escaping rules have proved to be somewhat confusing to localizers. If you want to use a literal { in your string, normally you would need to escape it using single quotes ('). In libGDX however, you just double it; for example, {{0} is interpreted as the literal string {0}, without any format elements. As a result, single quotes never need to be escaped!   Note that formats are localizable. This means that typed data like numbers, dates and times will be automatically expressed in the typical form of the specific locale. For example, the float number 3.14 becomes 3,14 for the Italian locale; notice the comma in place of the decimal point. (If you’re using GWT, there are some limitations to this.)   Unlike java.util.Properties, the default encoding is UTF-8. If, for whatever reason, you don’t want to use UTF-8 encoding, invoke one of the overloaded forms of the createBundle method that allows you to specify the desired encoding.   Plural Forms   Plural forms are supported through the standard choice format provided by MessageFormat. See the official documentation of the class java.text.ChoiceFormat. I’m going to show you just an example. Let’s consider the following property:  collectedCoins=You collected {0,choice,0#no coins|1#one coin|1&lt;{0,number,integer} coins|100&lt;hundreds of coins} along the path.  You can retrieve the localized string as usual:  System.out.println(myBundle.format(\"collectedCoins\", 0)); System.out.println(myBundle.format(\"collectedCoins\", 1)); System.out.println(myBundle.format(\"collectedCoins\", 32)); System.out.println(myBundle.format(\"collectedCoins\", 117));  And the output result would be like the following:  You collected no coins along the path. You collected one coin along the path. You collected 32 coins along the path. You collected hundreds of coins along the path.   It’s worth noting that the choice format can properly handle nested formats as we did with {0,number,integer} inside {0,choice,...}.   GWT Limitations and Compatibility   As said before, the I18N system provided by libGDX is cross-platform. However there are some limitations when it comes to the GWT back-end. In particular:     Simple format: The format syntax of java.text.MessageFormat is not fully supported. You’ll have to stick to a simplified syntax where formats are made only by their index, i.e. {index}. Format’s type and style are not supported and cannot be used; otherwise an IllegalArgumentException is thrown.   Non localizable arguments: Formats are never localized, meaning that the arguments passed to the format method are converted to a string with the toString method, so without taking into account the bundle’s locale.   If your application can run on both GWT and non-GWT back-ends, you should call I18NBundle.setSimpleFormat(true) when the application starts. This way all subsequent invocations of the factory method createBundle will create bundles having the same behavior on all back-ends. If you don’t do this, you’ll get the localized string 3,14 for the Italian locale on a non-GWT back-end (notice the comma and the rounding) and the non-localized string 3.1415927 for the same locale on the GWT back-end. On the contrary, if simpleFormat is set to true you’ll get the non-localized string 3.1415927 for any locale on any back-end.   Multiple Bundles   Of course you can use multiple bundles in your application. For example, you might want to use a different bundle for each level of your game. Using multiple bundles offers some advantages:     Your code is easier to read and to maintain.   You’ll avoid huge bundles, which may take somewhat long to load into memory.   You can reduce memory usage by loading each bundle only when needed.   iOS  For iOS, you need to specify the supported language in the Info.plist.xml file. Add the following for supporting English, Spanish and German:   &lt;key&gt;CFBundleLocalizations&lt;/key&gt; &lt;array&gt;     &lt;string&gt;en&lt;/string&gt;     &lt;string&gt;es&lt;/string&gt;     &lt;string&gt;de&lt;/string&gt; &lt;/array&gt;  ",
        
        "url": "/wiki/internationalization-and-localization" },{
        "title": "Interpolation",
        "excerpt":
        
        "Interpolation   Commonly known as tweening, interpolation is useful for generating values between two discrete end points using various curve functions. Often used with key-framed animation, interpolation allows an animator to specify a sparse collection of explicit frames for an animation and then generate a smooth transition between these frames computationally. The simplest form of interpolation is linear interpolation such as that available directly in the Vector2 (code) and Vector3 (code) classes. The Interpolation (code) class provides more interesting results by using non-linear curve functions to interpolate values.   Types of Interpolation   These are the basic built-in types of interpolation:      Bounce   Circle   Elastic   Exponential   Fade   Power   Sine   Swing   Code Example   // Written in Kotlin  val easAlpha:Interpolation = Interpolation.fade val lifeTime:Int = 2 var elapsed:Float = 0f .. fun update(delta:Float) {     elapsed += delta      val progress = Math.min(1f, elapsed/lifeTime)   // 0 -&gt; 1      val alpha = easAlpha.apply(progress) }    Most types offer three varieties which bias towards one or the other or both ends of the curve creating an easing in or out of the animation.   See InterpolationTest for a visual display of each interpolation.   Visual display of interpolations                  bounce       bounceIn       bounceOut       circle                                                             circleIn       circleOut       elastic       elasticIn                                                       elasticOut       exp5       exp5In       exp5Out                                                       exp10       exp10In       exp10Out       fade                                                       fastSlow       linear       pow2       pow2In                                                       pow2InInverse       pow2Out       pow2OutInverse       pow3                                                       pow3In       pow3InInverse       pow3Out       pow3OutInverse                                                       pow4       pow4In       pow4Out       pow5                                                       pow5In       pow5Out       sine       sineIn                                                       sineOut       slowFast       smooth       smooth2                                                       smoother       swing       swingIn       swingOut                                                ",
        
        "url": "/wiki/math-utils/interpolation" },{
        "title": "Issue Tracker",
        "excerpt":
        
        "If you have been sent to this page it is because you have posted something on the libGDX issue tracker that is not an issue. We handle thousands of issues and cannot burn our time explaining to each person how an issue tracker works. Instead we send you here, which explains your options.   What do I do now?   If you need help, we have an extensive guide called Getting Help. Read it. If you have questions you need to ask, the fastest way to get help is our official Discord server.   Why are you doing this to me?   The issue tracker is for reporting problems with the way libGDX works. It is the wrong place to ask questions of any kind. It is important to reduce clutter on the issue tracker so we can focus on the work that needs to be done to improve libGDX.  ",
        
        "url": "/wiki/misc/issue-tracker" },{
        "title": "Java Development Kit   Selection",
        "excerpt":
        
        "   Introduction   Background   Distributions   Versions   Caveat: At the release of this wiki page, Java 15 is the current released Java Development Kit (JDK).   Introduction  Every project will require the use of at least one Java Development Kit (JDK). This is the source, root, or core  of the code that will be used in each project. It may be a quick decision to just select the most recent JDK, though this might end in frustration and confusion during the build process. This wiki will review the different JDK versions and distributions. Yes, even distributions are different! Distributions are provided by various companies and may contain additional features from the root JDK. The wiki page objective is to remove any confusion about which version or distribution to use.   Background  Distribution  Various distributions from vendors existed prior to Oracle changing its licensing  structure and were used for various reasons, such as using variation(s) of the Java Virtual Machine (JVM) for  performance. So what does this mean for you as a developer for yourself or your company? Well we have good news  about which JDK to choose. Free open source distributions exist for commercial use. These will be listed in  Distributions.      Oracle - JDK Licensing - FAQs   Lakesidesoftware - Article - 2019   Aspera - Article - 2019   Version  With the change of the licensing model Oracle also updated their release pipeline to include regular feature updates at a faster rate. The overall goal of the JDK development team is to move  forward with the bigger Java Enhancement Proposals (JEPs) (Valhalla,  Panama) by working out the foundational JEPs. Some smaller but related  features that are foundational to the bigger JEPs are (Records,   Sealed Classes) will be released in between the major features.      All JEPs   Versions  Since each platform that Java works on is different the code; translation may leaves features out or may not be capable of being emulated from the desktop to mobile or web. Thus, if developing on a specific platform, you may not have much of a choice on which Java versions. As a developer you may also be faced with having to use multiple JDKs to support the features and/or debugging capabilities required by the project. Most of the versions listed  will be Long Term Supported (LTS). Hot swapping code is a feature that allows one to compile and swap code during runtime debugging. This feature enhanced by using Dynamic Code Execution Virtual Machine (DCEVM). This is  technically a separate distribution DCEVM. Some developers will use JDK 8 for language compatibility, JDK 11 for DCEVM and JDK 14 for packaging.   Note: LTS (Long Term Support)                  Version       Desktop       Android       iOS       HTML       DCEVM       ARM       Notes                       7       X       X       X       X       X                                 8       X       X       X       X       X               LTS - Most compatible                 9       X                       X                       Modules - Oracle Licensing Changed                 11       X                       X       X               LTS &amp; HTML Support with 3rd Party Backend                 15       X                                               JPackage                 16                                                       TBD - March - Sept 2021                 17                                                       TBD - Sept 2021 - ? (LTS)           Of these versions it is recommended to use LTS versions &amp; distributions. This way any issues that arise in the  JDK will be fixed or supported by the companies involved in the JDK maintenance.   Distributions  JDKs Distribution info can be found here sdkman.io - JDKs.  ",
        
        "url": "/wiki/articles/java-development-kit-selection" },{
        "title": "jnigen",
        "excerpt":
        
        "jnigen is a small library that can be used with or without libGDX which allows C/C++ code to be written inline with Java source code. This increases the locality of code that conceptually belongs together (the Java native class methods and the actual implementation) and makes refactoring a lot easier compared to the usual JNI workflow. Arrays and direct buffers are converted for you, further reducing boilerplate. Building the natives for Windows, Linux, OS X, and Android is handled for you. jnigen also provides a mechanism for loading native libraries from a JAR at runtime, which avoids “java.library.path” troubles.   Setup   You will need MinGW for both 32 and 64 bit. After installation, be sure the bin directory is on your path.   Note that gdx-jnigen is a Java project. It has a blank AndroidManifest.xml because the Android NDK requires it, but it is not an Android project.   Windows      MinGW 32 bit Run mingw-get-setup.exe, install with the GUI, choose mingw32-base and mingw32-gcc-g++ under “Basic Setup”, then Installation -&gt; Apply Changes.   MinGW 64 bit Download the MinGW 64 bit binaries and unzip.   Linux      Ubuntu and other Debian-based systems (unverified)   sudo apt-get install g++-mingw-w64-i686 g++-mingw-w64-x86-64      Arch Linux (this installs a compiler for both 32-bit and 64-bit; read more)   sudo pacman -S mingw-w64-gcc      Fedora, CentOS and RHEL-based systems   sudo dnf install mingw32-gcc-c++ mingw64-gcc-c++ mingw32-winpthreads-static mingw64-winpthreads-static   Quickstart   Here is a barebones example, first the Java source with inline native code:   public class Example { \t// @off  \tstatic public native int add (int a, int b); /* \t\treturn a + b; \t*/  \tpublic static void main (String[] args) throws Exception { \t\tnew SharedLibraryLoader().load(\"my-native-lib\"); \t\tSystem.out.println(add(1, 2)); \t} }   The @off comment turns off the Eclipse source formatter for the rest of the file. This prevents it from ruining the formatting of our native code in the comments. This feature has to be turned “on” in Eclipse preferences: Java &gt; Code Style &gt; Formatter. Click on “Edit” button, “Off/On Tags”, check off “Enable Off/On tags”.   Next, a native method is defined. Normally for JNI you would need to run javah to generate stub source files which you would edit and need to keep up to date with the Java source. With jnigen, you just use a multi-line comment immediately after the native method which contains your native code. The parameters for the native method are available to your native code.   Lastly, a main method is defined. The SharedLibraryLoader extracts the appropriate native library from the classpath and loads it. This allows you to distribute your native libraries inside your JARs and you will never have problems with java.library.path. If using jnigen without libgdx, you can use JniGenSharedLibraryLoader instead which does the same thing. JniGenSharedLibraryLoader is the only class from jnigen that is needed at runtime, if you choose to use it.   Here is the build for the native library above:   public class ExampleBuild { \tstatic public void main (String[] args) throws Exception { \t\tNativeCodeGenerator jnigen = new NativeCodeGenerator(); \t\tjnigen.generate(\"src\", \"bin\", \"jni\", new String[] {\"**/Example.java\"}, null);  \t\tBuildTarget win32 = BuildTarget.newDefaultTarget(TargetOs.Windows, false); \t\twin32.compilerPrefix = \"mingw32-\"; \t\tBuildTarget win64 = BuildTarget.newDefaultTarget(TargetOs.Windows, true); \t\tBuildTarget linux32 = BuildTarget.newDefaultTarget(TargetOs.Linux, false); \t\tBuildTarget linux64 = BuildTarget.newDefaultTarget(TargetOs.Linux, true); \t\tBuildTarget mac = BuildTarget.newDefaultTarget(TargetOs.MacOsX, true);  \t\tnew AntScriptGenerator().generate(new BuildConfig(\"my-native-lib\"), win32, win64); \t\tBuildExecutor.executeAnt(\"jni/build-windows32.xml\", \"-v\", \"-Drelease=true\", \"clean\", \"postcompile\"); \t\tBuildExecutor.executeAnt(\"jni/build-windows64.xml\", \"-v\", \"-Drelease=true\", \"clean\", \"postcompile\"); \t\t// BuildExecutor.executeAnt(\"jni/build-linux32.xml\", \"-v\", \"-Drelease=true\", \"clean\", \"postcompile\"); \t\t// BuildExecutor.executeAnt(\"jni/build-linux64.xml\", \"-v\", \"-Drelease=true\", \"clean\", \"postcompile\"); \t\t// BuildExecutor.executeAnt(\"jni/build-macosx32.xml\", \"-v\", \"-Drelease=true\", \"clean\", \"postcompile\"); \t\tBuildExecutor.executeAnt(\"jni/build.xml\", \"-v\", \"pack-natives\"); \t} }  You have to download JNI-GEN.JAR and add it as dependency in your Android Studio.   First, NativeCodeGenerator is used to generate the native source from the Java source. It needs to be told where to find the Java source, the class files for that source, the directory to output the native source, a list of glob patterns for what Java source files to process, and a list of glob patterns for what source files to exclude.   Next, build targets are defined for each platform. BuildTarget.newDefaultTarget is used to provide reasonable defaults for each target. This build is meant to be built on Windows, so the Windows 32 bit default compilerPrefix of “i686-w64-mingw32-“ (which is good for building on Linux) needs to be changed to “mingw32-“. There are other fields on BuildTarget that can be customized, such as source files to include/exclude, header directories, C/C++ flags, linker flags, linked libraries, etc.   Next, AntScriptGenerator is used to output the Ant build scripts. The BuildConfig specifies global build settings, such as the name of the native library (“my-native-lib” here), input and output directories, etc.   Lastly, the Ant scripts are run to build the actual native libraries and pack them into a JAR. To run the main method from the example above, the JAR just needs to be on the classpath.   How it works   jnigen has two parts:      Inspect Java source files in a specific folder, detect native methods and the attached C++ implementation, and spit out a C++ source file and header, similar to what you’d create manually with JNI.   Provide a generator for Ant build scripts that build the native source for every platform.   Native code generation   Here’s an example of Java/C++ mixed in a single Java source file as understood by jnigen (taken from BufferUtils):   private static native ByteBuffer newDisposableByteBuffer (int numBytes); /*    char* ptr = (char*)malloc(numBytes);    return env-&gt;NewDirectByteBuffer(ptr, numBytes); */  private native static void copyJni (float[] src, Buffer dst, int numFloats, int offset); /*    memcpy(dst, src + offset, numFloats &lt;&lt; 2 ); */   The C++ code is contained in a block comment after the Java native method declaration. Java side input parameters will be available to the C++ code by their Java names, and, if possible marshalled for a limited subset of types. The following marshalling takes place:      Primitive types are passed as is, using jint, jshort, jboolean, etc as their types.   One dimensional primitive type arrays are converted to typed pointers you can directly access. The arrays are automatically locked and unlocked for you via JNIEnv::GetPrimitiveArrayCritical and JNIEnv::ReleasePrimitiveArrayCritical.   Direct buffers are converted to unsigned char* pointers via JNIEnv::GetDirectBufferAddress. Note that the position of the buffer is not taken into account!   Any other type will be passed with its respective JNI type, eg normal Java objects will be passed as jobject.   The above two jnigen native methods would translate to the following C++ source (there would be a corresponding header file as well):   JNIEXPORT jobject JNICALL Java_com_badlogic_gdx_utils_BufferUtils_newDisposableByteBuffer(JNIEnv* env, jclass clazz, jint numBytes) { //@line:334    char* ptr = (char*)malloc(numBytes);    return env-&gt;NewDirectByteBuffer(ptr, numBytes); }  JNIEXPORT void JNICALL Java_com_badlogic_gdx_utils_BufferUtils_copyJni___3FLjava_nio_Buffer_2II(JNIEnv* env, jclass clazz, jfloatArray obj_src, jobject obj_dst, jint numFloats, jint offset) {    unsigned char* dst = (unsigned char*)env-&gt;GetDirectBufferAddress(obj_dst);    float* src = (float*)env-&gt;GetPrimitiveArrayCritical(obj_src, 0);  //@line:348    memcpy(dst, src + offset, numFloats &lt;&lt; 2 );     env-&gt;ReleasePrimitiveArrayCritical(obj_src, src, 0); }   As you can see, the marshalling is inserted at the top and bottom of the method automatically in copyJni(). If you return from your JNI method in place other than the end of your method, jnigen will wrap your function with a second function that does all the marshalling, like here: Java source and the C++ translation.   jnigen outputs the Java line numbers in the generated native code, telling us where in the original Java source file the C++ appeared. This is helpful when building jnigen generated C++ code, as the Ant script will spit out errors with Java line numbers to which we can jump to by clicking on the line in the console.   To let jnigen go through your source code and generated C/C++ header and source files, you do the following:   new NativeCodeGenerator().generate(\"src\", \"bin\", \"jni\", new String[] {\"**/*\"}, null);   You specify the source folder, the folder containing the compiled .class files of your Java classes, the Java files to include (using Ant path patterns) and the files you want to exclude. See the source of NativeCodeGenerator for more info.   Build script generation   Once the native code files have been generated, we also want to create build scripts for all supported platforms. This currently includes Windows (32-/64-bit), Linux (32-/64-bit), Mac OS X (x86, 32-/64-bit), Android (arm6/arm7) and iOS (i386, arm7). The build script generator of jnigen has template Ant script files that can be parametrized for each platform. The parameters are specified via a BuildTarget. You can create a BuildTarget for a specific platform like this:   BuildTarget linux32 = BuildTarget.newDefaultTarget(TargetOS.Linux, false);   This creates a default build target for Linux 32-bit. You can then add additional, platform specific settings to the BuildTarget. Repeat the process for other targets.   Once all targets are configured, you pull them together in a BuildConfig. You specify the name of the shared/static library, eg “gdx” which will end up as gdx.dll on Windows, libgdx.so on Linux and Android, libgdx.dylib on Mac OS X and libgdx.a on iOS. You can also specify there the build files should be output to, etc. The easiest way of using the config looks like this:   BuildConfig config = new BuildConfig(\"gdx\");   One the targets and config are in place, it’s time to generate the Ant scripts via the AntScriptGenerator:   new AntScriptGenerator().generate(config, linux32, linux64, windows32, windows64, macosx, android, ios)   The generated Ant build scripts will compile the native libraries and package them in a JAR. They can be executed from the command line, or from Java:   // Build natives: BuildExecutor.executeAnt(\"jni/build-windows32.xml\", \"-v\", \"-Drelease=true\", \"clean\", \"postcompile\"); BuildExecutor.executeAnt(\"jni/build-windows64.xml\", \"-v\", \"-Drelease=true\", \"clean\", \"postcompile\"); // etc // JAR natives: BuildExecutor.executeAnt(\"jni/build.xml\", \"-v\", \"pack-natives\");   More   A video of Mario showing off jnigen:      Jglfw makes extensive use of jnigen and shows how easy it can be to wrap a native API for use in Java. Note the /*JNI comment is used to define includes, statics, and functions.   Here are a number of jnigen builds that can serve as examples of varying complexity:      Jglfw’s build   AudioBuild   BulletBuild   DesktopControllersBuild   FreetypeBuild   ImageBuild   ccache  Using ccache is highly recommended if you build for all platforms (Linux, Windows, Mac OS X, Android, iOS, arm arm-v7, x86, x64 and all permutations). For libgdx, we use a very simple setup. On our build server, we have an /opt/ccache directory that houses a bunch of shell scripts, one for each compiler binary:   jenkins@badlogic:~/workspace/libgdx/gdx/jni$ ls -lah /opt/ccache/ -rwxr-xr-x  1 jenkins  www-data   44 Apr  6 13:14 g++ -rwxr-xr-x  1 jenkins  www-data   44 Apr  6 13:14 gcc -rwxr-xr-x  1 jenkins  www-data   78 Apr  6 13:15 i686-w64-mingw32-g++ -rwxr-xr-x  1 jenkins  www-data   78 Apr  6 13:15 i686-w64-mingw32-gcc -rwxr-xr-x  1 jenkins  www-data   82 Apr  6 13:15 x86_64-w64-mingw32-g++ -rwxr-xr-x  1 jenkins  www-data   82 Apr  6 13:15 x86_64-w64-mingw32-gcc   Each of these scripts looks like this (with a modified executable name of course):   echo \"g++ ccache!\" ccache /usr/bin/g++ \"$@\"   To make ccache work, just make sure the /opt/ccache directory is first in your PATH:   export PATH=/opt/ccache:$PATH   Any time one of the compilers is invoked, the shell scripts redirect to ccache.   For Android it is sufficient to set NDK_CCACHE   export NDK_CCACHE=ccache  ",
        
        "url": "/wiki/utils/jnigen" },{
        "title": "Logging",
        "excerpt":
        
        "The Application interface provides simple logging facilities that give granular control over which messages should be logged.   A message can be a normal info message, an error message with an optional exception or a debug message:   Gdx.app.log(\"MyTag\", \"my informative message\"); Gdx.app.error(\"MyTag\", \"my error message\", exception); Gdx.app.debug(\"MyTag\", \"my debug message\");   On desktop, the messages are logged to the console; on Android to LogCat; and on GWT they are logged either to the browser console or to a TextArea provided in the GwtApplicationConfiguration.   Logging can be limited to a specific logging level:   Gdx.app.setLogLevel(logLevel);   where logLevel can be one of the following values:      Application.LOG_NONE: mutes all logging.   Application.LOG_DEBUG: logs all messages.   Application.LOG_ERROR: logs only error messages.   Application.LOG_INFO: logs error and normal messages.  ",
        
        "url": "/wiki/app/logging" },{
        "title": "Managing your assets",
        "excerpt":
        
        "Why would I want to use the AssetManager   AssetManager (code) helps you load and manage your assets. It is the recommended way to load your assets, due to the following nice behaviors:      Loading of most resources is done asynchronously, so you can display a reactive loading screen while things load   Assets are reference counted. If two assets A and B both depend on another asset C, C won’t be disposed until A and B have been disposed. This also means that if you load an asset multiple times, it will actually be shared and only take up memory once!   A single place to store all your assets.   Allows to transparently implement things like caches (see FileHandleResolver below)   Still with me? Then read on.   Creating an AssetManager   This part is rather simple:   AssetManager manager = new AssetManager();   This sets up a standard AssetManager, with all the loaders libGDX has in store at the moment. Let’s see how the loading mechanism works.   Caution: don’t make your AssetManager or any other resources (like Texture, etc.) static, unless you properly manage them. E.g. the following code will cause issues:   public static AssetManager assets = new AssetManager();   This will cause problems on Android because the life-cycle of the static variable is not necessarily the same as the life-cycle of your application. Therefore the AssetManager instance of a previous instance of your application might be used for the next instance, while the resources are no longer valid. This typically would cause black/missing textures or incorrect assets.   On Android, it is even possible for multiple instances of your Activity to be active at the same time, so do not think you’re safe even if you handle life-cycle methods properly! (See this StackOverflow question for details.)   Adding Assets to the queue   To load assets, the AssetManager needs to know how to load a specific type of asset. This functionality is implemented via AssetLoaders. There are two variants, SynchronousAssetLoader and AsynchronousAssetLoader. The former loads everything on the rendering thread, the latter loads parts of the asset on another thread, e.g., the Pixmap needed for a Texture, and then loads the OpenGL dependent part on the rendering thread. The following resources can be loaded out of the box with the AssetManager as constructed above.      Pixmaps via PixmapLoader (code)   Textures via TextureLoader (code)   BitmapFonts via BitmapFontLoader (code)   FreeTypeFonts via FreeTypeFontLoader   TextureAtlases via TextureAtlasLoader (code)   Music instances via MusicLoader (code)   Sound instances via SoundLoader (code)   Skins via SkinLoader (code)   Particle Effects via ParticleEffectLoader (code)   I18NBundles via I18NBundleLoader (code)   FreeTypeFontGenerator via FreeTypeFontGeneratorLoader (code)   Loading a specific asset is simple:   manager.load(\"data/mytexture.png\", Texture.class); manager.load(\"data/myfont.fnt\", BitmapFont.class); manager.load(\"data/mymusic.ogg\", Music.class);   These calls will enqueue those assets for loading. The assets will be loaded in the order we called the AssetManager.load() method. Some loaders allow you to also pass parameters to them via AssetManager.load(). Say we want to specify a non-default filter and mipmapping setting for loading a texture:   TextureParameter param = new TextureParameter(); param.minFilter = TextureFilter.Linear; param.genMipMaps = true; manager.load(\"data/mytexture.png\", Texture.class, param);   Look into the loaders mentioned above to find out about their parameters.   Actually loading the assets  So far we only queued assets to be loaded. The AssetManager does not yet load anything. To kick this off we have to call AssetManager.update() continuously, say in our ApplicationListener.render() method:   public MyAppListener implements ApplicationListener {     public void render() {       if(manager.update()) {          // we are done loading, let's move to another screen!       }        // display loading information       float progress = manager.getProgress()       ... left to the reader ...    } }   As long as AssetManager.update() returns false you know it’s still loading assets. To poll the concrete state of loading you can use AssetManager.getProgress(), which returns a number between 0 and 1 indicating the percentage of assets loaded so far. There are other methods in AssetManager that give you similar information, like AssetManager.getLoadedAssets() or AssetManager.getQueuedAssets(). You have to call AssetManager.update() to keep loading!   If you want to block and make sure all assets are loaded you can call:   manager.finishLoading();   This will block until all the assets that have been queued are actually done loading. Kinda defeats the purpose of asynchronous loading, but sometimes one might need it (e.g., loading the assets needed to display the loading screen itself).   Optimize loading  In order to perform loading as efficiently/fast as possible while trying to keep a certain FPS, AssetManager.update() should be called with parameters. E.g. AssetManager.update(17) - In this case the AssetManager blocks for at least 17 milliseconds (only less if all assets are loaded) and loads as many assets as possible, before it returns control back to the render method. Blocking for 16 or 17 milliseconds leads to ~60FPS as 1/60*1000 = 16.66667. Note that it might block for longer, depending on the asset that is being loaded so don’t take the desired FPS as guaranteed.   Loading a TTF using the AssetHandler   Loading a TrueType file via the AssetHandler requires only a little bit extra tweaking. Before we can load a TTF, we need to set the type of loader we’re going to use for FreeType fonts. This is done with the following:   FileHandleResolver resolver = new InternalFileHandleResolver(); manager.setLoader(FreeTypeFontGenerator.class, new FreeTypeFontGeneratorLoader(resolver)); manager.setLoader(BitmapFont.class, \".ttf\", new FreetypeFontLoader(resolver));   Next, we’ll want to create a FreeTypeFontLoaderParameter that defines 1) our actual font file, and 2) our font size. There are other parameters we can define here, too, when you have time to dig more.   Let’s say we want to create two different fonts: a smaller, sans-serif font that will be used for one type of writing text, and a larger, serif font for titles and other fun things. I’ve decided to use Arial and Georgia for these two fonts, respectively. Here’s how I can load them using the AssetManager:   // First, let's define the params and then load our smaller font FreeTypeFontLoaderParameter mySmallFont = new FreeTypeFontLoaderParameter(); mySmallFont.fontFileName = \"arial.ttf\"; mySmallFont.fontParameters.size = 10; manager.load(\"arial.ttf\", BitmapFont.class, mySmallFont);  // Next, let's define the params and then load our bigger font FreeTypeFontLoaderParameter myBigFont = new FreeTypeFontLoaderParameter(); myBigFont.fontFileName = \"georgia.ttf\"; myBigFont.fontParameters.size = 20; manager.load(\"georgia.ttf\", BitmapFont.class, myBigFont);   Neat! Now we’ve got two different fonts, mySmallFont and myBigFont, that we can use to display different text.   We’re not quite done yet. Now that the fonts have been .loaded, we still need to set them. We can do this like so:   BitmapFont mySmallFont = manager.get(\"arial.ttf\", BitmapFont.class); BitmapFont myBigFont = manager.get(\"georgia.ttf\", BitmapFont.class);   The name you give the manager doesn’t have to match the name of the font, like in the above example. If you want to use the same font for different sizes, just make sure the name you give the asset manager when loading the font is unique. For example, here’s how you could load the Arial font in both 10pt and 20pt:   FreeTypeFontLoaderParameter arial10 = new FreeTypeFontLoaderParameter();  // This is the file that needs to exist in the assets directory. arial10.fontFileName = \"arial.ttf\"; arial10.fontParameters.size = 10;  // There is no file named arial10.ttf. This is just an identifier for the asset manager. // The .ttf extension is important, because it tells the asset manager which loader to use. manager.load(\"arial10.ttf\", BitmapFont.class, arial10);  // Now just change the font size, but use the same font. FreeTypeFontLoaderParameter arial20 = new FreeTypeFontLoaderParameter(); arial20.fontFileName = \"arial.ttf\"; arial20.fontParameters.size = 20;  // And create a new BitmapFont in 20pt. manager.load(\"arial20.ttf\", BitmapFont.class, arial20);   Getting Assets  That’s again easy:   Texture tex = manager.get(\"data/mytexture.png\", Texture.class); BitmapFont font = manager.get(\"data/myfont.fnt\", BitmapFont.class);   This of course assumes that those assets have been successfully loaded. If we want to poll whether a specific asset has been loaded we can do the following:   if(manager.isLoaded(\"data/mytexture.png\")) {    // texture is available, let's fetch it and do something interesting    Texture tex = manager.get(\"data/mytexture.png\", Texture.class); }   Disposing Assets  Easy again, and here you can see the real power of the AssetManager:   manager.unload(\"data/myfont.fnt\");   If that font references a Texture that you loaded manually before, the texture won’t get destroyed! It will be reference counted, getting one reference from the bitmap font and another from itself. As long as this count is not zero, the texture won’t be disposed.      Assets managed via the AssetManager shouldn’t be disposed manually, instead, call AssetManager.unload()!   If you want to get rid of all assets at once you can call:   manager.clear();   or   manager.dispose();   Both will dispose all currently loaded assets and remove any queued and not yet loaded assets. The AssetManager.dispose() method will also kill the AssetManager itself. After a call to this method you should not use the manager anymore.   And that’s pretty much everything there is. Now for the nitty-gritty parts.   I only supply Strings, where does the AssetManager load the assets from?  Every loader has a reference to a FileHandleResolver. That’s a simple interface looking like this:   public interface FileHandleResolver {    public FileHandle resolve(String file); }   By default, every loader uses an InternalFileHandleResolver. That will return a FileHandle pointing at an internal file (just like Gdx.files.internal(“data/mytexture.png”). You can write your own resolvers! Look into the assets/loaders/resolvers package for more FileHandleResolver implementation. One use case for this would be a caching system, where you check if you have a newer version downloaded to the external storage first, and fall back to the internal storage if it’s not available. The possibilities are endless.   You can set the FileHandleResolver to be used via the second constructor of AssetManager:   AssetManager manager = new AssetManager(new ExternalFileHandleResolver());   This will make sure all default loaders listed above will use that loader.   Writing your own Loaders  I can’t anticipate which other types of resources you want to load, so at some point you might want to write your own loaders. There are two interfaces called SynchronousAssetLoader and AsynchronousAssetLoader you can implement. Use the former if your asset type is fast to load, use the latter if you want your loading screen to be responsive. I suggest basing your loader on the code of one of the loaders listed above. Look into MusicLoader for a simple SynchronousAssetLoader, look into PixmapLoader for a simple AsynchronousAssetLoader. BitmapFontLoader is a good example of an asynchronous loader that also has dependencies that need to be loaded before the actual asset can be loaded (in that case it’s the texture storing the glyphs). Again, you can do pretty much anything with this.   Additionally, the loadAsync function can be used to load parts of the assets where the loading can be delegated to another thread (this is a requirement for responsive loading screen). The loadSync function must be used if some parts of the asset needs to be loaded on the main rendering thread. For example, OpenGL API function calls must be invoked on the main rendering thread, therefore any parts of the asset and its loading involving calls to OpenGL must be called in loadSync.   loadAsync will be called first and loadSync afterwards. You can pass information from loadASync to loadSync with the aid of your custom asset loader class’ attributes. Care must be taken to initialize these temporary variables to null between the loading of each separate asset or you might end up loading the same asset multiple times! Easiest way to achieve this is by setting your temporary variables to null at the beginning of loadASync implementation. PixmapLoader class demonstrates a simple way of using the temporary variables correctly, whereas TextureLoader shows a more complex way to pass information between loadAsync and loadSync.   Once you are done writing your loader, tell the AssetManager about it:   manager.setLoader(MyAssetClass.class, new MyAssetLoader(new InternalFileHandleResolver())); manager.load(\"data/myasset.mas\", MyAssetClass.class);   Resuming with a Loading Screen  On Android, your app can be paused and resumed. Managed OpenGL resources like Textures need to be reloaded in that case, which can take a bit of time. If you want to display a loading screen on resume, you can do the following after you created your AssetManager.   Texture.setAssetManager(manager);   In your ApplicationListener.resume() method you can then switch to your loading screen and call AssetManager.update() again until everything is back to normal.   If you don’t set the AssetManager as shown in the last snippet, the usual managed texture mechanism will kick in, so you don’t have to worry about anything.   And this concludes the long awaited article on the AssetManager.  ",
        
        "url": "/wiki/managing-your-assets" },{
        "title": "Masking",
        "excerpt":
        
        "Masking is the technique of hiding portions of an image using the pixel information of another to decide whether a pixel of the original should or should not be shown. There’s more than one way to achieve this effect in libGDX.   Table of Contents     Masking using glScissor   Masking using the ScissorStack   Masking using the Depth Buffer   Masking using Blending Function   Masking using Pixmaps   Masking using Shaders   Masking using the BlendFuncSeparate   Masking using Blending Function (Tinting)   1. Masking using glScissor (Rectangle)   For the simplest of masking needs here’s a technique that allows us to create and apply a single rectangular mask using OpenGL’s Scissor Test. The Scissor Test is a Per-Sample Processing operation that discards Fragments that fall outside of a certain rectangular portion of the screen.   Step 1 - Preparations   private ShapeRenderer shapeRenderer;  @Override public void create() {     /* We can use a SpriteBatch or a ShapeRenderer to draw our masked elements. */     shapeRenderer = new ShapeRenderer();     shapeRenderer.setAutoShapeType(true);      /* Increase the OpenGL line thickness for better visualization. */     Gdx.gl.glLineWidth(2); }   Step 2 - Drawing our masked elements   private void drawMasked() {     /* To activate the scissor test, first enable the GL_SCISSOR_TEST enumerator.      * Once enabled, pixels outside of the scissor box will be discarded. */     Gdx.gl.glEnable(GL20.GL_SCISSOR_TEST);      /* To define the scissor box, use this function: */     Gdx.gl.glScissor(100, 100, 200, 200);     /* The x and y is the window-space lower-left position of the scissor box,      * and width and height define the size of the rectangle. */      /* Draw our circle to be masked, we could also draw sprites with a SpriteBatch. */     shapeRenderer.set(ShapeType.Filled);     shapeRenderer.setColor(Color.RED);     shapeRenderer.circle(100, 100, 100);      /* Remember to flush before changing GL states again. */     shapeRenderer.flush();      /* Deactivate the scissor test before continuing with further rendering operations. */     Gdx.gl.glDisable(GL20.GL_SCISSOR_TEST); }   Step 3 - Drawing the contours for debugging purposes   private void drawContours() {     shapeRenderer.set(ShapeType.Line);      /* Draw the circle's contour for comparison. */     shapeRenderer.setColor(Color.GREEN);     shapeRenderer.circle(100, 100, 100);      /* Draw the clipped area contour for comparison. */     shapeRenderer.setColor(Color.CYAN);     shapeRenderer.rect(100, 100, 200, 200); }   Result   @Override public void render() {     shapeRenderer.begin();      drawMasked();     drawContours();      shapeRenderer.end(); }      2. Masking using the ScissorStack (Rectangles)   A single rectangle could easily not be enough, here’s a technique that allows us to create and apply multiple rectangular masks using libGDX’s ScissorStack.   Step 1 - Preparations   /* Some attributes we're gonna need. */ private ShapeRenderer shapeRenderer; private Rectangle scissors1, scissors2;  @Override public void create() {     /* The ScissorStack needs a camera to transform the clipping rectangles. */     OrthographicCamera camera = new OrthographicCamera();     camera.setToOrtho(false, CAMERA_WIDTH, CAMERA_HEIGHT);      /* We can use a SpriteBatch or a ShapeRenderer to draw our masked elements. */     shapeRenderer = new ShapeRenderer();     shapeRenderer.setAutoShapeType(true);     shapeRenderer.setProjectionMatrix(camera.combined);      /* Increase the OpenGL line thickness for better visualization. */     Gdx.gl.glLineWidth(2);      /* scissors1 and scissors2 store the results of calculateScissors(...).      * clipBounds is used to define the x, y, width and height of the clipping rectangles. */     scissors1 = new Rectangle();     Rectangle clipBounds = new Rectangle(100, 100, 200, 200);     ScissorStack.calculateScissors(camera, shapeRenderer.getTransformMatrix(), clipBounds, scissors1);      scissors2 = new Rectangle();     clipBounds.set(50f, 50f, 100f, 100f);     ScissorStack.calculateScissors(camera, shapeRenderer.getTransformMatrix(), clipBounds, scissors2); }   Step 2 - Drawing our masked elements   private void drawMasked() {     /* Feed the ScissorStack and store whether it could push the scissors or not. */     boolean pop1 = ScissorStack.pushScissors(scissors1);     boolean pop2 = ScissorStack.pushScissors(scissors2);      /* Draw the elements to be constrained to an area,      * without masking this would render a red filled circle. */     shapeRenderer.set(ShapeType.Filled);     shapeRenderer.setColor(Color.RED);     shapeRenderer.circle(100, 100, 100);     shapeRenderer.flush();      /* Safety check for the situations the scissor fails to be pushed to the stack      * (happens for example when the window is minimized on desktop or the clipping      * area is &lt;= 0). */     if (pop1) {         ScissorStack.popScissors();     }     if (pop2) {         ScissorStack.popScissors();     } }   It is also possible to push multiple rectangles. Only the pixels of the sprites or shapes that are within all of the rectangles will be rendered.   Also, if your camera moves, you’ll need to recalculate the scissor area afterwards.   Step 3 - Drawing the contours for debugging purposes   private void drawContours() {     shapeRenderer.set(ShapeType.Line);      /* The rectangular mask. */     shapeRenderer.setColor(Color.CYAN);     shapeRenderer.rect(100, 100, 200, 200);     shapeRenderer.rect(50, 50, 100, 100);      /* The masked circle. */     shapeRenderer.setColor(Color.GREEN);     shapeRenderer.circle(100, 100, 100); }   Result   @Override public void render() {     ScreenUtils.clear(Color.BLACK);      shapeRenderer.begin();      drawMasked();     drawContours();      shapeRenderer.end(); }      3. Masking using the Depth Buffer (Shapes)   Alright rectangles are great but our needs are greater what now. This upcoming technique allows us to create more diversely shaped masks using libGDX’s ShapeRenderer. You can use a SpriteBatch, but because the masks are built from the geometry of what you’re drawing it will not work as you expect. Texture regions will render as rectangles no matter what the image looks like.   Step 1 - Preparations   private ShapeRenderer shapeRenderer;  @Override public void create() {     /* We can use a SpriteBatch or a ShapeRenderer to draw our masked elements. */     shapeRenderer = new ShapeRenderer();     shapeRenderer.setAutoShapeType(true);      /* Increase the OpenGL line thickness for better visualization. */     Gdx.gl.glLineWidth(2); }   Step 2 - Draw the mask elements to the depth buffer   private void drawMasks() {     /* Clear our depth buffer info from previous frame. */     Gdx.gl.glClear(GL20.GL_DEPTH_BUFFER_BIT);      /* Set the depth function to LESS. */     Gdx.gl.glDepthFunc(GL20.GL_LESS);      /* Enable depth writing. */     Gdx.gl.glEnable(GL20.GL_DEPTH_TEST);      /* Disable RGBA color writing. */     Gdx.gl.glColorMask(false, false, false, false);      /* Render mask elements. */     shapeRenderer.set(ShapeType.Filled);     shapeRenderer.circle(100, 200, 100);     shapeRenderer.triangle(0, 0, 100, 100, 200, 0);     shapeRenderer.flush(); }   When using a SpriteBatch write this line right after SpriteBatch.begin() : Gdx.gl.glDepthMask(true);   Step 3 - Draw the masked elements   private void drawMasked() {     /* Enable RGBA color writing. */     Gdx.gl.glColorMask(true, true, true, true);      /* Set the depth function to LESS. */     Gdx.gl.glDepthFunc(GL20.GL_EQUAL);      /* Render masked elements. */     shapeRenderer.setColor(Color.RED);     shapeRenderer.circle(100, 100, 100);     shapeRenderer.flush(); }   Step 4 - Draw the contours for debugging purposes   private void drawContours() {     /* Disable depth writing. */     Gdx.gl.glDisable(GL20.GL_DEPTH_TEST);      shapeRenderer.set(ShapeType.Line);      /* The circle and triangle masks. */     shapeRenderer.setColor(Color.CYAN);     shapeRenderer.circle(100, 200, 100);     shapeRenderer.triangle(0, 0, 100, 100, 200, 0);      /* The masked circle. */     shapeRenderer.setColor(Color.GREEN);     shapeRenderer.circle(100, 100, 100); }   Result   @Override public void render() {     ScreenUtils.clear(Color.BLACK);      shapeRenderer.begin();      drawMasks();     drawMasked();     drawContours();      shapeRenderer.end(); }      4. Masking using Blending Function (Shapes or Textures)   For the demanding GDXer with complex masking needs, this technique allows us to have any mask imaginable and take the alpha channel into account for the first time! For this we’ll be using libGDX’s SpriteBatch.   Step 1 - Preparations   These are the images we’re gonna use: |  |  | | :-: | :-: | | The mask | The sprite to mask |   The images in a black background for clarity: |  |  | | :-: | :-: | | The mask | The sprite to mask |   /* Some attributes we're gonna need. */ private SpriteBatch spriteBatch; private Sprite mask, maskedSprite;  @Override public void create() {     spriteBatch = new SpriteBatch();      /* Load the mask containing the alpha information. */     mask = new Sprite(new Texture(\"mask.png\"));      /* Load the sprite which will be masked. */     maskedSprite = new Sprite(new Texture(\"sprite.png\"));     maskedSprite.setColor(Color.RED); }   Step 2 - Draw the mask elements to the frame buffer   private void drawMasks() {     /* Disable RGB color writing, enable alpha writing to the frame buffer. */     Gdx.gl.glColorMask(false, false, false, true);      /* Change the blending function for our alpha map. */     spriteBatch.setBlendFunction(GL20.GL_ONE, GL20.GL_ZERO);      /* Draw alpha masks. */     mask.draw(spriteBatch);      /* This blending function makes it so we subtract instead of adding to the alpha map. */     spriteBatch.setBlendFunction(GL20.GL_ZERO, GL20.GL_SRC_ALPHA);      /* Remove the masked sprite's inverse alpha from the map. */     maskedSprite.draw(spriteBatch);      /* Flush the batch to the GPU. */     spriteBatch.flush(); }   Step 3 - Draw the masked elements   private void drawMasked() {     /* Now that the buffer has our alpha, we simply draw the sprite with the mask applied. */     Gdx.gl.glColorMask(true, true, true, true);      /* Change the blending function so the rendered pixels alpha blend with our alpha map. */     spriteBatch.setBlendFunction(GL20.GL_DST_ALPHA, GL20.GL_ONE_MINUS_DST_ALPHA);      /* Draw our sprite to be masked. */     maskedSprite.draw(spriteBatch);      /* Remember to flush before changing GL states again. */     spriteBatch.flush(); }   Step 4 - Draw the original sprites for debugging purposes   private void drawOriginals() {     /* Switch to the default blend function */     spriteBatch.setBlendFunction(GL20.GL_SRC_ALPHA, GL20.GL_ONE_MINUS_SRC_ALPHA);      /* Draw the source images separately */     spriteBatch.draw(mask, 0, 256);     spriteBatch.draw(maskedSprite, 256, 256); }   Result   @Override public void render() {     ScreenUtils.clear(Color.BLACK);      spriteBatch.begin();      drawMasks();     drawMasked();     drawOriginals();      spriteBatch.end(); }      5. Masking using Pixmaps (Shapes or Textures)   This technique allows the mask to be any image or shape and takes the alpha channel into account. This time we’ll be using the libGDX’s Pixmap class.   Step 1 - Preparations   private ShapeRenderer shapeRenderer; private SpriteBatch spriteBatch; private Texture masked, original; private final int size = 256;  @Override public void create() {     /* The ShapeRenderer will only be used to draw the mask contours */     shapeRenderer = new ShapeRenderer();     shapeRenderer.setAutoShapeType(true);     Gdx.gl20.glLineWidth(2);      /* Needed to render our textures, a ShapeRenderer won't work with this technique. */     spriteBatch = new SpriteBatch();      /* The path to the image to mask. */     FileHandle imagePath = new FileHandle(\"images/shared/weirdShape.png\");      /* Load the pixels of our image into a Pixmap. */     Pixmap pixmap = new Pixmap(imagePath);      /* Have an unaltered version for comparison. */     original = new Texture(imagePath);      /* Apply the mask to our Pixmap. */     pixmap = applyMask(pixmap);      /* Load the pixel information of the Pixmap into a Texture for drawing. */     masked = new Texture(pixmap); }   Step 2 - Applying the mask   private Pixmap applyMask(Pixmap source) {     /* Create a Pixmap to store the mask information, at the end it will      * contain the result. */     Pixmap result = new Pixmap(source.getWidth(), source.getHeight(), Pixmap.Format.RGBA8888);      /* This setting lets us overwrite the pixels' transparency. */     result.setBlending(None);      /* Ignore RGB values unless you want funky results, alpha is for the mask. */     result.setColor(new Color(1f, 1f, 1f, 1f));      /* Draw a circle to our mask, any shape is possible since      * you can draw individual pixels to the Pixmap. */     result.fillCircle(size / 2, size / 2, size / 2);      /* Draw a rectangle with little alpha to our mask, this will turn     * a corner of the original image transparent. */     result.setColor(1f, 1f, 1f, 0.25f);     result.fillRectangle(size / 2, size / 2, size / 2, size / 2);      /* We can also define the mask by loading an image:      * result = new Pixmap(new FileHandle(\"image.png\")); */      /* Decide the color of each pixel using the AND bitwise operator. */     for (int x = 0; x &lt; result.getWidth(); x++) {         for (int y = 0; y &lt; result.getHeight(); y++) {             result.drawPixel(x, y, source.getPixel(x, y) &amp; result.getPixel(x, y));         }     }      return result; }   Step 3 - Drawing the original and masked images   private void drawImages() {     /* Draw the original image for comparison. */     spriteBatch.setColor(Color.WHITE);     spriteBatch.draw(original, 0, size, size, size);      /* Draw the masked image in red. */     spriteBatch.setColor(Color.RED);     spriteBatch.draw(masked, 0, 0, size, size); }   Step 4 - Drawing the contours of the mask for debugging purposes   private void drawContours() {     /* Draw the contour of the circle and rectangle used as masks. */     shapeRenderer.setColor(Color.CYAN);     shapeRenderer.circle(size / 2f, size / 2f, size / 2f);     shapeRenderer.rect(size / 2f, 0, size / 2f, size / 2f); }   Result   @Override public void render() {     ScreenUtils.clear(Color.BLACK);      spriteBatch.begin();     drawImages();     spriteBatch.end();      shapeRenderer.begin();     drawContours();     shapeRenderer.end(); }      6. Masking using Shaders (Textures)   This technique allows the mask to be any image or shape and takes alpha channel into account. This time we’ll be using the libGDX’s ShaderProgram class in conjunction with the Texture class.   Step 1 - Preparations   private final int size = 300; private Texture texture; private SpriteBatch spriteBatch1, spriteBatch2; private ShapeRenderer shapeRenderer;  @Override public void create() {     /* We'll be using a pixmap to define the mask this time. */     defineMask();      /* Some regular textures to draw on the screen. */     texture = new Texture(WEIRD_SHAPE_PATH);     texture.setFilter(Linear, Linear);      setupShader();      /* An unmodified SpriteBatch to draw the original image as reference      * we could also change the shader of spriteBatch1 back to the default. */     spriteBatch2 = new SpriteBatch();      /* Construct a simple ShapeRenderer to draw reference contours. */     shapeRenderer = new ShapeRenderer();     shapeRenderer.setAutoShapeType(true);     Gdx.gl.glLineWidth(2); }   Step 2 - Defining our mask   private void defineMask() {     /* The fragment shader simply multiplies the fragment's usual alpha with      * our mask alpha, since we only care about the alpha channel, the Alpha      * Pixmap format is just what we need. */     Pixmap pixmap = new Pixmap(size, size, Alpha);      /* Pixmap blending can result in some funky looking lines when      * drawing. You may need to disable it. */     pixmap.setBlending(None);      /* The default color of a newly created Pixmap has an alpha value of 0      * play with different alpha values for different levels of transparency. */     pixmap.setColor(0, 0, 0, 1);      /* This setting will let us see some portions of the masked image. */     pixmap.fillCircle(size / 2, size / 4, size / 4);     pixmap.setColor(0, 0, 0, 0.25f);     pixmap.fillRectangle(size / 4, size / 2, size / 2, size / 2);      /* Create a Texture based on the pixmap.      * IMPORTANT: How we create the texture doesn't matter, this technique      * also allows, for example, to create it out of any supported format image */     Texture pixmapTex = new Texture(pixmap);      /* Bind the mask texture to TEXTURE&lt;N&gt; (TEXTURE1 for our purposes),      * which also sets the currently active texture unit. */     pixmapTex.bind(1);      /* However SpriteBatch will auto-bind to the current active texture,      * so we must now reset it to TEXTURE0 or else our mask will be      * overwritten. */     Gdx.gl.glActiveTexture(GL20.GL_TEXTURE0); }   Step 3 - Setting up the shader   private void setupShader() {     /* It's nicer to keep shader programs as text files in the assets      * directory rather than dealing with horrid Java string formatting. */     FileHandle vertexShader = Gdx.files.internal(\"shaders/shared/vertex.glsl\");     FileHandle fragmentShader = Gdx.files.internal(\"shaders/masking/fragment.glsl\");      /* Bonus: you can set `pedantic = false` while tinkering with your      * shaders. This will stop it from crashing if you have unused variables      * and so on. */     ShaderProgram.pedantic = false;      /* Construct our shader program. Spit out a log and quit if the shaders      * fail to compile. */     ShaderProgram shader = new ShaderProgram(vertexShader, fragmentShader);     if (!shader.isCompiled()) {         Gdx.app.log(\"Shader\", shader.getLog());         Gdx.app.exit();     }      /* Tell our shader that u_texture will be in the TEXTURE0 spot and      * u_mask will be in the TEXTURE1 spot. We can set these now since      * they'll never change; we don't have to send them every render frame. */     shader.bind();     shader.setUniformi(\"u_texture\", 0);     shader.setUniformi(\"u_mask\", 1);      /* Construct a simple SpriteBatch using our shader program. */     spriteBatch1 = new SpriteBatch();     spriteBatch1.setShader(shader); }   The vertex.glsl shader file:   uniform mat4 u_projTrans;  attribute vec4 a_position; attribute vec4 a_color; attribute vec2 a_texCoord0;  varying vec4 v_color; varying vec2 v_texCoord0;  void main() {     v_color = a_color;     v_texCoord0 = a_texCoord0;     gl_Position = u_projTrans * a_position; }   The fragment.glsl shader file:   #ifdef GL_ES precision mediump float; #endif  uniform sampler2D u_texture; uniform sampler2D u_mask;  varying vec4 v_color; varying vec2 v_texCoord0;  void main() {     vec4 texColor = texture2D(u_texture, v_texCoord0);     vec4 mask = texture2D(u_mask, v_texCoord0);     texColor.a *= mask.a;     gl_FragColor = v_color * texColor; }   Step 4 - Drawing the contours of the mask for debugging purposes   private void drawContours() {     /* Draw the contour of the masks. */     shapeRenderer.setColor(Color.CYAN);     shapeRenderer.rect(size / 4f, 0f, size / 2f, size / 2f);     shapeRenderer.circle(size / 2f, size * 0.75f, size / 4f); }   Result   @Override public void render() {     ScreenUtils.clear(Color.BLACK);      /* Draw our masked image. */     spriteBatch1.begin();     spriteBatch1.setColor(Color.RED);     spriteBatch1.draw(texture, 0, 0, size, size);     spriteBatch1.end();      /* Draw the original image unmasked for comparison. */     spriteBatch2.begin();     spriteBatch2.draw(texture, 0, size, size, size);     spriteBatch2.end();      shapeRenderer.begin();     drawContours();     shapeRenderer.end(); }      7. Masking using BlendFuncSeparate (Removal)   Ideal if you wanna use the mask to hide portions of the masked elements.   Step 1 - Preparations   private ShapeRenderer shapeRenderer; private FrameBuffer frameBuffer; private SpriteBatch spriteBatch;  @Override public void create() {     shapeRenderer = new ShapeRenderer();     shapeRenderer.setAutoShapeType(true);     Gdx.gl20.glLineWidth(2);      frameBuffer = new FrameBuffer(Pixmap.Format.RGBA8888, Gdx.graphics.getWidth(), Gdx.graphics.getHeight(), false);      spriteBatch = new SpriteBatch(); }   Step 2 - Drawing the masked elements and the mask elements   private void drawCircles() {     shapeRenderer.set(ShapeType.Filled);      /* An example circle, remember to flush before changing the blending function */     shapeRenderer.setColor(Color.RED);     shapeRenderer.circle(200, 200, 100);     shapeRenderer.flush();      /* We'll need blending enabled for the technique to work*/     Gdx.gl.glEnable(GL20.GL_BLEND);      /* With this blending function, wherever we draw pixels next      * we will actually remove previously drawn pixels. */     Gdx.gl.glBlendFuncSeparate(GL20.GL_ZERO, GL20.GL_ZERO, GL20.GL_ONE_MINUS_SRC_ALPHA, GL20.GL_ONE_MINUS_DST_ALPHA);     shapeRenderer.circle(300, 200, 70);     shapeRenderer.circle(100, 200, 35);     shapeRenderer.flush();      /* Restore defaults. */     Gdx.gl.glDisable(GL20.GL_BLEND);      /* The default blend function in case we need standard blending elsewhere.      * Gdx.gl.glBlendFunc(GL20.GL_SRC_ALPHA, GL20.GL_ONE_MINUS_SRC_ALPHA); */ }   Step 3 - Drawing the contours for debugging purposes   private void drawContours() {     shapeRenderer.set(ShapeType.Line);      /* Contour of the masked circle */     shapeRenderer.setColor(Color.GREEN);     shapeRenderer.circle(200, 200, 100);      /* Contour of the masks */     shapeRenderer.setColor(Color.CYAN);     shapeRenderer.circle(300, 200, 70);     shapeRenderer.circle(100, 200, 35); }   Result   @Override public void render() {     ScreenUtils.clear(Color.GRAY);      frameBuffer.bind();     shapeRenderer.begin();     drawCircles();     drawContours();     shapeRenderer.end();     frameBuffer.end();      Texture texture = frameBuffer.getColorBufferTexture();     Sprite sprite = new Sprite(texture);     sprite.flip(false, true);      spriteBatch.begin();     sprite.draw(spriteBatch);     spriteBatch.end(); }      8. Masking using Blending Function (Tinting)   Ideal if you wanna use the mask to tint or texture portions of the masked elements.   Step 1 - Preparations   private ShapeRenderer shapeRenderer; private SpriteBatch spriteBatch; private BitmapFont menuItemFont; private FrameBuffer frameBuffer; private float textWidth, textHeight, textX, textY;  @Override public void create() {     shapeRenderer = new ShapeRenderer();     shapeRenderer.setAutoShapeType(true);      spriteBatch = new SpriteBatch();      menuItemFont = new BitmapFont();     menuItemFont.getData().setScale(6f);      int screenWidth = Gdx.graphics.getWidth();     int screenHeight = Gdx.graphics.getHeight();      GlyphLayout glyphLayout = new GlyphLayout();     glyphLayout.setText(menuItemFont, \"ONE PLAYER\");      textWidth = glyphLayout.width;     textHeight = glyphLayout.height;     textX = screenWidth / 2f - textWidth / 2f;     textY = screenHeight / 2f + textHeight / 2f;      frameBuffer = new FrameBuffer(Pixmap.Format.RGBA8888, screenWidth, screenHeight, false); }   Step 2 - Drawing the mask and masked elements   private void draw() {     spriteBatch.begin();     menuItemFont.draw(spriteBatch, \"ONE PLAYER\", textX, textY);     spriteBatch.end();      Gdx.gl.glEnable(GL20.GL_BLEND);      Gdx.gl.glBlendFunc(GL20.GL_DST_COLOR, GL20.GL_ZERO);      shapeRenderer.begin();     shapeRenderer.set(ShapeType.Filled);     shapeRenderer.rect(textX - 10, textY + 10, textWidth + 20, -(textHeight + 20),             Color.LIME, Color.LIME, Color.BLACK, Color.BLACK);     shapeRenderer.end();      Gdx.gl.glDisable(GL20.GL_BLEND); }   Result   @Override public void render() {     ScreenUtils.clear(Color.RED);      frameBuffer.bind();     draw();     frameBuffer.end();      Texture texture = frameBuffer.getColorBufferTexture();     Sprite sprite = new Sprite(texture);     sprite.flip(false, true);      spriteBatch.begin();     sprite.draw(spriteBatch);     spriteBatch.end(); }     ",
        
        "url": "/wiki/graphics/2d/masking" },{
        "title": "Material and environment",
        "excerpt":
        
        "In practice, when rendering, you are specifying what (the shape) to render and how (the material) to render. The shape is specified using the Mesh (or more commonly the MeshPart), which defines the vertices attributes for the shader. The material is most commonly used to specify the uniform values for the shader.   Uniforms can be grouped into model specific (e.g. the texture applied or whether or not to use blending) and environmental uniforms (e.g. the lights being applied or an environment cubemap). Likewise the 3D api allows you to specify a material and environment.   Materials   Materials are model (or modelinstance) specific. You can access them by index model.materials.get(0), by name model.getMaterial(\"material3\") or by nodepart model.nodes.get(0).parts(0).material. Materials are copied when creating a ModelInstance, meaning that changing the material of a ModelInstance will not affect the original Model or other ModelInstances.   The Material class extends the Attributes class, see below for more information about Attributes.   Environment   An Environment contains the uniform values specific for a location. For example, the lights are part of the Environment. Simple applications might use only one Environment, while more complex applications might use multiple environments depending on the location of a ModelInstance. A ModelInstance (or Renderable) can only contain one Environment though.   The Environment class extends the Attributes class, see below for more information about Attributes.   Lights   Lights use attributes as well, which means that you can attach a light to either an environment or a material. Adding a light to an environment can be done using the environment.add(light) method. However, you can also use the DirectionalLightsAttribute, PointLightsAttribute and SpotLightsAttribute attributes (see below). Each of these attributes has an array which you can use to attach one or more lights to it. Note however that you typically can only use one of both. If you add a light to the PointLightsAttribute of the environment and then add another light to the PointLightsAttribute of the material, then the DefaultShader will ignore the point light(s) added to the environment. Lights are always used by reference.   Lights should be sorted by importance. Usually this means that lights should be sorted on distance. The DefaultShader for example by default (configurable) only uses the first five point lights for shader lighting. Any remaining lights will be ignored.   Attributes   Both the Environment and Material classes extend the Attributes the class. Most commonly, the Attributes class is used to specify uniform values. For example a TextureAttribute can be used to specify an uniform to bound for a shader. However, attributes don’t have to be uniforms, for example DepthTestAttribute is used to alter the opengl state and doesn’t set an uniform.   The Attributes class is most comparable with a Set. It can contain at most one value for each attribute, just like an uniform can only be set to one value. Theoretically both the Material and Environment can contain the same attribute, the actual behavior in this scenario depends on the shader used, but in most cases the Materials attribute will be used instead of the Environment attribute.   Attribute type   Every attribute has a type long value, which is a bitmask used to identify the attribute. Therefor a complete material or environment can be represented with a single long, where each bit represents an attribute. Some attribute classes are dedicated to a single type value (bit). Others can be used for multiple type values (bits), in which case you must specify the type on construction. For example:   Attribute attribute = new ColorAttribute(ColorAttribute.Diffuse, Color.RED);   note that the ColorAttribute class contains a convenience method to do the same:   Attribute attribute = ColorAttribute.createDiffuse(Color.RED);   Using attributes   The most common actions for attributes are set, has, remove, and get.   You can use the set method to add or change an attribute. If there is already an attribute of the same type if will be first removed, after which the new attribute is added. For example: material.set(FloatAttribute.createAlphaTest(0.25f));   Using the has method it is possible to check if a specific attribute type is set. For example: material.has(FloatAttribute.AlphaTest);. It is also possible to check for multiple attributes, for example: material.has(FloatAttribute.AlphaTest | ColorAttribute.Diffuse);. In which case the has method will only return true if all attributes are set. Note that because the has method uses a bitwise check it is quite fast and can be used prior to e.g. a call to remove to ensure the attribute is actually set.   With the remove method you can remove an attribute of a specific type, for example: material.remove(FloatAttribute.AlphaTest);. You can also remove multiple attributes at once, for example: material.remove(FloatAttribute.AlphaTest | ColorAttribute.Diffuse);.   The get method can be used to fetch an attribute of a specific type. For example: material.get(FloatAttribute.AlphaTest);. If the attribute isn’t set the get method will return null. Because the type implies the class, you can safely cast the result without checking: (FloatAttribute)material.get(FloatAttribute.AlphaTest);. For convenience there’s also a template method: material.get(FloatAttribute.class, FloatAttribute.AlphaTest);.   Besides that you can also clear (to remove all attributes), iterate (it implements Iterable&lt;Attribute&gt;) and compare (it implements Comparator&lt;Attribute&gt;, however the same method provides additional options) attributes. The getMask() method provides access to the mask containing all attributes, for example material.getMask() &amp; FloatAttribute.AlphaTest == FloatAttribute.AlphaTest is the same as material.has(FloatAttribute.AlphaTest).   Custom attribute types   It is possible to use a standard attribute class for a new custom type. For example when you want to use the ColorAttribute class to specify a custom type of color. There are three things you must consider in that case:      Name your attribute type. Each attribute type must have an unique alias (name). You might want to (but don’t have to) use the uniform name for that. The alias will also be used for debugging, e.g. when calling attribute.toString().   Register your attribute type. This is to make sure there is only one attribute type for each bit. This can be done only from within the Attribute class or a subclass.   Make ColorAttribute accept the custom type. The ColorAttribute class and most other attribute classes checks the type on construction, this allows you to cast attributes without having to check anything other then the type. For example (ColorAttribute)material.get(ColorAttribute.Diffuse) will always work because ColorAttribute is the only attribute accepting the ColorAttribute.Diffuse type.   Because of step 2 and 3, you must extend the Attribute class to add a custom attribute type:   public class CustomColorTypes extends ColorAttribute {     public final static String AlbedoColorAlias = \"AlbedoColor\"; // step 1: name the type     public final static long AlbedoColor = register(AlbedoColorAlias); // step 2: register the type     static {         Mask |= AlbedoColor; // step 3: Make ColorAttribute accept the type     }     /** Prevent instantiating this class */     private CustomColorTypes() {         super(0);     } }   You can then create the custom attribute type using:   Attribute attribute = new ColorAttribute(CustomColorTypes.AlbedoColor, Color.RED);   Custom attributes  It is possible to create a custom attribute, in which case the process is not much different from above. You must extend the Attribute class, register at least one type and of course add some data to pass on to the shader. For example to add an attribute to pass a double value to the shader:   public class DoubleAttribute extends Attribute {     public final static String MyDouble1Alias = \"myDouble1\";     public final static long MyDouble1 = register(MyDouble1Alias);     public final static String MyDouble2Alias = \"myDouble2\";     public final static long MyDouble2 = register(MyDouble2Alias);     protected static long Mask = MyDouble1 | MyDouble2;     /** Method to check whether the specified type is a valid DoubleAttribute type */     public static Boolean is(final long type) {         return (type &amp; Mask) != 0;     }      public double value;      public DoubleAttribute (final long type) {         super(type);         if (!is(type))             throw new GdxRuntimeException(\"Invalid type specified\");     }      public DoubleAttribute (final long type, final double value) {         this(type);         this.value = value;     }      /** copy constructor */     public DoubleAttribute (DoubleAttribute other) {         this(other.type, other.value);     }      @Override     public Attribute copy () {         return new DoubleAttribute(this);     }      @Override     public int hashCode () {         final int prime = /* pick a prime number and use it here */;         final long v = NumberUtils.doubleToLongBits(value);         return prime * super.hashCode() + (int)(v^(v&gt;&gt;&gt;32));     }      @Override     public int compareTo (Attribute o) {         if (type != o.type) return type &lt; o.type ? -1 : 1;         double otherValue = ((DoubleAttribute)o).value;         return value == otherValue ? 0 : (value &lt; otherValue ? -1 : 1);     } }  Of course MyDouble1Alias, MyDouble1, \"myDouble1\", MyDouble2Alias, MyDouble2 and \"myDouble2\" should be replaced by a more meaningful description.   Note that the copy() method is for example called when creating a ModelInstance of a Model. It should return an identical instance of the attribute which can be modified independently of the attribute being copied. While not required, a copy constructor is typically a good method to implement this.   The hashCode() method should be implemented because it is used for comparing attributes and materials. For example, two materials are considered to be the same if they contain the same attributes (types) and the equals() method returns true for each pair of attribute types. By default, the equals() method of the Attribute class compares the hashCode() of both attributes for this.   The compareTo(Attribute) method must be implemented for sorting render calls based on the material. This implementation should typically always start with the line: if (type != o.type) return type &lt; o.type ? -1 : 1; to assure that it’s comparing attributes of the same type.      Attribute classes should be kept small and self contained, therefore it is best to always directly extend the Attribute class. Try to avoid extending a subclass of the Attribute class to add additional information.    Available attributes  Like stated above its possible to create custom attributes. However, there are a few attributes already included, which are listed below.   BlendingAttribute   By default the 3D api assumes everything is opaque. The BlendingAttribute is most commonly used for materials (in case of environment it will change the default behavior) and can be used to specify that the material is or is not blended. The BlendingAttribute doesn’t require you to specify a type on construction, its type is always BlendingAttribute.Type, unless it’s extended. It contains four properties which can be specified:     blended indicates whether or not the material should be treated as blended. This is primarily used for sorting, for example opaque objects are drawn prior to transparent objects.   sourceFunction OpenGL enum which specifies how the (incoming) red, green, blue, and alpha source blending factors are computed, by default it is set to GL_SRC_ALPHA.   destFunction OpenGL enum which specifies how the (existing) red, green, blue, and alpha destination blending factors are computed, by default it is set to GL_ONE_MINUS_SRC_ALPHA. For additive blending you might want to set it to GL_ONE.   opacity The amount of opacity (the source alpha value), ranging from 0 (fully transparent) to 1 (fully opaque).   ColorAttribute   The ColorAttribute allows you to pass a color to the shader. For that it only contains one property: .color. You can set the color during construction (it will be set by value) or using the .color.set(...) method. The ColorAttribute requires an attribute type to be specified, by default the following types are available:     ColorAttribute.Diffuse   ColorAttribute.Specular   ColorAttribute.Ambient   ColorAttribute.Emissive   ColorAttribute.Reflection   ColorAttribute.AmbientLight   ColorAttribute.Fog   Where the latter two are most commonly used for Environment, while the others or commonly used for Material.   CubemapAttribute  To pass a Cubemap to the shader the CubemapAttribute can be used. It’s value is the textureDescription member which can be used to specify the cubemap along with other texture related values. The CubemapAttribute requires an attribute type to be specified, by default the CubemapAttribute.EnvironmentMap is the only valid type.   DepthTestAttribute  Just like the BlendingAttribute, does the DepthTestAttribute not require an attribute type. It is always DepthTestAttribute.Type. The DepthTestAttribute can be used to specify depth testing and writing, using the following properties:     depthFunc The depth test function, or 0 (or GL_NONE) to disable depth test, by default it is GL20.GL_LEQUAL.   depthRangeNear Mapping of near clipping plane to window coordinates, by default 0.0   depthRangeFar Mapping of far clipping plane to window coordinates, by default 1.0   depthMask Whether or not to write to the depth buffer, enabled by default.   DirectionalLightsAttribute  The DirectionalLightsAttribute does not require an attribute type. It is always DirectionalLightsAttribute.Type. The ` DirectionalLightsAttribute can be used to specify an array of [DirectionalLight`](https://javadoc.io/doc/com.badlogicgames.gdx/gdx/latest/com/badlogic/gdx/graphics/g3d/environment/DirectionalLight.html) instances, using the following property:     lights The array of lights, should be sorted on importance.   FloatAttribute  To pass a single floating point value to the shader, the FloatAttribute can be used. The value can be specified on construction or using the .value member. The FloatAttribute requires an attribute type, which by default can be:     FloatAttribute.Shininess Used for specular lighting.   FloatAttribute.AlphaTest Used to discard pixels when the alpha value is equal or below the specified value.   IntAttribute  Similar to the FloatAttribute class, the IntAttribute allows you to pass an integer value to the shader. Likewise the .value member can be used or the value can be set on construction. The IntAttribute requires an attribute type, which by default can be:     IntAttribute.CullFace OpenGL enum to specify face culling, either GL_NONE (no culling), GL_FRONT (only render back faces) or GL_BACK (only render front faces). The default depends on the shader, the default shader uses GL_BACK by default.   PointLightsAttribute  The PointLightsAttribute does not require an attribute type. It is always PointLightsAttribute.Type. The PointLightsAttribute can be used to specify an array of PointLight instances, using the following property:     lights The array of lights, should be sorted on importance.   SpotLightsAttribute  Not supported by the default shader. The SpotLightsAttribute does not require an attribute type. It is always SpotLightsAttribute.Type. The SpotLightsAttribute can be used to specify an array of SpotLight instances, using the following property:     lights The array of lights, should be sorted on importance.   TextureAttribute  TextureAttribute can be used to pass a Texture to the shader. Just like the CubemapAttribute it has a textureDescription member which allows you to set the Texture amongst some texture related values like repeat and filter. Additionally it contains the offsetU, offsetV, scaleU and scaleY members, which can be used to specify the region (texture coordinates transformation) of the texture to use. It also has an uvIndex member (defaults to 0) which can be used to specify which texture coordinates should be used. Note that the default shader currently ignores this uvIndex member and always uses the first texture coordinates.   The TextureAttribute requires an attribute type, which by default can be one of the following:     TextureAttribute.Diffuse   TextureAttribute.Specular   TextureAttribute.Bump   TextureAttribute.Normal  ",
        
        "url": "/wiki/graphics/3d/material-and-environment" },{
        "title": "Math utilities",
        "excerpt":
        
        "Introduction   The math package contains various useful classes for dealing with problems in geometry, linear algebra, collision detection, interpolation, and common unit conversions.   Math Utils   The MathUtils  (code) class covers a number of useful odds and ends. There is a handy static Random member to avoid instantiating one in your own code. Using the same random instance throughout your code can also ensure consistently deterministic behavior as long as you store the seed value used. There are constants for conversion between radians and degrees as well as look-up tables for the sine and cosine functions. There are also float versions of common java.lang.Math functions to avoid having to cast down from double.   Catmull-Rom Spline   A Catmull-Rom spline (code) allows a continuous curve to be generated from a discrete list of control points. Splines can be useful for describing a smooth path of motion of a body or camera through space. Catmull-Rom splines are particularly useful in this regard as they are simple to compute and can guarantee the resulting path will pass through each control point (except the first and last points which control spline shape but do not become part of the path). Unlike Bezier splines, control tangents are implicit and control over spline shape is traded for ease of path definition. All that is required is a list of points through which the desired path must pass.   Ear-Clipping Triangulator   One way to triangulate a simple polygon is based on the fact that any simple polygon with at least four vertices and with no holes has at least two so called ‘ears’, which are triangles with two sides being the edges of the polygon and the third one completely inside it. The Ear-Clipping Triangulator (code) class implements this idea, producing a list of triangles from a list of supplied points representing a two-dimensional polygon. The input polygon may be concave and the resulting triangles have a clockwise order.   Windowed Mean   The Windowed Mean (code) class is useful for tracking the mean of a running stream of floating point values within a certain window. This can be useful for basic statistical analysis such as measuring average network speed, gauging user reaction times for dynamic difficulty adjustment, or energy-based beat detection in music visualizations.  ",
        
        "url": "/wiki/math-utils/math-utilities" },{
        "title": "Maven integration",
        "excerpt":
        
        "Instead of using Gradle to setup your libGDX project, you can also use Maven. However, this is not recommended! When issues arise, getting support will most certainly prove very difficult!   The article below may be outdated. Use it at your own risk!   Introduction   libGDX is currently published to the SonaType snapshot repository. This will continue to happen until the Maven integration is complete. The repository only contains the latest snapshot release for now. Eventually we will publish release builds to SonaType as well.   Add the following to your settings.xml or your project’s pom.xml:     &lt;repositories&gt;     &lt;repository&gt;       &lt;id&gt;gdx-nightlies&lt;/id&gt;       &lt;url&gt;https://oss.sonatype.org/content/repositories/snapshots/&lt;/url&gt;     &lt;/repository&gt;   &lt;/repositories&gt;   If you use the archetype you do not need to add the repository to your Maven settings.   Setting up libGDX with Maven is non trivial as it      contains native libraries   deploys to GWT   deploys to Android   To handle these issues, libGDX relies on the following Maven plugins:      GWT Maven plugin, version 2.5.0, to compile and package the GWT project.   Maven native dependencies plugin, to copy the native libraries to the appropriate place   Maven Android plugin, to compile and package the Android project.   To ease all of this, we supply a Maven archetype that will generate a multi-module Maven project.   Maven Archetype  The Maven archetype is currently not found in any repository. You can get it at https://github.com/libgdx/libgdx-maven-archetype and compile and install it yourself to your local Maven repository like this from your shell:   git clone git://github.com/libgdx/libgdx-maven-archetype.git cd libgdx-maven-archetype mvn install   To invoke the archetype, do the following:   mvn archetype:generate -DarchetypeGroupId=com.badlogic.gdx -DarchetypeArtifactId=gdx-archetype -DarchetypeVersion=1.2.0 -DgroupId=com.badlogic.test -DartifactId=test -Dversion=1.0-SNAPSHOT -Dpackage=com.badlogic.test -DJavaGameClassName=Test   The first three parameters specify the archetype, which has group id com.badlogic.gdx, artifact id gdx-archetype and a version (currently 1.0-SNAPSHOT).   The next parameters specify attributes of your project.      groupId: your project’s group id   artifactId: your project’s artifact id   version: your project’s version   package: your project’s main package   JavaGameClassName: the name of your ApplicationListener class, and the prefix for platform starter classes, e.g. MyClassDesktop, MyClassAndroid etc.   For the parameters given above, you will end up with the following project structure (we’ll use this example in subsequent sections)   test/       &lt;-- the base directory    core/    &lt;-- contains the apps core    desktop/ &lt;-- desktop starter &amp; assets    android/ &lt;-- android starter    html/    &lt;-- HTML starter    ios/     &lt;- stub, not working at the moment   The core project contains your application code. The desktop project contains the assets folder which is shared across all other projects and the desktop starter class. The Android project contains the start-up code for Android and depends on the core project. The same is true for the HTML project. The iOS project is currently a stub and does not work yet.   Building &amp; Deploying  Using Maven to build and deploy your application for the various backends is simple.   Desktop  To create a runnable jar file for the desktop, run:   mvn -Pdesktop package   This will create a file called test-desktop-1.0-SNAPSHOT-jar-with-dependencies.jar in the test/desktop/target folder. It contains all the necessary dependencies, the assets and a manifest file specifying the main class. You can run this file via:   java -jar test-desktop-1.0-SNAPSHOT-jar-with-dependencies.jar   Android  To create an unsigned APK for Android, run:   mvn -Pandroid package   This will create a file called test-android-1.0-SNAPSHOT.apk in the test/android/target folder. To install the apk to a connected device or emulator, run   mvn -Pandroid install   For more information on Android goals, see the Maven Android plugin   HTML5/GWT  To compile the HTML5 project to JavaScript, run:   mvn -Phtml package   The end result is located in the target/ folder. You can either use the .war file that was generated and deploy that to Jetty/Tomcat, or copy the contents of the HTML/target/test-html-1.0-SNAPSHOT/ folder to a location your web server can server(or even better, create a sym-link if you are on an appropriate OS). The war/folder contains all the compiled JavaScript code, the index HTML file and the assets.   To run and test the HTML5 project, run:  mvn -Phtml install   And browse to http://127.0.0.1:8080/index.html   IDE Integration  Eclipse, Intellij Idea and NetBeans all support Maven projects in some form. The archetype goes to great lengths to make your libGDX project usable within Eclipse and Intellij Idea. NetBeans is unsupported at the time of writing.   While Maven is IDE agnostic, the plugins for GWT and Android are not. Plugins for Eclipse differ in how the interpret the Maven configuration for GWT and Android projects from those in Intellij. The following sections describe how to import a project into both IDE’s after creating it using the libGDX archetype.   Eclipse  Before you can import your project, you need to install the following Eclipse plugins:      m2e, this should already available in a clean Eclipse installation (Java and Java EE editions). It provides the basic Maven support within Eclipse.   m2e-android provides Maven integration for Android projects in Eclipse.   Google Web Toolkit, the Eclipse plugin that allows you to develop GWT projects.   Once the plugins are installed, you can import your Maven libGDX projects by going to File -&gt; Import… -&gt; Maven -&gt; Existing Maven Projects. This will import the parent pom as project along with the core, desktop, android and html project.   Note that the HTML project might not be recognized as a GWT project by Eclipse. To fix this, right click the project, go to Properties -&gt; Google -&gt; Web Toolkit. Check “Use Google Web Toolkit”. Then go to Properties -&gt; Google -&gt; Web Application, check “This project has a WAR directory”, specify target/webapp and finally check “Launch and deploy from this directory”.   From there on you can run &amp; debug just as you’d do if you setup your projects via the gdx-setup-ui.   If you change anything in the assets, you need to run “mvn -Phtml package” again and refresh the html project in Eclipse.   IntelliJ Idea  Before you start, you should make sure IntelliJ Idea knows where your Maven installation is located. Go to File -&gt; Settings, and in the tree in the dialog chose Maven. Specify the directory where your Maven installation lives.   Once you created your project via the archetype you can import it into IntelliJ Idea. Go to File -&gt; Open Project, then navigate to the root directory of the project.   Once the project is loaded, you have to enable the profiles. Open the Maven Project view and check the three profiles, desktop, android and HTML.      To run the desktop project, create a new configuration via Run -&gt; Edit Configurations. Create a new configuration by clicking the + button in the top left, and select Application. Set the Main class to the desktop starter class, and select the desktop module.      Launch this configuration to run your app on the desktop.   To run the android project, create a new configuration, this time selecting Android Application when creating the configuration. Select the Android module, then check Run Maven Goal and click on the … button to the right. In the dialog, select the Android project, then Lifecycle, and from the list of goals package. You can also set Target Device to Show chooser dialog so you get prompted whether to deploy to a device or an emulator.      Launch this configuration to run your app on your Android device or emulator.  ",
        
        "url": "/wiki/articles/maven-integration" },{
        "title": "Memory management",
        "excerpt":
        
        "Games are resource heavy applications. Images and sound effects can take up a considerable amount of RAM. Also, most of these resources are not managed by the Java garbage collector. Instead they are managed by native drivers. Having the garbage collector decide when to release a 5 megabyte texture from video ram wouldn’t be a too bright idea either.   We want fine grained control over the life-time of our resources. There are multiple classes in libGDX which represent such resources. They all implement a common Disposable interface which indicates that instances of this class need to be disposed of manually at the end of the life-time. Failure to dispose resources will lead to severe memory leaks!   The following classes need to be disposed of manually (might not be complete, click here instead for the full list):      AssetManager   Bitmap   BitmapFont   BitmapFontCache   CameraGroupStrategy   DecalBatch   ETC1Data   FrameBuffer   Mesh   Model   ModelBatch   ParticleEffect   Pixmap   PixmapPacker   Shader   ShaderProgram   Shape   Skin   SpriteBatch   SpriteCache   Stage   Texture   TextureAtlas   TileAtlas   TileMapRenderer   com.badlogic.gdx.physics.box2d.World   all bullet classes   Resources should be disposed of as soon as they are no longer needed, freeing up memory associated with them. Accessing a disposed resource will result in undefined errors, so make sure to clear out all references you have to a disposed resource.   When in doubt about whether a specific class needs to be disposed of, check if it has a dispose() method which implemented from Disposable interface of com.badlogic.gdx.utils. If it does, you are now working with a native resource.      An idea to list all of such classes is to clone libGDX to your system, then use text editor such as Sublime Text or others to open its root directory, search for implements Disposable. It will show you result of all source files (thus class) you need to call dispose() when you’re done with it. Note that all other classes that extend from such result classes are also applicable. List above is already complete. Note that a few classes from result come from test class of libGDX itself which are not relevant to your interest.    Object pooling   Object pooling is the principle of reusing inactive or “dead” objects, instead of creating new objects every time. This is achieved by creating an object pool, and when you need a new object, you obtain it from that pool. If the pool has an available (free) object, it is returned. If the pool is empty, or does not contain free objects, a new instance of the object is created and returned. When you no longer need an object, you “free” it, which means it is returned to the pool. This way, object allocation memory is reused, and garbage collector is happy.   This is vital for memory management in games that have frequent object spawning, like bullets, obstacles, monsters, etc.   libGDX offers a couple tools for easy pooling.      Poolable interface   Pool   Pools   Implementing the Poolable interface means you will have a reset() method in your object, which will be automatically called when you free the object.   Below is a minimal example of pooling a bullet object.   public class Bullet implements Pool.Poolable {      public Vector2 position;     public boolean alive;      /**      * Bullet constructor. Just initialize variables.      */     public Bullet() {         this.position = new Vector2();         this.alive = false;     }          /**      * Initialize the bullet. Call this method after getting a bullet from the pool.      */     public void init(float posX, float posY) {     \tposition.set(posX,  posY);     \talive = true;     }      /**      * Callback method when the object is freed. It is automatically called by Pool.free()      * Must reset every meaningful field of this bullet.      */     @Override     public void reset() {         position.set(0,0);         alive = false;     }      /**      * Method called each frame, which updates the bullet.      */     public void update (float delta) {     \t     \t// update bullet position     \tposition.add(1*delta*60, 1*delta*60);     \t     \t// if bullet is out of screen, set it to dead     \tif (isOutOfScreen()) alive = false;     } }   In your game world class:  public class World {      // array containing the active bullets.     private final Array&lt;Bullet&gt; activeBullets = new Array&lt;Bullet&gt;();      // bullet pool.     private final Pool&lt;Bullet&gt; bulletPool = new Pool&lt;Bullet&gt;() { \t@Override \tprotected Bullet newObject() { \t\treturn new Bullet(); \t}     };      public void update(float delta) { \t         // if you want to spawn a new bullet:         Bullet item = bulletPool.obtain();         item.init(2, 2);         activeBullets.add(item);          // if you want to free dead bullets, returning them to the pool:     \tBullet item;     \tint len = activeBullets.size;     \tfor (int i = len; --i &gt;= 0;) {     \t    item = activeBullets.get(i);     \t    if (item.alive == false) {     \t        activeBullets.removeIndex(i);     \t        bulletPool.free(item);     \t    }     \t}     } }   The Pools class provides static methods for dynamically creating pools of any objects (using ReflectionPool and black magic). In the above example, it could be used like this.  private final Pool&lt;Bullet&gt; bulletPool = Pools.get(Bullet.class);   How to Use Pool   A Pool&lt;&gt; manages a single type of object, so it is parameterized by that type. Objects are taken from a specific Pool instance by invoking obtain and then should be returned to the Pool by invoking free. The objects in the pool may optionally implement the Pool.Poolable interface (which just requires a reset() method be present), in which case the Pool will automatically reset the objects when they are returned to the pool. By default, objects are initially allocated on demand (so if you never invoke obtain, the Pool will contain no objects). It is possible to force the Pool to allocate a number of objects by calling fill() after instantiation. Initial allocation is useful to have control over when these first time allocations occur.   You must implement your own subclass of Pool&lt;&gt; because the newObject method is abstract.   Pool Caveats   Beware of leaking references to Pooled objects. Just because you invoke “free” on the Pool does not invalidate any outstanding references. This can lead to subtle bugs if you’re not careful. You can also create subtle bugs if the state of your objects is not fully reset when the object is put in the pool.   Profiling Memory leaks   If you’re encountering memory leaks, tools like VisualVM (free) and JProfiler (trial/paid) prove useful in tracking down the issue. These memory profilers will tell you what type of object is eating up the memory. From there on you can start tracking down the leak.   LeakCanary is a free tool initially developed to automatically detect leaks in Android applications, but it can be configured to run on the JVM as well.  ",
        
        "url": "/wiki/articles/memory-management" },{
        "title": "Meshes",
        "excerpt":
        
        "A mesh is a collection of vertices (and optionally indices) which describe a batch of geometry for rendering. The vertices are held either in VRAM in form of vertex buffer objects (VBOs) or in RAM in form of vertex arrays. VBOs are faster and are used by default if the hardware supports it. Like Textures, meshes are managed and will be automatically reloaded when the context is lost.   Meshes are used by many core graphics classes in Libgdx, such as SpriteBatch and DecalBatch as well as the various 3D format loaders. A key design principle of libGDX is in storing geometry in a mesh in order to upload all vertex information in one batch for rendering. Especially on mobile platforms, there are significant performance gains in batching by reducing the overhead of individual draw calls.   Creating a Mesh   Sometimes a procedural mesh is preferred over the use of an imported model from a 3D modeling application. The following code creates a simple full screen quad often useful for frame-based shader effects:   public Mesh createFullScreenQuad() {    float[] verts = new float[20];   int i = 0;    verts[i++] = -1; // x1   verts[i++] = -1; // y1   verts[i++] = 0;   verts[i++] = 0f; // u1   verts[i++] = 0f; // v1    verts[i++] = 1f; // x2   verts[i++] = -1; // y2   verts[i++] = 0;   verts[i++] = 1f; // u2   verts[i++] = 0f; // v2    verts[i++] = 1f; // x3   verts[i++] = 1f; // y2   verts[i++] = 0;   verts[i++] = 1f; // u3   verts[i++] = 1f; // v3    verts[i++] = -1; // x4   verts[i++] = 1f; // y4   verts[i++] = 0;   verts[i++] = 0f; // u4   verts[i++] = 1f; // v4    Mesh mesh = new Mesh( true, 4, 0,  // static mesh with 4 vertices and no indices     new VertexAttribute( Usage.Position, 3, ShaderProgram.POSITION_ATTRIBUTE ),     new VertexAttribute( Usage.TextureCoordinates, 2, ShaderProgram.TEXCOORD_ATTRIBUTE+\"0\" ) );    mesh.setVertices( verts );   return mesh; } // original code by kalle_h   Notice the use of a simple float array to build the basic vertex information. We define four vertices each composed of a position in window coordinates as well as a texture coordinate. Next we tell the mesh constructor this will be a static mesh with 4 vertices and no indices. We define two vertex attributes, stating their respective sizes and describing their properties using the built-in libGDX constants for common attribute types. The usage constants tell libGDX how to interpret each of the float values so that it can point OpenGL at the proper data when rendering. Note that in OpenGL ES2, the use of Shaders does free one up to include other types of attributes in vertices (ie. vertex illumination values in a baked vertex-lighting situation, or even physical properties like mesh flexibility for simple wind based vertex animation). Finally we set the the mesh vertices using our previously built float array.   Notice also the use of ShaderProgram constants to name the attributes. While not mandatory, it can be useful to maintain the same name for shader attributes for common properties such as vertex positions, normals, or texture coordinates as these are the names used across common libGDX shaders. This naming uniformity can help when one wants to swap shaders on the same meshes. See Shaders for more information.   Rendering   To render a mesh, simply set up the environment and call render with the desired primitive type. To render the above full-screen quad, we use a triangle fan (because that was how we structured the vertex positions):   mesh.render( GL10.GL_TRIANGLE_FAN );   More typically you will render with other primitives:   mesh.render( GL10.GL_TRIANGLES );           // OpenGL ES1.0/1.1  mesh.render( shader, GL20.GL_TRIANGLES );   // OpenGL ES2 requires a shader  mesh.render( shader, GL20.GL_LINES );       // renders lines instead   By default, a mesh will auto-bind its data upon a call to render(). Prior to calling render(), you will need to bind the texture and set model transformations and if using OpenGL ES2 you will need bind an appropriate Shaders and pass whatever uniforms it requires.  ",
        
        "url": "/wiki/graphics/opengl-utils/meshes" },{
        "title": "ModelBatch",
        "excerpt":
        
        "ModelBatch (code) is a class used for managing render calls. It is typically used to render instances of models, although it is not restricted to models. The ModelBatch class abstracts away all rendering code, providing a layer on top of it and allowing you to focus on more game specific logic. Every part of the ModelBatch functionality is customizable by design.   Caution: because ModelBatch manages the render calls and therefore the rendering context, you should not try to manually modify the render context (e.g. bind shaders, texture or meshes, or call any function starting with gl) in between the ModelBatch.begin() and ModelBatch.end() calls. This will not work and might cause unpredictable behavior. Instead use the customization options that ModelBatch offers.      Common misconceptions   Overview   Using ModelBatch   Gather render calls            What are render calls?       RenderableProvider           Gather Shaders            What is a shader?       ShaderProvider       Default shader           Sorting render calls   Managing the render context            RenderContext       TextureBinder                    TextureDescriptor                           Common misconceptions     ModelBatch is often compared to SpriteBatch. While this might be understandable from an API view, there are some very big differences making them less comparable. The main difference is that SpriteBatch merges multiple sprites into a single draw call, while ModelBatch doesn’t combine render calls. This does have performance implications, so be aware to merge any render calls before sending them to the ModelBatch.   ModelBatch does not perform (frustum) culling. It simply hasn’t enough information to do this using the best performing method. By default, every call to ModelBatch.render() will at least lead to one actual render call. ModelBatch does allow you to customize this though and perform frustum culling prior to actually rendering. However, typically, you should perform (frustum) culling prior to calling ModelBatch.render().   Overview  So what does ModelBatch actually do?      It gathers render calls   It gathers a shader for each render call   It sorts the render calls   It manages the rendering context   It executes the render calls   That’s it. Nothing more, nothing less. And every part of this is customizable. Please be aware that because of this design, there might be multiple ways to accomplish the same basic task.   Using ModelBatch  ModelBatch is a relatively heavy weight object, because of the shaders it might create. When possible you should try to reuse it. You’d typically create a ModelBatch in the create() method. Because it contains native resources (like the shaders it uses), you’ll need to call the dispose() method when no longer needed.   Rendering should be done every frame, typically in your render() method. To start rendering you should call modelBatch.begin(camera);. Next use the modelBatch.render(...); method to add one or more render calls. When done adding render calls, you must call modelBatch.end(); to actually render to specified calls.      ModelBatch modelBatch;     ...      @Override     public void create () {         modelBatch = new ModelBatch();         ...     }      @Override     public void render () {         ... // call glClear etc.         modelBatch.begin(camera);         modelBatch.render(...);         ... // add other render calls         modelBatch.end();         ...     }      @Override     public void dispose () {         modelBatch.dispose();         ...     }  The call to modelBatch.render(...) is only valid in between the call to modelBatch.begin(camera) and modelBatch.end(). The actual rendering is performed at the call to end();. If you want to force rendering in between, then you can use the modelBatch.flush(); method.   The Camera you supply is hold by reference, meaning that it must not be changed in between the begin and end calls. If you need to switch camera in between the begin and end calls, then you can call the modelBatch.setCamera(camera);, which will flush() the batch if needed.   Gather render calls  What are render calls?  The purpose of ModelBatch is to manage render calls. So what exactly is a render call and how do you specify them?   A “render call” (often also referred to as “draw call”) is basically the instruction to the GPU to render (display) something. Simply said, each “render call” displays a shape with some properties (e.g. a location, image, color, etc). Or to be more precise, it instructs the GPU to render a given part of a mesh using a given shader in a given context. We will look more in depth on this later.      For basic usage, you don’t have to know the exact details about a render call, because ModelBatch abstracts them away. However, there’s a performance impact of render calls. Typically each render call is executed on the GPU, meaning that it is executed in parallel to CPU code. Whenever a new render call is executed, it might imply that GPU and CPU need to be synchronized. Or in other words, having are few large render calls is typically better performing than having many smaller render calls.    To specify a render call, libGDX contains the Renderable (code) class, which contains almost everything (except for the camera) required to perform a single render call. Basically it contains how (the shader) and where (the transformation) to render the shape (the mesh part) in which context (the environment and material).   The render(...) method of ModelBatch has many signatures (variations with different method arguments), one of which is ModelBatch.render(Renderable);. Using this method you can directly specify the render call you want to add to the ModelBatch. The other render(...) methods allow you to specify one or more render calls using a RenderableProvider. In those methods, the other arguments let you provide default values for those render calls. For example, when using the ModelBatch#render(RenderableProvider, Environment, Shader) method it will set (override) the environment and shader members of every Renderable the RenderableProvider provides.   RenderableProvider  RenderableProvider (code) is an interface which you can implement to supply one or more Renderable instances. Probably the most common implementation of this interface is the ModelInstance class, which traverses all nodes (parts) and translates them into a Renderable instance. However, you can use the RenderableProvider anyway you need. For example, when creating a voxel engine, you could create a Renderable for each chunk. Or when using an entity component system, you could use RenderableProvider as component.   RenderableProvider only has one method:  public void getRenderables (Array&lt;Renderable&gt; renderables, Pool&lt;Renderable&gt; pool);  This is a very open API (e.g. it gives you access to the Array of all previously added Renderables), but you should restrict your usage to only adding elements to the array. The pool can optionally be used to avoid allocation. You’re free to ignore it or to use it for any dynamic Renderable needed. Any Renderable you obtain() from it, will be automatically be free’d by the ModelBatch, you don’t have to take care for that.   Gather shaders  What is a shader?  A Shader  (code) is an interface that abstracts the implementation of actually performing the render call. Typically its implementation uses a ShaderProgram which is the GPU program (e.g. the vertex shader and fragment shader) needed to perform the render call. The Shader implementation also contains everything needed to use this ShaderProgram, like setting uniform values.   ShaderProvider  Independent of how the actual rendering is performed, the ModelBatch needs one Shader per Renderable. For this it uses the ShaderProvider (code) interface. For every Renderable added to the batch (even if it contains a shader), the getShader of the ShaderProvider will be called to fetch to shader to render it.   public Shader getShader (Renderable renderable);   By default the DefaultShaderProvider (code) is used, which will create a DefaultShader whenever a previous created shader can’t be reused. You can, however, customize this by supplying your own ShaderProvider or by extending the DefaultShaderProvider.   ModelBatch delegates managing Shader’s to the ShaderProviders. Because a Shader typically uses a ShaderProgram, they need to be disposed. When modelBatch.dispose(); is called, ModelBatch will call the dispose() method the ShaderProvider.   To help managing and reusing shaders, libGDX offers the abstract BaseShaderProvider (code). This class keeps track of all shaders created, reuses them if possible and disposes them when no longer needed. If you extend this class, it will call the createShader(Renderable) method when it hasn’t got a shader it can reuse. Whether a Shader can be reused, is determined by the call to shader.canRender(Renderable).   A typical use-case is to extend DefaultShaderProvider (which extends BaseShaderProvider) and provide a custom shader when needed, while falling back to the DefaultShader when you can’t use your custom shader.  public static class MyShaderProvider extends DefaultShaderProvider { \t@Override \tprotected Shader createShader (Renderable renderable) { \t\tif (renderable.material.has(CustomColorTypes.AlbedoColor)) \t\t\treturn new MyShader(renderable); \t\telse \t\t\treturn super.createShader(renderable); \t} }  Here the Material is used to decide whether the custom shader should be used. This is the preferred and easiest method. However, you can use any value, including the generic renderable.userData to decide which shader to use, as long as its shader.canRender(renderable) method returns true for the given renderable.   Default shader  When you don’t specify a custom ShaderProvider, then ModelBatch will use the DefaultShaderProvider. This provider creates a new DefaultShader instance when needed.   The DefaultShader class provides a default implementation of most of the standard material and environment attributes, including lighting, normal maps, reflection cubemaps, etc. That is: it binds the attribute values to the corresponding uniforms. A list of uniform names can be found here.      NOTE: by default, the shader program (the glsl files) use per-vertex lighting (Gouraud shading).Normal mapping, reflection etc. is not applied by default.    The behavior of this class is configurable by supplying a DefaultShader.Config instance to the DefaultShaderProvider.   DefaultShader.Config config = new DefaultShader.Config(); config.numDirectionalLights = 1; config.numPointLights = 0; config.numBones = 16; modelBatch = new ModelBatch(new DefaultShaderProvider(config));      NOTE: the default configuration is rarely the most optimal for each use-case. For example it uses 5 point lights and 2 directional lights, even if you’re only using 1 direction and 1 point light. Make sure to adjust it to your specific use-case to get the most out of it. If you’re using skinning, then the number of bones must match the number of bones the model is created with.    The GPU shader (the vertex and fragment shader) to be used is also configurable using this config. Because this shader can be used for various combinations of attributes, it typically is a so-called ubershader. This is shader glsl code of which parts are enabled or disabled based on the Renderable by using pre-processor macro directives. For example:   #ifdef blendedFlag \tgl_FragColor.a = diffuse.a * v_opacity; \t#ifdef alphaTestFlag \t\tif (gl_FragColor.a &lt;= v_alphaTest) \t\t\tdiscard; \t#endif #else \tgl_FragColor.a = 1.0; #endif   In this snippet, the actual code used for the shader depends on whether blendedFlag and/or alphaTestFlag are defined. The DefaultShader class defines these based on the values of the Renderable.   If you don’t specify a custom ubershader, then the default ubershader will be used (see the source of the: vertex shader and fragment shader). Although this shader supports most basic attributes (like skinning, diffuse and specular per-vertex lighting etc.), it is very generic and cannot support every possible combination of attributes.      If you want to have per-fragment lighting, normal mapping, reflection then you can use this “unofficial” shader. But, please note that this shader adds this functionality at the cost of e.g. being restricted to a single directional light.    If you look at the source of the default ubershader, then you’ll probably notice that it is huge and almost impossible to read, let alone maintain it. Luckily you don’t have to support every possible combination of attributes and as we’ve seen in the previous paragraph you can extend the DefaultShaderProvider and break it into multiple shaders to make it easier to maintain. For example:   public static class MyShaderProvider extends DefaultShaderProvider { \tDefaultShader.Config albedoConfig;  \tpublic MyShaderProvider(DefaultShader.Config defaultConfig) { \t\tsuper(defaultConfig); \t\talbedoConfig = new DefaultShader.Config(); \t\talbedoConfig.vertexShader = Gdx.files.internal(\"data/albedo.vertex.glsl\").readString(); \t\talbedoConfig.fragmentShader = Gdx.files.internal(\"data/albedo.fragment.glsl\").readString(); \t} \t@Override \tprotected Shader createShader (Renderable renderable) { \t\tif (renderable.material.has(CustomColorTypes.AlbedoColor)) \t\t\treturn new DefaultShader(renderable, albedoConfig); \t\telse \t\t\treturn super.createShader(renderable); \t} }   Sorting render calls  If render calls would be executed in a random order, then it would cause strange and less performing result. For example, if a transparent object would be rendered prior to an object that’s behind it then you won’t see the object behind it. This is because the depth buffer will prevent the object further away from being rendered. Sorting the render calls helps to solve this.   By default ModelBatch will use the DefaultRenderableSorter (code) to sort the render calls. This implementation will cause that opaque objects are rendered first from front to back, after which transparent objects are rendered from back to front. To decide whether an object is transparent or not, the default implementation checks the BlendingAttribute#blended value.   Customizing sorting can help increase performance. For example, sorting based on shader, mesh or used textures might help decreasing shader, mesh or texture switches. These kind of optimizations are very application specific. You can customize sorting by specifying your own RenderableSorter (code) implementation while constructing the ModelBatch. This interface contains only one method:  public void sort (Camera camera, Array&lt;Renderable&gt; renderables);  This method provides all information the ModelBatch has just before the actually rendering. It is also a very open API, you are allowed to modify the array as needed. This makes it possible to perform any last-minute actions (that might not be even related to sorting, like frustum culling) in this interface. The order of the renderables after this method completes, will be the order in which the render calls will be actually executed.   Managing the render context  ModelBatch allows you to avoid redundant OpenGL calls, including texture binds, across multiple Shader implementations. For example, when a Shader requires backface culling and a previous shader enabled backface culling, then the redundant call to glEnable and glCullFace can be avoided.   RenderContext  The RenderContext class tries to avoid these unnecessary calls. This class acts as a thin layer on top of OpenGL ES that keeps track of previous calls and therefore avoids making redundant calls. Only a small subset of the GL calls is implemented, but you can extend it to add additional calls. When not specified as argument in the constructor, ModelBatch will create and manage a RenderContext for you.   Caution: Obviously this will only work if all Shader implementations use the RenderContext instead of directly making GL calls. You should always use the RenderContext if possible, instead of directly calling the corresponding GL call.   For example: When depth testing is enabled using the RenderContext, then it will enable depth testing for you. Now when you use e.g. SpriteBatch then that disables depth testing but doesn’t update the RenderContext. This will lead to unexpected results. To avoid this, by default (when you don’t specify a RenderContext yourself) ModelBatch will reset the RenderContext on both the begin() and end() methods, by calling the same named methods on the RenderContext. This is to make sure that context switches outside the ModelBatch don’t interfere with the rendering.   However, when you specify your own RenderContext (which doesn’t have to be a custom implementation of it) then you’re responsible for calling the context.begin() and context.end() methods. This allows you use the same context for multiple ModelBatch instances or even avoid having to reset the context all together.      If you specify a RenderContext on construction then you own that RenderContext and are expected to reset (call its begin() and end() methods) when needed. You can call the modelBatch.ownsRenderContext method to check whether the ModelBatch owns and manages the RenderContext.    TextureBinder  To keep track of the textures currently being bound, RenderContext contains a textureBinder member. TextureBinder (code) is an interface used to keep track of texture binds, as well as texture context (e.g. the minification/magnification filters). By default the DefaultTextureBinder (code) is used. Although you can specify your own implementation, this is rarely required.   The DefaultTextureBinder uses every available texture unit (within the specified range) to avoid unneeded texture binds. A typical modern mobile GPU offers around 16 or 32 texture units to which textures can be bound. OpenGL ES limits the number of units to 32. You can specify the range to use when constructing the DefaultTextureBinder, using the offset and count arguments. If you don’t specify an offset, then 0 is assumed. If you don’t specify a count then all available remaining units will be used. ModelBatch will, by default, exclude texture unit 0 from the range, because this is often used for GUI. So by default, texture unit 1 to 31 will be used, unless the GPU supports less texture units.   DefaultTextureBinder supports two methods:     ROUNDROBIN: When a texture is already bound, then it is reused. Otherwise the first texture is bound to the first available unit, the next texture is bound to the next available unit, and so on. When all available units are used, then binding restarts at the first available unit, overwriting the previous bound texture.   WEIGHTED: Weights the textures by counting the number of times a texture is used or not. Often reused textures are less likely to be overwritten, while less reused textures are more likely to be overwritten.   By default ModelBatch will use the WEIGHTED method.   You can bind a texture using TextureBinder by calling the bind(...) method. This method will return the unit the texture is bound to. So, practically, you can bind a texture in your Shader to an uniform like this:  program.setUniformi(uniformLocation, context.textureBinder.bind(texture));   TextureDescriptor  The api often uses a TextureDescriptor when specifying a texture. This is because you might want to use a texture but do require specific context properties. These properties currently include the minification and magnification filters, as well as the horizontal wrapping and vertical wrapping. Therefor the TextureDescriptor is also used by e.g. the TextureAttribute and CubemapAttribute. For convenience, TextureBinder allows you to directly specify a TextureDescriptor:  program.setUniformi(uniformLocation, context.textureBinder.bind(     (TextureAttribute)(renderable.material.get(TextureAttribute.Diffuse)))         .textureDescription));  ",
        
        "url": "/wiki/graphics/3d/modelbatch" },{
        "title": "ModelBuilder, MeshBuilder and MeshPartBuilder",
        "excerpt":
        
        "ModelBuilder  ModelBuilder (code) is a utility class to create one or more models on code. It allows you to include one or more nodes, each node consisting of one or more parts. It does, however, not support building a node hierarchy (child nodes). Be aware that building a model on code can be a costly operation and might trigger the garbage collector.  Building one or more models  To start building a model use the begin() method, after which you must call the end() when you’re done building the model. The end() method will return the newly created model. You can build multiple models using the same ModelBuilder, but not at the same time. For example:  ModelBuilder modelBuilder = new ModelBuilder();  modelBuilder.begin(); ... //build one or more nodes Model model1 = modelBuilder.end();  modelBuilder.begin(); ... //build one or more nodes Model model2 = modelBuilder.end();  Managing resources  Keep in mind that models contain one or more meshes and therefore needs to be disposed. A model built via ModelBuilder will always be responsible for disposing all meshes it contains, even if you provide the Mesh yourself. Do not share a Mesh along multiple Models.   A Model built via ModelBuilder will not be made responsible for disposing any textures or any other resources contained in the materials. You can, however, use the manage(disposable) method to make the model responsible for disposing those resources. For example:  ModelBuilder modelBuilder = new ModelBuilder(); modelBuilder.begin(); Texture texture = new Texture(...); modelBuilder.manage(texture); ... //build one or more nodes Model model = modelBuilder.end(); ... //use the model and when done: model.dispose(); // this will dispose the texture as well  Creating nodes  A Model consists of one or more nodes. To start building a new node inside the model you can use the node() method. This will add a new node and make it active for building. It will also return the Node so you can reference it for later use or for example set its id.  ModelBuilder modelBuilder = new ModelBuilder(); modelBuilder.begin(); Node node1 = modelBuilder.node(); node1.id = \"node1\"; node1.translation.set(1, 2, 3); ...//build node1 Node node2 = modelBuilder.node(); node2.id = \"node2\"; ...//build node2 Model model = modelBuilder.end();  Note that node id’s should unique within the model. A typical use-case would be:  ModelBuilder modelBuilder = new ModelBuilder(); modelBuilder.begin(); modelBuilder.node().id = \"node1\"; ...//build node1 modelBuilder.node().id = \"node2\"; ...//build node2 Model model = modelBuilder.end();  Using the node() method for the first node is optional. For example, for models consisting of only a single node, you can immediately start creating the node parts without having to call the node() method.   There can only be one Node active for building at a time. Calling the node() method will stop building the previous node (if any) and start building the newly created node. The nodes will only be valid (complete), however, after the Model is completely built (the call to end() is made).   Creating node parts  A Node can contain one or more parts. Each part of a node will be rendered at the same location (the node transformation), but can be made up of a different material (e.g. shader uniforms) and/or mesh (e.g. shader (vertex) attributes).      A NodePart is the smallest renderable part of a Model. Every visible NodePart implies a render call (or “draw call” if you prefer). Reducing the number of render calls can help to decrease the time it takes to render the model. Therefore it is advised to try to combine multiple parts to a single part where possible.    To add a NodePart to the current node you can use one of the part(...) methods. A NodePart is basically the combination of a MeshPart and Material. You must always supply the material when calling one of the part(...) methods. For the MeshPart, however, ModelBuilder allows you to either specify the (part of the) mesh yourself, or to start building the MeshPart using a MeshPartBuilder.      Keep in mind that the Model will always be made responsible for disposing the Mesh, regardless the method used to create part.    A MeshPartBuilder is an interface (implemented by MeshBuilder, see below) which contains various helper methods to create a mesh. If you use the part(...) method to construct the MeshPart using a MeshPartBuilder, then ModelBuilder will try to combine multiple parts into the same Mesh. This will in most cases reduce the number of Mesh binds. This is only possible if the parts are made up using the same vertex attributes. For example:  ModelBuilder modelBuilder = new ModelBuilder(); modelBuilder.begin(); MeshPartBuilder meshBuilder; meshBuilder = modelBuilder.part(\"part1\", GL20.GL_TRIANGLES, Usage.Position | Usage.Normal, new Material()); meshBuilder.cone(5, 5, 5, 10); Node node = modelBuilder.node(); node.translation.set(10,0,0); meshBuilder = modelBuilder.part(\"part2\", GL20.GL_TRIANGLES, Usage.Position | Usage.Normal, new Material()); meshBuilder.sphere(5, 5, 5, 10, 10); Model model = modelBuilder.end();  This will create a model consisting of two nodes. Each node consisting of one part. The Mesh of both parts is shared, so there’s only a single Mesh created for this Model. Note that in this example the vertex attributes are specified using a bit mask, which will cause ModelBuilder to create default (3D) VertexAttributes. You could also specify the VertexAttributes yourself.      Because ModelBuilder reuses the MeshPartBuilder instances for multiple parts, you cannot build multiple parts at the same time. Per ModelBuilder you can only build one Model, Node and MeshPart at any given time. Calling the part(...) method will make the previous MeshPartBuilder invalid.    See the MeshPartBuilder section below for more information on how to use the MeshPartBuilder to create the shape of the part.  MeshBuilder  MeshBuilder (code) is a utility class to create one or more meshes, optionally consisting of one or more MeshParts. While a MeshBuilder is typically constructed and maintained by the ModelBuilder using the ModelBuilder#part(...) method, it is possible to use MeshBuilder without using a ModelBuilder. For this, you can use the begin(...) method to start building a mesh, after which you must call the end() method when you’re done building the mesh. The begin methods accepts various arguments to specify the vertex attributes and optionally primitive type (required when not creating mesh part(s)). The end() method will return the newly created Mesh. You can build multiple meshes using the same MeshBuilder instance, but not at the same time:  MeshBuilder meshBuilder = new MeshBuilder(); meshBuilder.begin(Usage.Position | Usage.Normal, GL20.GL_TRIANGLES); ...//build the first mesh Mesh mesh1 = meshBuilder.end();  meshBuilder.begin(Usage.Position | Usage.Normal | Usage.ColorPacked, GL20.GL_TRIANGLES); ...//build the second mesh Mesh mesh2 = meshBuilder.end();      Keep in mind that the Mesh must be disposed when you no longer need it.    Use the part(...) method to create a Mesh consisting of multiple parts. This will create a new MeshPart and set it active for building.  MeshBuilder meshBuilder = new MeshBuilder(); meshBuilder.begin(Usage.Position | Usage.Normal); MeshPart part1 = meshBuilder.part(\"part1\", GL20.GL_TRIANGLES); ... // build the first part MeshPart part2 = meshBuilder.part(\"part2\", GL20.GL_TRIANGLES); ... // build the second part Mesh mesh = meshBuilder.end();  While the part(...) method returns the MeshPart so you can reference it for later use, it will not be valid until the end() method is called. You can only create one MeshPart at a time, calling the part(...) method will stop building the previous part and start building the new part.   All parts of the same Mesh share the same VertexAttributes. The primitive type can vary among parts though. For example, the following snippet creates two parts each with a different primitive type, but sharing the same mesh:  meshBuilder.begin(Usage.Position | Usage.Normal); MeshPart part1 = meshBuilder.part(\"part1\", GL20.GL_TRIANGLES); ... // build the first part MeshPart part2 = meshBuilder.part(\"part2\", GL20.GL_TRIANGLE_STRIP); ... // build the second part Mesh mesh = meshBuilder.end();   MeshBuilder implements MeshPartBuilder. Creating the actual shape of the (part of the) mesh, is described in the MeshPartBuilder section.   MeshPartBuilder  MeshPartBuilder (code) is a utility interface which supplies various methods for creating a (part of a) mesh. You can either use ModelBuilder.part(...) or construct a MeshBuilder to obtain a MeshPartBuilder. All methods of the MeshPartBuilder interface can only be called as long as the MeshPart is being build (most commonly between the call to the part(...) method and the call to the next part(...) or end() method of either ModelBuilder or MeshBuilder).   Use the getMeshPart() method to obtain the MeshPart currently being build. Use the getAttributes() method to obtain the VertexAttributes of the Mesh being build.   A VertexAttribute of Usage.Position (either 2D or 3D) is required. There are no further restriction on the specified vertex attributes. However, most (especially higher level) methods are only implemented for position, normal, color (either packed or unpacked) and texture coordinates attributes. If you use other attributes as well, then a default (commonly zero) value will be used.   Most methods allow multiple signatures to specify the values for the vertex attributes. A little helper class VertexInfo is used to specify these on a per vertex basis. For example the rect(...) method accepts a VertexInfo for each corner, instead of using a method with all possible combinations or arguments for the vertex values of each corner. Be aware that the VertexInfo keeps track of whether a specific value has been set, therefor you should always use the setXXX methods. Use null in case you want to unset a value.   Use the setColor(...) method to specify the default color that will be used when the VertexAttributes contain a color VertexAttribute, but no color is set in e.g. the VertexInfo. For example:  meshPartBuilder.setColor(Color.RED); VertexInfo v1 = new VertexInfo().setPos(0, 0, 0).setNor(0, 0, 1).setCol(null).setUV(0.5f, 0.0f); VertexInfo v2 = new VertexInfo().setPos(3, 0, 0).setNor(0, 0, 1).setCol(null).setUV(0.0f, 0.0f); VertexInfo v3 = new VertexInfo().setPos(3, 3, 0).setNor(0, 0, 1).setCol(null).setUV(0.0f, 0.5f); VertexInfo v4 = new VertexInfo().setPos(0, 3, 0).setNor(0, 0, 1).setCol(null).setUV(0.5f, 0.5f); meshPartBuilder.rect(v1, v2, v3, v4);  In this example, because the VertexInfo has no color set (setCol(null)), the default will be used which is set to a red color.   Use the setUVRange to specify the default texture coordinates range that will be used when no texture coordinates are specified. Along with the default color, this is especially useful for simple shapes where only positions are needed and other vertex information can be derived from the shape. For example:  meshPartBuilder.setColor(Color.RED); meshPartBuilder.setUVRange(0.5f, 0f, 0f, 0.5f); meshPartBuilder.rect(0,0,0, 3,0,0, 3,3,0, 0,3,0, 0,0,1); // the last three arguments specify the normal  Use the setVertexTransform(...) method to supply a transformation matrix that should be applied to all vertices following after that call.  ",
        
        "url": "/wiki/graphics/3d/modelbuilder-meshbuilder-and-meshpartbuilder" },{
        "title": "ModelCache",
        "excerpt":
        
        "When rendering many Models using ModelBatch you might notice that it will have an impact on performance. This is often because every [part of the model will cause a render call. Every render call requires the GPU and the CPU to synchronize, which is a relatively costly operation. Therefor you typically want to keep the number of render calls to a minimum.   There are multiple ways to reduce the number of render calls. For example by frustum culling (not rendering what wont be visible anyway), by optimizing your models to contain less parts or by merging models to reduce the number of models. ModelCache (code) allows to do the latter two of those at runtime.      BE AWARE merging and optimizing models at runtime is typically still a relatively costly operation and when done incorrectly it can even make performance worse. Always make sure to implement other options of reducing the render calls, like frustum culling or optimizing assets, as well.    ModelCache is somewhat comparable to SpriteCache and SpriteBatch which are used for 2D rendering. However, models typically are more complex compared to sprites in for example the number of vertices, vertex attributes and the material.   ModelCache will not merge skinned meshes. When you add a skinned mesh it will not be merged, but instead kept as is. So it is safe to add a skinned mesh to the cache, but it won’t reduce the number of render calls (because skinning is applied inside the render call).   Using ModelCache   Using ModelCache is similar to using ModelBatch. To start merging and optimizing models you’ll have to call the begin() method. Then you can add() one or more ModelInstances or other RenderableProviders. After that call the end() method to actually perform the merging.   When the cache is created (after the call to end()) you can use it in the call to ModelBatch.render.   ModelCache owns several native resources. Therefor you should dispose() the cache when no longer needed.   ModelBatch batch; ModelCache cache; public void create() {     //...     Array&lt;ModelInstance&gt; instances = createTheInstanceYouWantToMerge();     cache = new ModelCache();     cache.begin();     cache.add(instances);     cache.end();     //... create the batch and such } public void render() {     //... call glClear and such     batch.begin();     batch.render(cache, environment);     batch.end(); } public void dispose() {     // dispose models and such     cache.dispose();     batch.dispose(); }   It is possible to use ModelCache for dynamic models, for example objects that move every frame. In that case you can recreate the cache on every render call.   ModelBatch batch; ModelCache cache; Array&lt;ModelInstance&gt; instances; public void create() {     //...     instances = createTheInstanceYouWantToMerge();     cache = new ModelCache();     //... create the batch and such } public void render() {     //... call glClear and such     cache.begin();     cache.add(instances);     cache.end();      batch.begin();     batch.render(cache, environment);     batch.end(); } public void dispose() {     // dispose models and such     cache.dispose();     batch.dispose(); }  ",
        
        "url": "/wiki/graphics/3d/modelcache" },{
        "title": "Models",
        "excerpt":
        
        "A model represents a 3D asset consisting of a hierarchy of nodes, where each node is a combination of a geometry (mesh) and material. Optionally a model can also contain information about animation and/or skinning.   A model is not intended to be rendered directly. Instead you should create one or more ModelInstances of a Model, which are used for the actual rendering. The structure of a ModelInstance is roughly the same as a Model.   Nodes  A model is a hierarchical representation of nodes. In practice this means that a model contains an array of nodes and each node contains also an array of nodes. Nodes can be accessed using the public nodes array or using one of the getNode(...) methods. Each node has a unique id within a model.   Each Node can belong to only one Model or one ModelInstance at a time and is never shared amongst multiple Models or ModelInstances. Modifications to a Node of a ModelInstance will therefore only affect that particular ModelInstance. Modifications to a Node of a Model will affect that Model and all ModelInstances created from it after the modifications, but previously created ModelInstances from that Model remain unchanged.   Node transformation  Nodes can be transformed (translate, rotate and/or scale), causing all child nodes to be also transformed. This transformation can be set while loading the model and/or changed programmatically. To change the transformation the node has a translation and scale vector and a rotation quaternion. When these values are changed the transformation (including all children) must be updated to reflect the changes. This can be done using the model.calculateTransforms(); method (also available for the ModelInstance class).   When the Node transformations are calculated or recalculated, they are stored in the localTransform and globalTransform matrices. The localTransform matrix represents the transformation of the node relative to its parent node. The globalTransform matrix represents the transformation of the node relative to the model or modelinstance. In other words: the globalTransform of a Node is the globalTransform of its parent node multiplied by the node’s localTransform.   When an animation is applied to a ModelInstance, the isAnimated value is set to true. This will cause the translation, scale and rotation values not to be used when recalculating the transforms.   NodePart  A node can optionally have a visual representation. Therefor the Node class contains an array of NodeParts. Each NodePart consist of a MeshPart and Material, specifying how (the material) and where (the node transformation) the shape (the MeshPart) should be rendered. Optionally it also contains information about mesh deformation (used for skinning) and UV mapping (used for multiple texture coordinates).   Managing resources  A model consists of one or more meshes and in most cases one or more textures. Both the Mesh and Texture classes (and potentially other resources) must be properly disposed when no longer needed. A model is responsible for disposing all the resources it contains. When no longer needed, a model should be disposed using it’s dispose(); method, causing all backing resources to be disposed and therefor invalidating all depending objects (like ModelInstance).   To get the list of Disposables a model is responsible for, use the getManagedDisposables() method. To programmatically make a model responsible for a resource, use the manageDisposable(Disposable) method. For example, when changing the texture of a model and you want the texture to be disposed when the model is disposed.   Because a model is responsible for its resources, it is recommended to keep them separated. For example, it is not advised to share resources amongst multiple models. Instead, in most cases it is possible to combine models completely (prior to loading or building them) or to share resources amongst ModelInstances instead of Models.  ",
        
        "url": "/wiki/graphics/3d/models" },{
        "title": "Modules overview",
        "excerpt":
        
        "Introduction   libGDX comprises several modules that provide services for each step of a typical game architecture.      Input - Provides a unified input model and handler for all platforms. Supports keyboard, touchscreen, accelerometer and mouse where available.   Graphics - Enables the drawing of images to the screen using the hardware provided OpenGL ES implementation.   Files - Abstracts file access on all platforms by providing convenient methods for read/write operations regardless of the media.   Audio - Facilitates sound recording and playback on all platforms.   Networking - Provides methods to perform networking operations, such as simple HTTP get and post requests, and TCP server/client socket communication.   The following diagram shows the modules in a simple game architecture:      Modules   The following part briefly describes each module providing the most common use cases for each. For more in-depth information, be sure to check out the individual wiki sections for the respective modules!   Input  The Input module enables the polling of different input states on every platform. It allows polling the state of each key, touchscreen and accelerometer. On the desktop the touchscreen is replaced by the mouse while the accelerometer is not available.   It also offers the means to register input processors to use an event based input model.   The following code snippet gets the current touch coordinates if a touch (or mouse down on desktop) event is in progress:  if (Gdx.input.isTouched()) {   System.out.println(\"Input occurred at x=\" + Gdx.input.getX() + \", y=\" + Gdx.input.getY()); }  In similar fashion all the supported input means can be polled and handled.   Graphics  The Graphics module abstracts the communication with the GPU and provides convenience methods to obtain instances of OpenGL ES wrappers. It takes care of all the boilerplate code needed to get hold of the OpenGL instance and handles all implementations provided by the manufacturer.   Depending on the underlying hardware, the wrappers may be or may not be available.   The Graphics module also provides methods to generate Pixmaps and Textures.   For example, to obtain the OpenGL API 2.0 instance, the following code will be used:  GL20 gl = Gdx.graphics.getGL20 ();  The method will return an instance that can be used to draw onto the screen. In case the hardware configuration does not support OpenGL ES v2.0, null is returned.   The following snippet clears the screen and paints it with red.  gl.glClearColor(1f, 0.0f, 0.0f, 1); gl.glClear(GL20.GL_COLOR_BUFFER_BIT);  It always returns the specific implementation of the API (lwjgl, jogl or android), so the main application doesn’t need to know specifics and will work across the whole range of platforms if supported.   The following API versions are supported:                  GL Version       Method to access                       2.0       Gdx.graphics.getGL20();                 3.0       Gdx.graphics.getGL30();           To learn more about the Graphics module check its documentation here.   Files  The Files module provides a generic way to access files regardless of the platform. It makes it easy to read and write files. File writing has some limitations, which are due to the platform security limitations.   The most common use case for the Files module, is to load game assets (textures, sound files) from the same sub-directory of the application for all platforms. It is also very useful for writing high scores or game state to files.   The following example creates a Texture from a file present in the $APP_DIR/assets/textures directory.  Texture myTexture = new Texture(Gdx.files.internal(\"assets/textures/brick.png\"));  This is a very powerful abstraction layer as it works on both Android and desktop.   Audio  The Audio module makes the creation and playback of audio files extremely simple. It also gives direct access to the sound hardware.   It handles two types of sound files. Music and Sound. Both types support the WAV, MP3 and OGG formats.   Sound instances are loaded into memory and can be played back any time. It is ideal for in-game sound effects that will be used multiple times, like explosions or gunshots.   Music instances on the other hand are streams from files on the disk (or SD card). Every time a file is played, it is streamed from the file to the audio device.   The following code snippet plays a sound file, myMusicFile.mp3, from disk repeatedly with the volume half turned up:  Music music = Gdx.audio.newMusic(Gdx.files.getFileHandle(\"data/myMusicFile.mp3\", FileType.Internal)); music.setVolume(0.5f); music.play(); music.setLooping(true);   Networking  The Networking module offers functions useful for game networking and can be used to add multiplayer, send players to your website, or perform other networking tasks. These features are available across multiple platforms, although some platforms may require additional considerations or lack certain features.   The Networking module includes configurable TCP client and server sockets with settings optimized for low latency.   There are also methods and utilities for making HTTP requests. One such utility is the Request Builder, which uses method chaining to easily create HTTP Requests.   The Request Builder can be used to create HTTP Requests using the following code snippet:  HttpRequestBuilder requestBuilder = new HttpRequestBuilder(); HttpRequest httpRequest = requestBuilder.newRequest()    .method(HttpMethods.GET)    .url(\"http://www.google.de\")    .build(); Gdx.net.sendHttpRequest(httpRequest, httpResponseListener);   It can also be used to create HTTP Requests with arguments using the following code snippet:  HttpRequestBuilder requestBuilder = new HttpRequestBuilder(); HttpRequest httpRequest = requestBuilder.newRequest()    .method(HttpMethods.GET)    .url(\"http://www.google.de\")    .content(\"q=libgdx&amp;example=example\")    .build(); Gdx.net.sendHttpRequest(httpRequest, httpResponseListener);  ",
        
        "url": "/wiki/app/modules-overview" },{
        "title": "More Info",
        "excerpt":
        
        "If you have been sent to this page it is because you failed to provide enough information for us to take action on your issue. We handle hundreds of issues and cannot burn our time explaining to each person how to properly post an issue. Instead we send you here, which explains what you need to do.   What do I do now?   We have an extensive guide called Getting Help. Read it. Yes, the whole thing. 99% of the time you will need to post an executable example.   If your issue has been closed, don’t take it personal or be offended. Instead, remedy the problem by adding the necessary information to your issue and it will be re-opened. Do not file another separate issue for the same problem.   Why are you doing this to me?   A problem must be reproducible in order to 1) test and identify the problem, 2) implement a fix, and 3) verify the fix works. If we can’t reproduce your problem, we can’t fix it. We do not blindly implement fixes and hope they work.   If you think you posted enough information for us to work on your issue, you are wrong. Either your post lacks required information or it requires too much effort for us to figure out what you are talking about. You almost certainly did not include an executable example.   Indignation, resentment, exasperation, and friends   libGDX is a community-based open source project. If you are not able to contribute a fix for your issue then we are happy to help, but we do require you to provide enough information so that we can be productive when we spend our time on your issue. Please respect our time.  ",
        
        "url": "/wiki/misc/more-info" },{
        "title": "Mouse, Touch and Keyboard",
        "excerpt":
        
        "The main input devices libGDX supports are the mouse on the desktop/browser, touch screens on Android and keyboards. Let’s review how libGDX abstracts those.   Keyboard  Keyboards signal user input by generating events for pressing and releasing a key. Each event carries with it a key-code that identifies the key that was pressed/released. These key-codes differ from platform to platform. libGDX tries to hide this fact by providing its own key-code table, see the Keys (source) class. You can query which keys are currently being pressed via Polling.   Key-codes alone do not give us information about which character the user actually entered. This information is often derived from the state of multiple keys, e.g. the character ‘A’ is generated by the keys ‘a’ and ‘shift’ being pressed simultaneously. In general, deriving characters from the keyboard’s state (which keys are down) is non-trivial. Thankfully, the operating system usually has a means to hook up an event listener that not only reports key-code events (key pressed/key released), but also characters. libGDX uses this mechanism under the hood to provide you with character information. See Event Handling.   Mouse &amp; Touch  Mouse and touch input allow the user to point at things on the screen. Both input mechanisms report the location of interaction as 2D coordinates relative to the upper left corner of the screen, with the positive x-axis pointing to the right and the y-axis pointing downward.   Mouse input comes with additional information, namely which button was pressed. Most mice feature a left and a right mouse button as well as a middle mouse button. In addition, there’s often a scroll wheel which can be used for zooming or scrolling in many applications.   Touch input does not have the notion of buttons and is complicated by the fact that multiple fingers might be tracked depending on the hardware. First generation Android phones only supported single-touch. Starting with phones like the Motorola Droid, multi-touch became a standard feature on most Android phones.   Note that touch can be implemented quite differently on different devices. This can affect how pointer indexes are specified and released and when touch events are fired. Be sure to test your control scheme on as many devices as possible. There are also many input test apps available in the market which can help determine how a particular device reports touch and aid in designing a control scheme that works best across a range of devices.   libGDX abstracts unified handling of mouse and touch input. We view mouse input as a specialized form of touch input. Only a single finger is tracked, and in addition to coordinates we also report which buttons were pressed. For touch input we support tracking multiple fingers (pointers) and report the left mouse button for all events.   Note that on Android the coordinate system is either relative to portrait or landscape mode, depending on what you set for your application.   Mouse and touch input can either be polled or processed via Event Handling   Touch Point   To get correct world position of touch point or mouse cursor it is necessary to unproject the raw screen position coordinates with camera that operate in world space. Below is self contained example of doing just that.   public class SimplerTouchTest extends ApplicationAdapter implements InputProcessor { \t// we will use 32px/unit in world \tpublic final static float SCALE = 32f; \tpublic final static float INV_SCALE = 1.f/SCALE; \t// this is our \"target\" resolution, note that the window can be any size, it is not bound to this one \tpublic final static float VP_WIDTH = 1280 * INV_SCALE; \tpublic final static float VP_HEIGHT = 720 * INV_SCALE;  \tprivate OrthographicCamera camera; \tprivate ExtendViewport viewport;\t\t \tprivate ShapeRenderer shapes;  \t@Override public void create () { \t\tcamera = new OrthographicCamera(); \t\t// pick a viewport that suits your thing, ExtendViewport is a good start \t\tviewport = new ExtendViewport(VP_WIDTH, VP_HEIGHT, camera); \t\t// ShapeRenderer so we can see our touch point \t\tshapes = new ShapeRenderer(); \t\tGdx.input.setInputProcessor(this); \t}  \t@Override public void render () { \t\tGdx.gl.glClear(GL20.GL_COLOR_BUFFER_BIT); \t\tshapes.setProjectionMatrix(camera.combined); \t\tshapes.begin(ShapeRenderer.ShapeType.Filled); \t\tshapes.circle(tp.x, tp.y, 0.25f, 16); \t\tshapes.end(); \t}  \tVector3 tp = new Vector3(); \tboolean dragging; \t@Override public boolean mouseMoved (int screenX, int screenY) { \t\t// we can also handle mouse movement without anything pressed //\t\tcamera.unproject(tp.set(screenX, screenY, 0)); \t\treturn false; \t}  \t@Override public boolean touchDown (int screenX, int screenY, int pointer, int button) { \t\t// ignore if its not left mouse button or first touch pointer \t\tif (button != Input.Buttons.LEFT || pointer &gt; 0) return false; \t\tcamera.unproject(tp.set(screenX, screenY, 0)); \t\tdragging = true; \t\treturn true; \t}  \t@Override public boolean touchDragged (int screenX, int screenY, int pointer) { \t\tif (!dragging) return false; \t\tcamera.unproject(tp.set(screenX, screenY, 0)); \t\treturn true; \t}  \t@Override public boolean touchUp (int screenX, int screenY, int pointer, int button) { \t\tif (button != Input.Buttons.LEFT || pointer &gt; 0) return false; \t\tcamera.unproject(tp.set(screenX, screenY, 0)); \t\tdragging = false; \t\treturn true; \t}  \t@Override public void resize (int width, int height) { \t\t// viewport must be updated for it to work properly \t\tviewport.update(width, height, true); \t}  \t@Override public void dispose () { \t\t// disposable stuff must be disposed \t\tshapes.dispose(); \t}  \t@Override public boolean keyDown (int keycode) { \t\treturn false; \t}  \t@Override public boolean keyUp (int keycode) { \t\treturn false; \t}  \t@Override public boolean keyTyped (char character) { \t\treturn false; \t}  \t@Override public boolean scrolled (int amount) { \t\treturn false; \t}  \tpublic static void main (String[] arg) { \t\tLwjglApplicationConfiguration config = new LwjglApplicationConfiguration(); \t\tconfig.width = 1280; \t\tconfig.height = 720; \t\tconfig.useHDPI = true; \t\tnew LwjglApplication(new SimplerTouchTest(), config); \t} }                  Prev       Next          ",
        
        "url": "/wiki/input/mouse-touch-and-keyboard" },{
        "title": "Networking",
        "excerpt":
        
        "libGDX includes some classes for cross-platform network operations. These classes are more commonly known as Gdx.net (source)   Features      Cross-platform HTTP requests   Multi-platform TCP client and server socket support (excludes GWT) with configurable settings   Optimized  TCP client and server settings aiming for low-latency   Cross-platform browser access. (ex: You can create a link to your website in game and it will open the browser on all platforms.)   Implementation  Class Explanation:     Net.java is an interface used for the cross-platform networking. This is where you can get the objects needed to communicate with the network.   Socket.java is an interface that provides you with the remote socket address, connection state, and a java.io.InputStream and java.io.OutputStream to work with the socket.   SocketHints.java is a class used to configure TCP client sockets   ServerSocket.java is an interface used to create TCP server sockets. It provides the standard accept() method to get a TCP client that connected.   ServerSocketHints.java is a class used to configure TCP server sockets.   HttpStatus.java is a class used to give an easy way to see what the status code returned is.   HttpParameterUtils.java is a class used to provide utility methods for HTTP requests.   HttpRequestBuilder is a class to help with creating HttpRequests.   To create a TCP client socket use this little piece of code:  Socket socket = Gdx.net.newClientSocket(Protocol protocol, String host, int port, SocketHints hints);   To create a TCP server socket use this:  ServerSocket server = Gdx.net.newServerSocket(Protocol protocol, int port, ServerSocketHints hints);   To send an HTTP Request use this:  HttpRequestBuilder requestBuilder = new HttpRequestBuilder(); HttpRequest httpRequest = requestBuilder.newRequest().method(HttpMethods.GET).url(\"http://www.google.de\").build(); Gdx.net.sendHttpRequest(httpRequest, httpResponseListener);   To send a GET HTTP Request with arguments use this:  HttpRequestBuilder requestBuilder = new HttpRequestBuilder(); HttpRequest httpRequest = requestBuilder.newRequest().method(HttpMethods.GET).url(\"http://www.google.de\").content(\"q=libgdx&amp;example=example\").build(); Gdx.net.sendHttpRequest(httpRequest, httpResponseListener);  To open the system browser use this:  Gdx.net.openURI(String URI)   Receiving Response   There are different technique to flexibly receive response back from HTTP request as shown above. Example in Kotlin as follows.           Via class which implemented HttpResponseListener       For this, you have to create a class and implements HTTPResponseListener, then supply a method parameter with instance of such class.       class MyReceiverOfResult : HttpResponseListener {    override fun cancelled() {        // do something when request gets cancelled    }     override fun failed(t: Throwable?) {        // do something when it fails    }     override fun handleHttpResponse(httpResponse: Net.HttpResponse) {        // do something when gets result back    } }  ...  // assume you hold instance of such class as variable namely `receiver`. // then you just pass it to `sendHttpRequest()` method Gdx.net.sendHttpRequest(req, receiver)                Anonymous object      Gdx.net.sendHttpRequest(req, object: Net.HttpResponseListener {       override fun cancelled() {           // do something when request gets cancelled       }        override fun failed(t: Throwable?) {           // do something when it fails       }        override fun handleHttpResponse(httpResponse: Net.HttpResponse) {           // do something when gets result back       }    })          Depends on your use case and which way you find it more flexible to do so.       Notes  There are various notes needed when working with networking on different platforms.      TCP client and server sockets do not work on GWT. This is due to java.net not being supported on GWT and there is not a viable alternative at this point other than websockets.   Opening the browser is not supported on the headless backend, Android Daydreams, or Android Live Wallpapers. This is due to the limitations with the implementation and/or platform.   On Android: You must have the following permission declared in the AndroidManifest.xml file to access the network: &lt;uses-permission android:name=\"android.permission.INTERNET\" /&gt;    On Android: You cannot access the network on the main thread without disabling strict mode. This is done to prevent network operations from hanging the main thread. See here   When targeting mobile devices: Be careful about how you implement networking. The wireless radios themselves are a big power drain when on. Also be careful about data limits that could be imposed on a 1G/2G/3G/4G LTE network. libGDX has configuration optimizations done to allow low-latency, but still have the benefits of TCP.   Supported networking configurations vary between backend and java implementation.   Battery drain is more common when data is being sent and received due to the power needed by the radios.   Be sure to set the Content-Type header for POST requests. Not all backends default to the same value (due to differences in the underlying implementations). The most common value for this header is application/x-www-form-urlencoded; however, depending on the type of data you are sending, you may need a different value (such as application/xml or application/json).   See Also   Great articles about mobile data battery efficiency here   Source code for Gdx.net classes here  ",
        
        "url": "/wiki/networking" },{
        "title": "Ninepatches",
        "excerpt":
        
        "This article introduce NinePatch images, how they can be created and how they are used in a libGDX context.   Before you start   This guide was intended for the old scene2d and the skinpacker (the versions before 0.9.6). If you are running of the nightlies, this guide can give some hints, but won’t work as a step-by-step guide. The main differences are that skinpacker and texturepacker has been unified in the texturepacker2 and that skins gets loaded in a different way.   Introduction   A NinePatch image is an image with defined “stretchable” areas. With this property one can create images that repeats either to very small regions, or scale to very big regions. Since the areas are pre-defined, the image won’t look stretched (given that it has been created with scaling in mind). The corresponding NinePatch class in libGDX is located here (code).   A NinePatch is used in various of the Scene2d components of Libgdx, including:     Buttons   ScrollPanes   Textfields   In libGDX there are several ways to load resources, which also counts for NinePatches. When manually creating and instantiating a ninepatch, we are specifying the patch regions in code. When loading a NinePatch using Skins (and the SkinPacker to create them) the process is a bit different. This will also be covered on this page.   Creating and instantiating a NinePatch manually   This is a short introduction to instantiate a NinePatch image manually in code. It’s basically to create an image, figure out what regions to stretch and note down the pixels of those regions.   Create your scalable image   Keep in mind that some area of your image needs to hold the content (text, other images etc), and can therefore not contain any “special features” (since this area will be scaled). In this case I’ll create a “button”. My art skills are non existent, so please bear with me on this example.      As one may notice, this “button” is fairly round. One of the wonders with NinePatch is that it will wrap around the content we give it, thus expand in the horizontal direction when we feed it some text. The corners of this button are plain translucent pixels. Notice that we do not define the stretchable areas on the image, since we will do this in code instead.   Instantiate the NinePatch   The simplest form of instantiating a new NinePatch:  NinePatch patch = new NinePatch(new Texture(Gdx.files.internal(\"knob.png\")), 12, 12, 12, 12);  The four integer arguments are specifying what regions (in pixels) we will allow the image to stretch in.   The true power of NinePatch reveals itself when applied to Scene2D elements. Below is an example of instantiation a button, with our newly created image.  // Create a new TextButtonStyle TextButtonStyle style = new TextButtonStyle(patch, patch, patch, 0, 0, 0, 0, new BitmapFont(), new Color(0.3f, 0.2f, 0.8f, 1f), new Color(0, 0, 0, 1f), new Color(0, 0, 0, 1f)); // Instantiate the Button itself. TextButton button = new TextButton(\"hello world\", style);   The result of adding this TextButton to a stage is illustrated below:      Our round image has now scaled with the content length (the text). The button use the standard BitmapFont and some awful colours.   Limitations when instantiating in code   Limitations with instantiating a NinePatch directly (using Libgdx) is that your fixed regions all will be the same square. Below I have attached an image illustrating what the four integer arguments actually define in the NinePatch. The gray area not overlapped by cyan is the scaleable area.      Creating and instantiating a NinePatch using SkinPacker   Note: For the SkinPacker to properly recognize/parse NinePatch images the image needs to be postfixed with .9.png (if .png is your file ending).   The NinePatch image needs to have some special properties within the image itself, to be able to act as a NinePatch. These properties are added by padding the image with a 1 pixel border. The steps to create a NinePatch are described below.   Define stretchable areas   Now we need to alter the image, and add black borders where we want to allow the image to stretch. This can be done in any image editor. But there are editors to ease the process.   GitHub AndroidAssetStudio generator  This is my personal favorite. No need for anything, just go to the site, specify your image, it automatically guestimates it (and gets it right in my experience) and hit the download button, it’ll even put it in a nice zip for you for android (if you’re not on android you can ignore it and go for the smallest one, since it is a ninepatch and scaleable as high as you want) https://romannurik.github.io/AndroidAssetStudio/nine-patches.html   WebLaF ninepatch-editor  A more up-to-date and functional tool than the below Android SDK one, it was actually created because of the shortcomings of the draw9patch tool, can be found in the weblaf project. Scroll down and find the ninepatch-editor tool standalone jar release available for download.   Android SDK draw9patch  The Android SDK contains an excellent tool for exactly this purpose, and is located in _android-sdk/tools/draw9patch_. This tool provides a pre-view of the scaled image. Below is just the image loaded into the draw9patch tool. Notice the “pre-view” to the left, and how the image does not scale well at all.      In the following picture, I have defined what area the content will be placed in (in other words, what will be scaled), and what areas I don’t want to scale. Again, this is achieved by padding with a 1 pixel border in the image. You see that the tool previews the content (pink area), and that the previews scales much better (in the right side of the screenshot).      Now save the image as image.9.png. This can’t be underlined enough, since libGDX won’t recognize the file as a NinePatch otherwise. Below is the finished image in all its NinePatch glamour, and ready to be used in code.      Defining a NinePatch programmatically   See this NinePatch constructor.      Pack the image using SkinPacker   This step should be covered in other areas of this Wiki (preferable a node about the SkinPacker). Since the image is postfixed with .9.png its areas will be analyzed by looking at the 1 pixel padded outer region (as we defined in the previous step).   When this is the only picture from the export-folder run in the SkinPacker the result will be:   \"resources\": {         \"com.badlogic.gdx.graphics.g2d.NinePatch\": {                 \"knob\": [                         { \"x\": 2, \"y\": 2, \"width\": 13, \"height\": 11 },                         { \"x\": 15, \"y\": 2, \"width\": 7, \"height\": 11 },                         { \"x\": 22, \"y\": 2, \"width\": 12, \"height\": 11 },                         { \"x\": 2, \"y\": 13, \"width\": 13, \"height\": 9 },                         { \"x\": 15, \"y\": 13, \"width\": 7, \"height\": 9 },                         { \"x\": 22, \"y\": 13, \"width\": 12, \"height\": 9 },                         { \"x\": 2, \"y\": 22, \"width\": 13, \"height\": 12 },                         { \"x\": 15, \"y\": 22, \"width\": 7, \"height\": 12 },                         { \"x\": 22, \"y\": 22, \"width\": 12, \"height\": 12 }                 ]         } }   We see that the packer actually defined nine patches (somebody should be mind blown by now!). One huge advantage with this is that we are no longer constrained to the 1 square for each region (as opposed to instantiate Ninepatches manually). We can now define more fine-grained nine patches. In addition to this, its much easier to just alter an image and run it through the packer, and it will define regions.  ",
        
        "url": "/wiki/graphics/2d/ninepatches" },{
        "title": "On screen keyboard",
        "excerpt":
        
        "Most Android devices and all iOS devices do not possess a hardware keyboard. Instead, a soft- or on-screen keyboard can be presented to the user. To bring up the on-screen keyboard we can call this method:   Gdx.input.setOnscreenKeyboardVisible(true);   Once visible, any key presses will be reported as events to the application. Additionally, polling can be used to check for a specific key’s state.   Note that there is currently a bug in the on-screen keyboard implementation when landscape mode is used on Android. The default Android on-screen keyboard can be switched for a custom keyboard, and many handset manufacturers like HTC make use of this. Sadly, their keyboard implementations tend to be buggy which leads to problems described in this issue. If you’re using a buggy custom keyboard or your manufacturer supplied a buggy custom keyboard, Google’s keyboard works correctly with libGDX.   On-screen keyboard functionality is only available the Android and iOS platforms.   Prev  ",
        
        "url": "/wiki/input/on-screen-keyboard" },{
        "title": "OpenGL (ES) Support",
        "excerpt":
        
        "What is what (GL, GL ES, GLSL)?   Whenever libGDX is talking about GL20 or GL30, it is in fact referring to GL ES 2.0 and GL ES 3.0. OpenGL ES can be seen as a subset of OpenGL and is designed for embedded systems (smartphones in particular).   By default, version 2.0 of OpenGL ES is used, but libGDX can be configured to use 3.0 as well. See for example:   Lwjgl3ApplicationConfiguration cfg = new Lwjgl3ApplicationConfiguration(); // ... cfg.setOpenGLEmulation(GLEmulation.GL30, 3, 2);   And what about GLSL (ES)?  GLSL (or GLSL ES for mobile and web) is the language used for shaders. Its versioning is somewhat confusing:      GLSL - click to expand                                    Open GL Version           GLSL Version                                           2.0           110                             2.1           120                             3.0           130                             3.1           140                             3.2           150                             3.3           330                             4.0           400                             4.1           410                             4.2           420                             4.3           430                             4.4           440                             4.5           450                             4.6           460                           For some advice on porting shaders from version 120 to 330+, see here.           GLSL ES - click to expand                                    OpenGL ES Version           GLSL EL Version           Based on GLSL Version (OpenGL)                                           2.0           100           120 (2.1)                             3.0           300 es           330 (3.3)                             Platform specificities  Desktop (Windows, Mac, Linux)  On Desktop, libGDX is mapping all its graphics calls to OpenGL.   GL ES 2.0 is roughly based on Open GL 2.0, however, there are some incompatibilities that weren’t resolved until Open GL 4.1. To mimic GL ES 2.0, libGDX does not request any specific OpenGL version, so the driver will be more forgiving.   GL ES 3.0 is the successor of OpenGL ES 2.0. On desktop, OpenGL 4.3 provides full compatibility with OpenGL ES 3.0. For mimicking GL ES 3.0 on desktop, one can specify the exact OpenGL version, that should be used. Please note that MacOS only supports the OpenGL 3.2 core profile.   Android  On Android Open GL ES 2.0 and 3.0 can be used. To prevent your application from being shown to unsupported devices in the Play Store, add one of the following lines to your Android Manifest:     OpenGL ES 2: &lt;uses-feature android:glEsVersion=\"0x00020000\" android:required=\"true\" /&gt;   OpenGL ES 3: &lt;uses-feature android:glEsVersion=\"0x00030000\" android:required=\"true\" /&gt;   iOS  Please note that support for OpenGL ES 3.0 is experimental on iOS.   Web  On Web, the graphic stuff is handled by WebGL. Web only supports GL ES 2.0.   Precision modifiers  OpenGL ES 2.0 requires the specification of precision modifiers for attributes, uniforms and locals. Desktop OpenGL does not support this. You will have to guard against that in your fragment shader with something similar to this code snippet:   #ifdef GL_ES #define LOW lowp #define MED mediump #define HIGH highp precision mediump float; #else #define MED #define LOW #define HIGH #endif   This will define the LOWP, MED, and HIGH macros to equivalent OpenGL ES precision modifiers and sets the default precision for float to medium. This will only happen on platforms actually running OpenGL ES, on the desktop, the macros are defined to be empty.   OpenGL ES 2.0 Documentation     OpenGL ES 2.0 Reference Pages   OpenGL ES 2.0 Reference Card   OpenGL ES 2.0 Specification   OpenGL ES Programming Guide for iOS: the core concepts are valid for other platforms as well   OpenGL ES 2.0 Pipeline Structure  ",
        
        "url": "/wiki/graphics/opengl-es-support" },{
        "title": "Orthographic camera",
        "excerpt":
        
        "This page presents the OrthographicCamera class and usage. The orthographic camera is to be used in 2D environments only as it implements a parallel (orthographic) projection and there will be no scale factor for the final image regardless where the objects are placed in the world.   Code for a simple example of a Camera zooming and moving is available on LibGDX.info here   Description   The Camera class operates as a very simple real world camera. It is possible to     move and rotate the camera around,   zoom in and out,   change the viewport,   project/unproject points to and from window coordinate/ world space   Using the camera is the easy way to move around a game world without having to manually operate on the matrices. All the projection and view matrix operations are hidden in the implementation.   The position field referes to the position of the center of the camera. The camera extends the selected viewport of the world so this matches the screen size of the device.   The following little app demonstrates the use of a simple OrthographicCamera to move around a flat world.   import com.badlogic.gdx.ApplicationListener; import com.badlogic.gdx.Gdx; import com.badlogic.gdx.Input; import com.badlogic.gdx.backends.lwjgl.LwjglApplication; import com.badlogic.gdx.graphics.GL20; import com.badlogic.gdx.graphics.OrthographicCamera; import com.badlogic.gdx.graphics.Texture; import com.badlogic.gdx.graphics.g2d.Sprite; import com.badlogic.gdx.graphics.g2d.SpriteBatch; import com.badlogic.gdx.math.MathUtils;  public class OrthographicCameraExample implements ApplicationListener {  \tstatic final int WORLD_WIDTH = 100; \tstatic final int WORLD_HEIGHT = 100;  \tprivate OrthographicCamera cam; \tprivate SpriteBatch batch;  \tprivate Sprite mapSprite; \tprivate float rotationSpeed;  \t@Override \tpublic void create() { \t\trotationSpeed = 0.5f;  \t\tmapSprite = new Sprite(new Texture(Gdx.files.internal(\"sc_map.png\"))); \t\tmapSprite.setPosition(0, 0); \t\tmapSprite.setSize(WORLD_WIDTH, WORLD_HEIGHT);  \t\tfloat w = Gdx.graphics.getWidth(); \t\tfloat h = Gdx.graphics.getHeight();  \t\t// Constructs a new OrthographicCamera, using the given viewport width and height \t\t// Height is multiplied by aspect ratio. \t\tcam = new OrthographicCamera(30, 30 * (h / w));  \t\tcam.position.set(cam.viewportWidth / 2f, cam.viewportHeight / 2f, 0); \t\tcam.update();  \t\tbatch = new SpriteBatch(); \t}  \t@Override \tpublic void render() { \t\thandleInput(); \t\tcam.update(); \t\tbatch.setProjectionMatrix(cam.combined);  \t\tGdx.gl.glClear(GL20.GL_COLOR_BUFFER_BIT);  \t\tbatch.begin(); \t\tmapSprite.draw(batch); \t\tbatch.end(); \t}  \tprivate void handleInput() { \t\tif (Gdx.input.isKeyPressed(Input.Keys.A)) { \t\t\tcam.zoom += 0.02; \t\t} \t\tif (Gdx.input.isKeyPressed(Input.Keys.Q)) { \t\t\tcam.zoom -= 0.02; \t\t} \t\tif (Gdx.input.isKeyPressed(Input.Keys.LEFT)) { \t\t\tcam.translate(-3, 0, 0); \t\t} \t\tif (Gdx.input.isKeyPressed(Input.Keys.RIGHT)) { \t\t\tcam.translate(3, 0, 0); \t\t} \t\tif (Gdx.input.isKeyPressed(Input.Keys.DOWN)) { \t\t\tcam.translate(0, -3, 0); \t\t} \t\tif (Gdx.input.isKeyPressed(Input.Keys.UP)) { \t\t\tcam.translate(0, 3, 0); \t\t} \t\tif (Gdx.input.isKeyPressed(Input.Keys.W)) { \t\t\tcam.rotate(-rotationSpeed, 0, 0, 1); \t\t} \t\tif (Gdx.input.isKeyPressed(Input.Keys.E)) { \t\t\tcam.rotate(rotationSpeed, 0, 0, 1); \t\t}  \t\tcam.zoom = MathUtils.clamp(cam.zoom, 0.1f, 100/cam.viewportWidth);  \t\tfloat effectiveViewportWidth = cam.viewportWidth * cam.zoom; \t\tfloat effectiveViewportHeight = cam.viewportHeight * cam.zoom;  \t\tcam.position.x = MathUtils.clamp(cam.position.x, effectiveViewportWidth / 2f, 100 - effectiveViewportWidth / 2f); \t\tcam.position.y = MathUtils.clamp(cam.position.y, effectiveViewportHeight / 2f, 100 - effectiveViewportHeight / 2f); \t}  \t@Override \tpublic void resize(int width, int height) { \t\tcam.viewportWidth = 30f; \t\tcam.viewportHeight = 30f * height/width; \t\tcam.update(); \t}  \t@Override \tpublic void resume() { \t}  \t@Override \tpublic void dispose() { \t\tmapSprite.getTexture().dispose(); \t\tbatch.dispose(); \t}  \t@Override \tpublic void pause() { \t}  \tpublic static void main(String[] args) { \t\tnew LwjglApplication(new OrthographicCameraExample()); \t} }    The above class is the libGDX application that will use the orthographic camera to move around the world. Our world size is in arbitrary units that we can define however we want to. In this specific case, our world is 100x100 units.       static final int WORLD_WIDTH = 100;     static final int WORLD_HEIGHT = 100;   Many people make the mistake of thinking in pixels when it comes to their world, and this is something that you should avoid doing. It leads to unnecessary multiplying and dividing by constants, having weird “Pixel per unit” ratios dotted around your code, poor understanding of the pipeline and it confuses you!  There are many other problems, which can be easily avoided when you stop “thinking” in pixels.   What are these units though? What do they mean? How will I know what size to make objects? How many units are displayed on the screen?  We will get to that shortly! Stick tight.   \tprivate OrthographicCamera cam;  #1 \tprivate SpriteBatch batch;       #2  \tprivate Sprite mapSprite;        #3 \tprivate float rotationSpeed;     #4   #1 - The OrthographicCamera instance we will control to look at the world.   #2 - The SpriteBatch instance we will use to render our world   #3 - A Sprite that we will use to draw our world map   #4 - Rotation speed for rotating our camera     @Override public void create() { \trotationSpeed = 0.5f;                                                    #1  \tmapSprite = new Sprite(new Texture(Gdx.files.internal(\"sc_map.png\")));   #2 \tmapSprite.setPosition(0, 0);                                             #3 \tmapSprite.setSize(WORLD_WIDTH, WORLD_HEIGHT);                            #4  \tfloat w = Gdx.graphics.getWidth();                                       #5 \tfloat h = Gdx.graphics.getHeight();                                      #6 \tcam = new OrthographicCamera(30, 30 * (h / w));                          #7 \tcam.position.set(cam.viewportWidth / 2f, cam.viewportHeight / 2f, 0);    #8 \tcam.update();                                                            #9  \tbatch = new SpriteBatch();                                               #10 }   The create method is called when we create a new instance of our ApplicationListener, and it is where we initialize our variables   #1 - Sets the current rotation speed to 0.5 degree.   #2 - Creates our Sprite, from a new Texture that uses the file: sc_map.png Download the file here, rename it to sc_map.png and place it in the assets/ directory.   #3 - We set the position of our mapSprite to 0,0.(This isn’t strictly required as the Sprite has default x,y of 0,0 anyway.)   #4 - We set the size of mapSprite, with width of WORLD_WIDTH and height of WORLD_HEIGHT. So our sprite now has dimensions of 100x100, or the size of our world.   #5 - We create a local variable that has the value of the current width of our application display. (This is in pixels)   #6 - We create a local variable that has the value of the current height of our application display. (This is in pixels)   #7 - We Create the OrthographicCamera. The 2 parameters specify the width and height of the viewport that will be created. These values determine how much of our world we can see in each axis.   In our example, we use 30 for our viewport width, and 30 * (h / w) for our viewport height. The width is trivial, we can see 30 units in the X axis. For the viewport height we use 30 multiplied by the aspect ratio of our display. This is so we see objects we draw in correct proportions. Imagine if we ignored the aspect ratio, and just went with viewport width and height of 30,  unless we have a square display, which we most likely don’t, when we render an object that has dimensions of 30x30 for example, it would show as a squished rectangle with the same shape as our display. How can an object that is 30x30 not be a square?  This is because we assumed 30 viewport width and 30 viewport height, which doesn’t match the aspect ratio of our device.   Some examples -  If we created our camera with viewport width of 100 and viewport height of 100 ('new OrthographicCamera(100, 100)') and centered it correctly, we would be able to see the 'whole' map, our 'whole' world at once.  If we created our camera with viewport width of 100 and viewport height of 50 ('new OrthographicCamera(100, 50)') we would be able to see 'half' of the map at any given time  If we created our camera with viewport width of 50 and viewport height of 50 ('new OrthographicCamera(50, 50)') we would be able to see a 'quarter' of the map at any given time   #8 - Setting the camera’s initial position to the bottom left of the map. But… the camera’s position is in the center of the camera.   So we need to offset the camera’s position by +half viewport width and +half viewport height so that the bottom left of our camera is actually at 0,0.   #9 - Update our camera! This step is vital to call whenever we have manipulated our camera as it updates all the matrices under the hood.   #10 - Create our SpriteBatch instance.   And we are setup! So let’s get rendering and manipulating the camera.     @Override public void render() { \thandleInput();                             #1 \tcam.update();                              #2                              \tbatch.setProjectionMatrix(cam.combined);   #3  \tGdx.gl.glClear(GL20.GL_COLOR_BUFFER_BIT);  #4  \tbatch.begin();                             #5 \tmapSprite.draw(batch);                     #6 \tbatch.end();                               #7 }   #1 - Controls the camera by updating its position, zooming, rotation based on different keys being pressed.   #2 - Updates our OrthographicCamera, we have just manipulated it with handleInput() method, so we must remember to call the update() method.   #3 - Updates our SpriteBatch instance with our Camera’s view and projection matrices.   #4 - Clears the screen (actually the colour buffer).   #5 - Begin our SpriteBatch   #6 - Draw our mapSprite!   #7 - End our SpriteBatch     Let’s take a deeper look at controlling our camera, which is all handled in our handleInput() method.   \tprivate void handleInput() { \t\tif (Gdx.input.isKeyPressed(Input.Keys.A)) { \t\t\tcam.zoom += 0.02; \t\t\t//If the A Key is pressed, add 0.02 to the Camera's Zoom \t\t} \t\tif (Gdx.input.isKeyPressed(Input.Keys.Q)) { \t\t\tcam.zoom -= 0.02; \t\t\t//If the Q Key is pressed, subtract 0.02 from the Camera's Zoom \t\t} \t\tif (Gdx.input.isKeyPressed(Input.Keys.LEFT)) { \t\t\tcam.translate(-3, 0, 0); \t\t\t//If the LEFT Key is pressed, translate the camera -3 units in the X-Axis \t\t} \t\tif (Gdx.input.isKeyPressed(Input.Keys.RIGHT)) { \t\t\tcam.translate(3, 0, 0); \t\t\t//If the RIGHT Key is pressed, translate the camera 3 units in the X-Axis \t\t} \t\tif (Gdx.input.isKeyPressed(Input.Keys.DOWN)) { \t\t\tcam.translate(0, -3, 0); \t\t\t//If the DOWN Key is pressed, translate the camera -3 units in the Y-Axis \t\t} \t\tif (Gdx.input.isKeyPressed(Input.Keys.UP)) { \t\t\tcam.translate(0, 3, 0); \t\t\t//If the UP Key is pressed, translate the camera 3 units in the Y-Axis \t\t} \t\tif (Gdx.input.isKeyPressed(Input.Keys.W)) { \t\t\tcam.rotate(-rotationSpeed, 0, 0, 1); \t\t\t//If the W Key is pressed, rotate the camera by -rotationSpeed around the Z-Axis \t\t} \t\tif (Gdx.input.isKeyPressed(Input.Keys.E)) { \t\t\tcam.rotate(rotationSpeed, 0, 0, 1); \t\t\t//If the E Key is pressed, rotate the camera by rotationSpeed around the Z-Axis \t\t}  \t\tcam.zoom = MathUtils.clamp(cam.zoom, 0.1f, 100/cam.viewportWidth);  \t\tfloat effectiveViewportWidth = cam.viewportWidth * cam.zoom; \t\tfloat effectiveViewportHeight = cam.viewportHeight * cam.zoom;  \t\tcam.position.x = MathUtils.clamp(cam.position.x, effectiveViewportWidth / 2f, 100 - effectiveViewportWidth / 2f); \t\tcam.position.y = MathUtils.clamp(cam.position.y, effectiveViewportHeight / 2f, 100 - effectiveViewportHeight / 2f); \t}   So we can see that this method polls Keys, if a certain key is pressed, we do something to the camera.   The last 5 lines are responsible for keeping the camera within the bounds of our world.   We need to make sure the camera’s zoom does not grow or shrink to values that would invert our world, or show too much of our world. To do this, we can calculate the effectiveViewportWidth and effectiveViewportHeight, which are just the viewportWidth/height * zoom (this gives us what we can see in the world given the current zoom). We can then clamp the value of the camera’s zoom to values we require. 0.1f to prevent being too zoomed in. 100/cam.viewportWidth to prevent us being able to see more than the world’s entire width.   The last two lines are responsible for making sure we can’t translate out of the world boundaries. &lt; 0, or more than 100 in either Axis.     What to do when the application changes size?  This is when you implement different strategies for handling devices with different resolutions/aspect ratios. I will include a few basic strategies to give you the basic idea.   If you want a slightly higher level method of handling this, you should use viewports -&gt; Wiki Article on Viewports   The following resize strategy will ensure that you will always see 30 units in the x axis no matter what pixel-width your device has.  \t@Override \tpublic void resize(int width, int height) { \t\tcam.viewportWidth = 30f;                 // Viewport of 30 units! \t\tcam.viewportHeight = 30f * height/width; // Lets keep things in proportion. \t\tcam.update(); \t}    The following resize strategy will show less/more of the world depending on the resolution  \t@Override \tpublic void resize(int width, int height) { \t\tcam.viewportWidth = width/32f;  //We will see width/32f units! \t\tcam.viewportHeight = cam.viewportWidth * height/width; \t\tcam.update(); \t}      The main application to bootstrap the listener is a simple LWJGL application.  \tpublic static void main(String[] args) { \t\tnew LwjglApplication(new OrthographicCameraExample()); \t}     The result is the following application:      Most of the time, one should not need to access the internals of a camera as the most common use-cases are covered by the following methods:                  Method       Description                       lookAt(float x, float y, float z)       Recalculates the direction of the camera to look at the point defined by the coordinates on all axes. - The z axis is ignored for 2D                 translate(float x, float y, float z)       Moves the camera by the given amount on each axis. - Note that z is ignored for the OrthographicCamera                 rotate(float angle, float axisX, float axisY, float axisZ)       Rotates the direction and up vector of this camera by the given angle around the given axis. The direction and up vector will not be orthogonalized. The angle is persisted so the camera will be rotated by angle relative to its previous rotation.                 update()       Recalculates the projection and view matrix of the camera and the frustum planes          ",
        
        "url": "/wiki/graphics/2d/orthographic-camera" },{
        "title": "Overlap2D survival guide for #libGDXJAM",
        "excerpt":
        
        "So you are planning to join #libGDXJAM and you are considering (or already decided upon) choosing Overlap2D editor as your primary weapon of choice. If so, read on!   In this article I (as the creator of Overlap2D) will try to provide as much information as possible on how to be friends with O2D and use it wisely!   Getting Started   Prerequisites      Make sure you have JDK 1.8 (It’s required for Overlap2D to run, and you always can have both 1.7 and 1.8)   Download the Editor itself from our website   Make yourself a nice cozy folder to work in   Get some test assets (png sprites) later you can substitute that with your own.   Learn more about Ashley (our runtime uses that architecture)   Working with Overlap2d   Working with Overlap2D consists of several parts:     Creating your main libGDX project using setup tool   Creating empty project with overlap2d   Making a test scene in the editor (import some images, put some stuff on the scene)   Export that data to your assets folder in your libGDX project   Use overlap2d-libgdx-runtime to render your scene in your game.   Now I can write a lot and bore you to death. Or, you can just watch me do all this steps in detail: https://www.youtube.com/watch?v=bhvHm2sM0qo   Important things to know:     When using setup app, click on Third Party Extensions and select overlap2d from the list, also make sure to have freetype and box2d checkboxes selected.   For the JAM better use latest runtime, which is 0.1.2-SNAPSHOT (make sure to set that in build.gralde file, because it’s not default value)   When creating your o2d project, don’t think about multiple resolutions and huge textures. This is just a JAM. I would go with a small camera-world size (say 800x480 or similar), and pixel per world unit somewhere between 1-4.   What game are you making?   Let’s figure out what you plan to make, to help you most.   Side scroller or runner   So you are making a side scroller or a runner, meaning you have this character that walks or runs on this background from left to right. That is perfect candidate for Overlap2D. You can make several layers, like background, foreground. Combine things like trees and mountains into groups, position things behind. Then have your main character (probably an animation) be in front. Give it a unique identifier, And so on. There is even tutorial series on how to do it in our documentation   Tiled view from above   Could be tower defense or strategy or dungeon kind of thing. In this case you want your world to have tiles. Decide on images you want to use as tiles (make sure they are same size and square) and put them lined up somewhere on the scene. give each of them tag “tile”. In upper panel find grid size, and set it to your tile size (this way things will snap to grid) Now you can just copy paste your initial tiles, and put them where they fit. You can then give them additional tags like “wall” or “turret” to mention their behaviour or if player can or cannot walk through them. Of course later you need to write a System in your code, to use this tags accordingly. There is a sample “Tower defense” project I made you can take a look at as an example  here   Dynamic world generation   So you don’t want to have entire map made up, because it has to be random each time. For this you can make a predefined world “chunks” conevrt them into composite items, put them into library, and then randomly load from library and stitch together.   I need it for UI   Okay so your game itself is kind of different and does not need a predefined scene data. Maybe you generate all stuff with code, and that’s fine. But you want to make your menus, dialogs and buttons with overlap2d? That’s cool too! You can make buttons in overlap2d easily. just chose button assets, and right click convert to button. This will create a composite with some predefined layers. Make sure to go in, and put things in correct places (pressed layer and so on) You can test your button directly in the editor. To make some dialogs use 9patch images for background (actually same for buttons) If you do not have a 9patch images, you can make one directly from the editor. Just right click on regular image, and choose “Convert to 9patch” this will open a dialog to set it up. You can later convert all the buttons and texts to composite items, and put that dialog into library. Then later form your code create a CompositeActor from that library data. (Get VO object using resource manager, and then send it to CompositeActor constructor)   Tips and Tricks      If you need to navigate through nested items, best way to do it, is use helper class ItemWrapper. Just wrap an entity into it, and then use methods like getChild to navigate.   If you have many objects that need to behave in same way (for example moving clouds) best thing to do is, give all clouds on scene a same tag. Then from use sceneLoader.addComponentsByTagName method to add a cloud component to all clouds. Then you can create a System that manages that family of components to have one class that does your “cloud” logic.   If you have one unique object that needs logic (like for example player) just give it a unique id, and then make an IScript for it. Overlap2D has a System for iScripts, and you can attach IScript to an item using ItemWrapper class. IScript is a custom class you make that implements specific interface. To make sure it has methods init, act, and dispose.   You can render your UI using entity enine as well, by just putting your UI on the scene. But if you need your camera to zoom out and move, then maybe it’s a better idea to create a separate stage for your UI, and use CompositeActor’s for it.   When programming systems, to monitor user inputs, use Gdx.input, it has all you may ever need. No need for ye-olde event listeners.   So you have this composite inside a composite inside a composite, and then there is different physics shapes and bodies inside, and they all behave wrong - yeah… don’t do that. Keep it simple, and it’ll work :-P  ",
        
        "url": "/wiki/misc/overlap2d-survival-guide-for-libgdxjam" },{
        "title": "Overlap2D",
        "excerpt":
        
        "The original Overlap2D     Overlap2D is a 2D level and UI editor with an engine agnostic philosophy for game development. Made to separate coding from content, it enables developer to create rich content using images, animations, particle effects, light system, physics and complex grouped items.   The project is hosted on github, but seems to be abandoned.   HyperLap2D?     HyperLap2D strives to be a modern replacement for Overlap2D. It is made to be a powerful, platform-independent, visual editor for complex 2D worlds and scenes.   VisEditor (discontinued)  VisRuntime is no longer maintained. It was an open source 2D game level editor with a libGDX runtime.   You can find more information at https://kotcrab.com/blog/2017/03/26/visui-130-released-viseditor-deprecated/.  ",
        
        "url": "/wiki/tools/overlap2d" },{
        "title": "Packing atlases at runtime",
        "excerpt":
        
        "   Texture-packer   PixmapPacker (code)   The original blog post detailing the use of PixmapPacker  ",
        
        "url": "/wiki/graphics/2d/packing-atlases-at-runtime" },{
        "title": "Packing atlases offline",
        "excerpt":
        
        "Use the Texture Packer utility to packages offline (as opposed to during runtime of your libGDX program)   Texture Packer  ",
        
        "url": "/wiki/graphics/2d/packing-atlases-offline" },{
        "title": "Path interface and Splines",
        "excerpt":
        
        "Introduction   The Path interface (code) have implementations that allows you to traverse smoothly through a set of defined points (in some cases, tangents too).   Paths can be defined to be bi-dimensional or tri-dimensional, because it is a template that takes a derived of the Vector class, thus you can either use it with a set of Vector2’s or Vector3’s.   Mathematically, it is defined as F(t) where t is [0, 1], where 0 is the start of the path, and 1 is the end of the path.   Types   As of v0.9.9, we have 3 implementations of the Path interface (the splines), these are:     Bezier (code)   BSpline (code)   CatmullRomSpline (code)   Use   Splines can be used statically like this:       Vector2 out = new Vector2();     Vector2 tmp = new Vector2();     Vector2[] dataSet = new Vector2[size];     /* fill dataSet with path points */     CatmullRomSpline.calculate(out, t, dataSet, continuous, tmp);// stores in the vector out the point of the catmullRom path of the dataSet in the time t. Uses tmp as a temporary vector. if continuous is true, the path is a loop.     CatmullRomSpline.derivative(out, t, dataSet, continuous, tmp); // the same as above, but stores the derivative of the time t in the vector out   or they can be stored like this:       CatmullRomSpline&lt;Vector2&gt; myCatmull = new CatmullRomSpline&lt;Vector2&gt;(dataSet, true);     Vector2 out = new Vector2();     myCatmull.valueAt(out, t);     myCatmull.derivativeAt(out, t);   It is preferred that you do the second way, since it’s the only way guaranteed by the Path interface.   Snippets   Caching a spline   This is often done at loading time, where you load the data set, calculates the spline and stores the points calculated. So you only evaluates the spline once. Trading memory for CPU time. (You usually should always cache if drawing static things)   /*members*/     int k = 100; //increase k for more fidelity to the spline     Vector2[] points = new Vector2[k]; /*init()*/     CatmullRomSpline&lt;Vector2&gt; myCatmull = new CatmullRomSpline&lt;Vector2&gt;(dataSet, true);     for(int i = 0; i &lt; k; ++i)     {         points[i] = new Vector2();         myCatmull.valueAt(points[i], ((float)i)/((float)k-1));     }   Rendering cached spline   How to render the spline previously cached   /*members*/     int k = 100; //increase k for more fidelity to the spline     Vector2[] points = new Vector2[k];     ShapeRenderer shapeRenderer; /*render()*/     shapeRenderer.begin(ShapeType.Line);     for(int i = 0; i &lt; k-1; ++i)     {         shapeRenderer.line(points[i], points[i+1]);     }     shapeRenderer.end();   Calculating on the fly   Do everything at render stage   /*members*/     int k = 100; //increase k for more fidelity to the spline     CatmullRomSpline&lt;Vector2&gt; myCatmull = new CatmullRomSpline&lt;Vector2&gt;(dataSet, true);     Vector2 out = new Vector2();     ShapeRenderer shapeRenderer; /*render()*/     shapeRenderer.begin(ShapeType.Line);     for(int i = 0; i &lt; k-1; ++i)     {         shapeRenderer.line(myCatmull.valueAt(points[i], ((float)i)/((float)k-1)), myCatmull.valueAt(points[i+1], ((float)(i+1))/((float)k-1)));     }     shapeRenderer.end();   Make sprite traverse through the cached path   This way uses a LERP through the cached points. It looks roughly sometimes but is very fast.   /*members*/     float speed = 0.15f;     float current = 0;     int k = 100; //increase k for more fidelity to the spline     Vector2[] points = new Vector2[k];  /*render()*/     current += Gdx.graphics.getDeltaTime() * speed;     if(current &gt;= 1)         current -= 1;     float place = current * k;     Vector2 first = points[(int)place];     Vector2 second;     if(((int)place+1) &lt; k)     {         second = points[(int)place+1];     }     else     {         second = points[0]; //or finish, in case it does not loop.     }     float t = place - ((int)place); //the decimal part of place     batch.draw(sprite, first.x + (second.x - first.x) * t, first.y + (second.y - first.y) * t);   Make sprite traverse through path calculated on the fly   Calculate sprite position on the path every frame, so it looks much more pleasant (you usually should always calculate on the fly if drawing dynamic things)   /*members*/     float speed = 0.15f;     float current = 0;     Vector2 out = new Vector2(); /*render()*/     current += Gdx.graphics.getDeltaTime() * speed;     if(current &gt;= 1)         current -= 1;     myCatmull.valueAt(out, current);     batch.draw(sprite, out.x, out.y);   Make sprite look at the direction of the spline   The angle can be found when applying the atan2 function to the normalised tangent(derivative) of the curve.       myCatmull.derivativeAt(out, current);     float angle = out.angle();   Make the sprite traverse at constant speed   As the arc-length of the spline through the dataSet points is not constant, when going from 0 to 1 you may notice that the sprite sometimes goes faster or slower, depending on some factors. To cancel this, we will change the rate of change of the time variable. We can easily do this by dividing the speed by the length of the rate of change. Instead of       current += Gdx.graphics.getDeltaTime() * speed;   change to:       myCatmull.derivativeAt(out, current);     current += (Gdx.graphics.getDeltaTime() * speed / myCatmull.spanCount) / out.len();   You should change the speed variable too, since it doesn’t take a “percent per second” value anymore, but a “meter(pixel?) per second”) now. The spanCount is necessary since the derivativeAt method takes into account the current span only.  ",
        
        "url": "/wiki/math-utils/path-interface-and-splines" },{
        "title": "Physics",
        "excerpt":
        
        "Libraries   libGDX comes with 2 different Physics libraries. Box2D which is used for 2D physics and also a experimental Bullet Physics wrapper which can be used for 3D physics.   Documentation   Box2D Documentation - Most documentation is compatible with the libGDX implementation but will need some changes from C++ to Java.   Bullet Wiki - Documentation for Bullet should be compatible with the Java wrapper of Bullet but will need various syntax and class name changes.  ",
        
        "url": "/wiki/extensions/physics/physics" },{
        "title": "Pixmaps",
        "excerpt":
        
        "Introduction   A Pixmap (code) encapsulates image data resident in memory. It supports simple file loading and draw operations for basic image manipulation. The most typical use is preparation of an image for upload to the GPU by wrapping in a  Texture (code) instance. There are also methods for image saving/loading through the  PixmapIO (code) class. PixmapIO supports uncompressed PNG as well as CIM, a compression format peculiar to libGDX which is useful for quick storage access such as during state saving/loading between application focus changes.   As a Pixmap resides in native heap memory it must be disposed of by calling dispose() when no longer needed to prevent memory leaks.   Pixmap Creation   Pixmaps can be created from a byte array containing image data encoded as JPEG, PNG or BMP, a  FileHandle (code), or a specification of dimensions and a format. Once created it can be further manipulated before being uploaded to an OpenGL Texture (code)  for rendering or saved for some future use.   The following example creates a 64x64 32-bit RGBA Pixmap, draws a filled green circle inside it, uploads it to a Texture and then disposes of the memory:   Pixmap pixmap = new Pixmap( 64, 64, Format.RGBA8888 ); pixmap.setColor( 0, 1, 0, 0.75f ); pixmap.fillCircle( 32, 32, 32 ); Texture pixmaptex = new Texture( pixmap ); pixmap.dispose();   Note that the memory of the Pixmap is no longer needed after being wrapped in the Texture and uploaded to the GPU. It is therefore disposed of. Note also that this Texture will be unmanaged as it was not created from a persistent file, but a volatile piece of memory which was subsequently discarded.   The next example shows a pause/resume life cycle of a typical Android application:    FileHandle dataFile = Gdx.files.external( dataFolderName + \"current.cim\" );  @Override public void pause() {   Pixmap pixmap;   // do something with pixmap...   PixmapIO.writeCIM( dataFile, pixmap ); }  @Override public void resume() {    if ( dataFile.exists() ) {     Pixmap pixmap = PixmapIO.readCIM( dataFile );      // do something with pixmap...   } }   In the preceding example, pixmap will be written to an external location using a simple compression scheme upon application focus loss, and subsequently upon regaining focus it will be reloaded if extant at the specified location.   Drawing   Pixmap supports simple drawing operations such as the drawing of lines, filled or unfilled rectangles and circles, as well as the setting of individual pixels and drawing of other pixmaps. These operations are also affected by color, blending, and filters which are controlled by setColor(), setBlending(), and setFilter() respectively.  ",
        
        "url": "/wiki/graphics/2d/pixmaps" },{
        "title": "Playing PCM audio",
        "excerpt":
        
        "The audio module can provide you direct access to the audio hardware for writing PCM samples to it.   The audio hardware is abstracted via the AudioDevice (source) interface.   To create a new AudioDevice instance we do the following:   AudioDevice device = Gdx.audio.newAudioDevice(44100, true);   This creates a new AudioDevice that has a sampling frequency of 44.1khz and outputs mono. If the device couldn’t be created, a GdxRuntimeException will be thrown.   We can write either 16-bit signed PCM or 32-bit float PCM data to the device:   float[] floatPCM = ... generated from a sine for example ... device.writeSamples(floatPCM, 0, floatPCM.length);  short[] shortPCM = ... generated from a decoder ... device.writeSamples(shortPCM, 0, shortPCM.length);   If stereo is used, left and right channel samples are interleaved as usual (first float/short -&gt; left, second float/short -&gt; right).   The latency in milliseconds can be queried like this:   int latencyInSamples = device.getLatency();   This will return the size of the audio buffer in samples and thus give you a good indicator about the latency. The bigger the return value, the longer it takes for the audio to arrive at the recipient after it was written.   Note that latency on almost all Android phones is ridiculously high. Real-time audio applications have a hard time to get in the useful 10-30ms range. Usually you can achieve 100ms latency, many phones will have up to 400ms latency. Sadly, this is a driver/OS related problem and can’t be worked around.   An AudioDevice is a native resource and needs to be disposed of when no longer used:   device.dispose();   Direct PCM output is not supported in the JavaScript/WebGL backend.  ",
        
        "url": "/wiki/audio/playing-pcm-audio" },{
        "title": "Pollfish in libGDX",
        "excerpt":
        
        "Introduction   This is a simple tutorial that will help you integrate Pollfish surveys in your libGDX Android app.   Integration of Pollfish in an Android application is simple, and is described in detail in the official guide here: Pollfish Android Documentation   STEPS SUMMARY      Sign Up as a Publisher at Pollfish website, create a new app and grab its API key from the dashboard   Download Pollfish SDK (either Google Play or Universal) or reference it in your gradle file through jcenter()   Add relevant Pollfish aar or jar file in your project, import relevant classes and add required permissions in your app’s manifest as described in the documentation   Call Pollfish init function in your onResume of your AndroidLauncher    package com.mygdx.game.android;  import android.os.Bundle; import com.badlogic.gdx.backends.android.AndroidApplication; import com.badlogic.gdx.backends.android.AndroidApplicationConfiguration; import com.mygdx.game.MyGdxGame;  import com.pollfish.main.PollFish; import com.pollfish.constants.Position;  public class AndroidLauncher extends AndroidApplication{   @Override  protected void onCreate (Bundle savedInstanceState) {    super.onCreate(savedInstanceState);        AndroidApplicationConfiguration config = new AndroidApplicationConfiguration();    initialize(MyGdxGame(), config);  }   @Override  public void onResume() {      super.onResume();        PollFish.initWith(this, new ParamsBuilder(\"YOUR_API_KEY\").build());  }  }   With this simple implementation you should be able to see Pollfish surveys in your app within a few minutes.   Optional Steps   1. Listen to Pollfish listeners (optional)   You can listen to Pollfish listeners by implementing them in your AndroidLauncher, for example:   import com.pollfish.interfaces.PollfishSurveyCompletedListener;  public class AndroidLauncher extends AndroidApplication implements PollfishSurveyCompletedListener{   @Override public void onPollfishSurveyCompleted(boolean playfulSurveys , int surveyPrice) {     Log.d(\"Pollfish\", \"Pollfish survey completed - Playful survey: \" + playfulSurveys + \" with price: \" + surveyPrice);   }   2. Manually show or hide Pollfish (optional)   You can manually show or hide Pollfish in your Android App with a simple implementation as the following one:   In your and in your AndroidLauncher.java file:    package com.mygdx.game.android;  import android.os.Bundle; import com.badlogic.gdx.backends.android.AndroidApplication; import com.badlogic.gdx.backends.android.AndroidApplicationConfiguration; import com.mygdx.game.MyGdxGame;  import com.pollfish.main.PollFish; import com.pollfish.constants.Position;  public class AndroidLauncher extends AndroidApplication implements MyGdxGame.MyPollfishCallbacks {    protected MyGdxGame myGdxGame;    @Override  protected void onCreate (Bundle savedInstanceState) {    super.onCreate(savedInstanceState);        AndroidApplicationConfiguration config = new AndroidApplicationConfiguration();        myGdxGame=new MyGdxGame();    myGdxGame.setMyPollfishCallbacks(this);     initialize(myGdxGame, config);  }   @Override  public void onResume() {       super.onResume();          PollFish.initWith(this, new ParamsBuilder(\"YOUR_API_KEY\")             .customMode(true)             .build());                  PollFish.hide();    }   @Override  public void onShowPollfish() {      PollFish.show();  }   @Override  public void hidePollfish() {      PollFish.hide();  }  }   and in your MyGdxGame.java file:   package com.mygdx.game;  import com.badlogic.gdx.ApplicationAdapter; import com.badlogic.gdx.Gdx; import com.badlogic.gdx.graphics.GL20; import com.badlogic.gdx.graphics.g2d.SpriteBatch; import com.badlogic.gdx.scenes.scene2d.InputEvent; import com.badlogic.gdx.scenes.scene2d.Stage; import com.badlogic.gdx.scenes.scene2d.ui.Skin; import com.badlogic.gdx.scenes.scene2d.ui.TextButton; import com.badlogic.gdx.scenes.scene2d.utils.ClickListener;  public class MyGdxGame extends ApplicationAdapter {          private SpriteBatch batch;     private Skin skin;     private Stage stage;      // Define an interface for your various callbacks to the android launcher     public interface MyPollfishCallbacks {         public void showPollfish();         public void hidePollfish();     }      // Local variable to hold the callback implementation     private MyPollfishCallbacks myPollfishCallbacks;      // ** Additional **     // Setter for the callback     public void setMyPollfishCallbacks(MyPollfishCallbacks callback) {         myPollfishCallbacks = callback;     }      @Override     public void create() {          batch = new SpriteBatch();         skin = new Skin(Gdx.files.internal(\"data/uiskin.json\"));         stage = new Stage();          final TextButton showBtn = new TextButton(\"Show\", skin, \"default\");          showBtn.setWidth(400f);         showBtn.setHeight(100f);         showBtn.setPosition(Gdx.graphics.getWidth() /2 - 600f, Gdx.graphics.getHeight()/2 - 10f);          showBtn.addListener(new ClickListener(){             @Override             public void clicked(InputEvent event, float x, float y){                    if(myPollfishCallbacks!=null){                          myPollfishCallbacks.showPollfish();                    }             }         });          final TextButton hideBtn = new TextButton(\"Hide\", skin, \"default\");          hideBtn.setWidth(400f);         hideBtn.setHeight(100f);         hideBtn.setPosition(Gdx.graphics.getWidth() /2 + 300f, Gdx.graphics.getHeight()/2 - 10f);          hideBtn.addListener(new ClickListener(){             @Override             public void clicked(InputEvent event, float x, float y){                    if(myPollfishCallbacks!=null){                          myPollfishCallbacks.hidePollfish();                    }             }         });          stage.addActor(showBtn);         stage.addActor(hideBtn);          Gdx.input.setInputProcessor(stage);     }      @Override     public void dispose() {         batch.dispose();     }      @Override     public void render() {         Gdx.gl.glClearColor(1, 0, 1, 1);         Gdx.gl.glClear(GL20.GL_COLOR_BUFFER_BIT);          batch.begin();         stage.draw();         batch.end();     }      @Override     public void resize(int width, int height) {     }      @Override     public void pause() {     }      @Override     public void resume() {     } }   3. Check if Pollfish survey is still available on your device   It happens that time had past since you initialized Pollfish and a survey is received. If you want to check if survey is still avaialble on your device and has not expired you can check by calling:   PollFish.isPollfishPresent();   ",
        
        "url": "/wiki/third-party/pollfish-in-libgdx" },{
        "title": "Polling",
        "excerpt":
        
        "Polling refers to checking the current state of an input device, e.g. is a specific key pressed, where is the first finger on the screen and so on. It’s a quick and easy way to process user input and will suffice for most arcade games.   Caution: If you rely on polling, you might miss events, e.g. a fast paced key down/key up. If you need to make sure a specific sequence of input action was completed, use event handling instead.   Polling the Keyboard   Polling for input from a Keyboard is done with just one simple line of code, like below.   boolean isAPressed = Gdx.input.isKeyPressed(Keys.A);   The parameter passed to that method is a Key Code. Rather than having to memorize these codes there is a static class within the Input interface that contains the codes which you can use. They can be seen  here.   Polling the Touch Screen / Mouse   There are a number of methods concerning polling the touch screen/mouse. To check whether one or more fingers are currently on the screen (which is equivalent to a mouse button being pressed) you can do the following:   boolean isTouched = Gdx.input.isTouched();   For multi-touch input you might be interested whether a specific finger (pointer) is currently on the screen:   // Will Return whether the screen is currently touched boolean firstFingerTouching = Gdx.input.isTouched(0); boolean secondFingerTouching = Gdx.input.isTouched(1); boolean thirdFingerTouching = Gdx.input.isTouched(2);   Each finger that goes down on the screen gets a so called pointer index. The first finger to go down gets the index 0, the next one gets the index 1 and so on. If a finger is lifted off the screen and touched down again, while other fingers are still on the screen, the finger will get the first free index. An example:      first finger goes down -&gt; 0   second finger goes down -&gt; 1   third finger goes down -&gt; 2   second finger goes up -&gt; 1 becomes free   first finger goes up -&gt; 0 becomes free, at this point only 2 is used   another finger goes down -&gt; 0, as it is the first free index   On the desktop or the browser you will only ever have a single “finger” so to speak.   If you want to check if the user touched down and released any finger again you can use the following method:   // Will return whether the screen has just been touched boolean justTouched = Gdx.input.justTouched();   This can be used in situations where you want to check a touch down/up sequence really quickly, e.g. on a screen that says “touch screen to continue”. Note that it is not a reliable method as it is based on polling.   To get the coordinates of a specific finger you can use the following methods:   int firstX = Gdx.input.getX(); int firstY = Gdx.input.getY(); int secondX = Gdx.input.getX(1); int secondY = Gdx.input.getY(1);   Here we get the touch coordinates at pointer index 0 (0 is default) and pointer index 1. Coordinates are reported in a coordinate system relative to the screen. The origin (0, 0) is in the upper left corner of the screen, the x-axis points to the right, the y-axis points downwards.   Pressure   You can get the Pressure applied on a pointer using   Gdx.input.getPressure()   This returns a value between 0 and 1   Mouse Buttons  On the desktop you can also check which mouse buttons are currently pressed:   boolean leftPressed = Gdx.input.isButtonPressed(Input.Buttons.LEFT); boolean rightPressed = Gdx.input.isButtonPressed(Input.Buttons.RIGHT);   See the Buttons class for more constants.   Note that on Android we only emulate the left mouse button. Any touch event will be interpreted as if it was issued with a left mouse button press. Touch screens obviously don’t have a notion of left, right and middle button.                  Prev       Next          ",
        
        "url": "/wiki/input/polling" },{
        "title": "Preferences",
        "excerpt":
        
        "Preferences are a simple way to store small data for your application, e.g. user settings, small game state saves and so on. Preferences work like a hash map, using strings as keys, and various primitive types as values. Preferences are also the only way to date to write persistent data when your application is run in the browser.   Obtaining a Preferences instance  Preferences are obtained via the following snippet:   Preferences prefs = Gdx.app.getPreferences(\"My Preferences\");   Note that your application can have multiple preferences, just give them different names.   Writing And Reading Values  Modifying preferences is as simple as modifying a Java Map:   prefs.putString(\"name\", \"Donald Duck\"); String name = prefs.getString(\"name\", \"No name stored\");  prefs.putBoolean(\"soundOn\", true); prefs.putInteger(\"highscore\", 10);   Note that getter methods come in two flavors: with and without a default value. The default value will be returned if there is no value for the specified key in the preferences.   Flushing  Your changes to a preferences instance will only get persisted if you explicitly call the flush() method.   // bulk update your preferences prefs.flush();   Storage   On Windows, Linux, and OS X, preferences are stored in an xml file within the user’s home directory.                  OS       Preferences storage location                       Windows       %UserProfile%/.prefs/My Preferences                 Linux and OS X       ~/.prefs/My Preferences           The file is named whatever you passed to Gdx.app.getPreferences().   This is useful to know if you want to change or delete them manually for testing.   On Android, the system’s SharedPreferences class is used. This means preferences will survive app updates, but are deleted when the app is uninstalled.   On iOS, an NSMutableDictionary will be written to the given file. [per javadocs]  ",
        
        "url": "/wiki/preferences" },{
        "title": "Profiling",
        "excerpt":
        
        "This article describes the little helpers and utilities that might come in handy in case you are running into performance problems and need to start profiling your game.   FPSLogger   The FPSLogger is a simple helper class to log the frames per seconds achieved. Just invoke the log() method in your rendering method. The output will be logged once per second.   PerformanceCounter   The PerformanceCounter keeps track of the time and load (percentage of total time) a specific task takes. Call start() just before starting the task and stop() right after. You can do this multiple times if required. Every render or update call tick() to update the values. The time FloatCounter provides access to the minimum, maximum, average, total and current time the task takes. Likewise for the load value, which is the percentage of the total time.   OpenGL  Profiling  Profiling the actual OpenGL calls that happen while your game is running is often not very easy to do, since libGDX tries to abstract all those low-level things away. In order to enable the collection of these information, there is the GLProfiler.   To enable it you have to call the static method GLProfiler.enable(). Behind the scenes this will replace the original GL20 and GL30 instances (Gdx.gl etc.) with the profilers.   Now those will be active and start to monitor the actual GL calls (and GL errors, see below) for you. One information you might be interested in, could be the amount of texture bindings that happen, which are costly and might slow down your game. To optimize this, you might start to use a TextureAtlas. To prove with actual numbers that the texture bindings become less, you can read the static field GLProfiler.textureBindings from the profiler.   You might also implement something like view frustum culling to render only those things that are visible on the screen. The static field GLProfiler.drawCalls will show the results of these kind of optimizations.   Currently the following informations are provided by the profiler:     Amount of total OpenGL calls   Amount of draw calls   Amount of texture bindings   Amount of shader switches   Amount of used vertices   GLProfiler.vertexCount is actually a FloatCounter. Besides GLProfiler.vertexCount.total it has more information like GLProfiler.vertexCount.min, GLProfiler.vertexCount.max or GLProfiler.vertexCount.average, which are the values based on individual drawcalls.   In order to reset all these numbers once you have read and displayed them (probably once per frame), you have to call the GLProfiler.reset() method. To completely disable the profiling and replace the profilers with the original GL20 and GL30 instances, use GLProfiler.disable().   Note that in case you are using Gdx.graphics.getGL20() or Gdx.graphics.getGL30() you are bypassing the profiler and that’s why you should use Gdx.gl20 or Gdx.gl30 directly.   To see how to use this you can have a look at the Benchmark3DTest   Error checking (since 1.6.5)  GLProfiler has one more useful feature and that is error checking.   Almost all GL calls can in some circumstances produce errors. These errors are not thrown or logged like Java errors, but they have to be explicitly checked for (Gdx.gl.getError()), so they can be hard to find. Enabled GLProfiler (see above on how to enable) will automatically check for GL errors after every GL call and report it, so you don’t have to.   By default, encountered errors will be printed to console (using Gdx.app.error). However, this can be customized (for example for your own logging/crash reporting system) by setting up a different error listener in GLProfiler.listener.   If you want to know where exactly did the error happen in your code, you may want to use the GLErrorListener.THROWING_LISTENER which throws an exception on any GL error. Error listener callback is called inside GL call, so the stack trace will reveal where exactly things went wrong. (Throwing an exception on GL error will most likely crash your application in case of errors, so it is not used by default.)   For example use and testing, there is a GLProfilerErrorTest   apitrace   apitrace is an open source cross platform debugger and profiler for OpenGL. You run the tracer to record the state and each call (including contents of buffers). You then run the viewing tool will read the trace and playback any state at any point in time. It breaks down the cpu/gpu time, every single OpenGL call. You can see the contents of the framebuffer and each texture bound at any point you choose.   Running: For linux, do: apitrace trace java -cp /home/me/my-app/desktop/build/libs/*.jar  -Dorg.lwjgl.opengl.libname=/usr/lib/apitrace/wrappers/glxtrace.so com.my.app.desktop.DesktopLauncher   Then just exit your app (if you want), run qapitrace and open that trace file.  ",
        
        "url": "/wiki/graphics/profiling" },{
        "title": "ProGuard DexGuard and libGDX",
        "excerpt":
        
        "ProGuard and the newer R8 are optimizers and obfuscators for Java and Android applications. You can use these tools with your libGDX application to make it harder for 3rd parties to decompile your app, reduce your apps size and even increase the runtime speed by ahead-of-time optimizations like inlining.   The following configuration file will make your libGDX app work with ProGuard/R8:   # To enable ProGuard in your project, edit project.properties # to define the proguard.config property as described in that file. # # Add project specific ProGuard rules here. # By default, the flags in this file are appended to flags specified # in ${sdk.dir}/tools/proguard/proguard-android.txt # You can edit the include path and order by changing the ProGuard # include property in project.properties. # # For more details, see #   https://developer.android.com/studio/build/shrink-code  # Add any project specific keep options here:  # If your project uses WebView with JS, uncomment the following # and specify the fully qualified class name to the JavaScript interface # class: #-keepclassmembers class fqcn.of.javascript.interface.for.webview { #   public *; #}  -verbose  -dontwarn com.badlogic.gdx.backends.android.AndroidFragmentApplication -dontwarn com.badlogic.gdx.utils.GdxBuild -dontwarn com.badlogic.gdx.physics.box2d.utils.Box2DBuild -dontwarn com.badlogic.gdx.jnigen.BuildTarget* -dontwarn com.badlogic.gdx.graphics.g2d.freetype.FreetypeBuild  # Required if using Gdx-Controllers extension -keep class com.badlogic.gdx.controllers.android.AndroidControllers  # Required if using Box2D extension -keepclassmembers class com.badlogic.gdx.physics.box2d.World {    boolean contactFilter(long, long);    void    beginContact(long);    void    endContact(long);    void    preSolve(long, long);    void    postSolve(long, long);    boolean reportFixture(long);    float   reportRayFixture(long, float, float, float, float, float); }   Note that you will also have to keep any classes that you access via reflection yourself! Please refer to the ProGuard/R8 documentation for more details.   To apply ProGuard/R8 to your Android project on Release builds, you need to add the following config to build.gradle file (the one in the android/ folder of your project, not the root build.gradle)   buildTypes {         release {             minifyEnabled true             proguardFiles getDefaultProguardFile('proguard-android.txt'), 'proguard-rules.pro'         }     }  ",
        
        "url": "/wiki/third-party/proguard-dexguard-and-libgdx" },{
        "title": "Creating a Project",
        "excerpt":
        
        "To setup your first project and download the necessary dependencies, libGDX offers a setup tool.                                                                                                   Set Up a Dev Environment                                                                                           Generate a Project                                                                                         Importing &amp; Running                                                                                         A Simple Game                                               Download the libGDX Project Setup Tool (gdx-setup):         Download            Double-click the downloaded file. If this doesn’t work, open your command line tool, go to the download folder and run  java -jar gdx-setup.jar       This will open the following setup that will allow you to generate your project:      Note: Instead of the User Interface of the Setup Tool you can also use the command-line to create your project.   You are asked to provide the following parameters:     Name: the name of the application; lower case with minuses is usually a good idea, e.g. my-game   Package: the Java package under which your code will reside, e.g. com.badlogic.mygame   Game Class: the name of the main game Java class of your app, e.g. MyGame   Destination: the folder where your app will be created   Android SDK: the location of your Android SDK. With Android Studio, to find out where it is, start Android Studio and click “Configure” -&gt; “SDK Manager”. By default it is in /Users/username/Library/Android/sdk            Supported Platforms: libGDX is cross-platform. By default, all the target platforms are included as sub projects (Desktop; Android; iOS; HTML). There is no need to change the default value unless you are sure you will never compile for a specific target.   Note: To compile your game for iOS you need Xcode, which is only available on macOS!      Official Extensions: the extensions offered are:             Bullet: 3D Collision Detection and Rigid Body Dynamics Library.        FreeType: Scalable font. Great to manipulate font size dynamically. However be aware that it does not work with HTML target if you cross compile for that target.        Tools: Set of tools including: particle editor (2d/3d), bitmap font and image texture packers.        Controller Library to handle controllers (e.g.: XBox 360 controller).        Box2d: Box2D is a 2D physics library.        Box2dlights: 2D lighting framework that uses box2d for raycasting and OpenGL ES 2.0 for rendering.        Ashley: A tiny entity framework.        Ai: An artificial intelligence framework.            By clicking “Show Third-Party Extensions” you can access a list of community-made libGDX extensions. If you want to add extensions later on, please take a look at this wiki page.   When ready, click “Generate”.   Note: You may get a message indicating that you have a more recent version of Android build tools or Android API than the recommended. This is not a blocking message and you may continue.   Project Layout  This will create a directory called mygame with the following layout:   settings.gradle            &lt;- definition of sub-modules. By default core, desktop, android, html, ios build.gradle               &lt;- main Gradle build file, defines dependencies and plugins gradlew                    &lt;- local Gradle wrapper gradlew.bat                &lt;- script that will run Gradle on Windows gradle                     &lt;- script that will run Gradle on Unix systems local.properties           &lt;- IntelliJ only file, defines Android SDK location  assets/                    &lt;- contains your graphics, audio, etc.  core/     build.gradle           &lt;- Gradle build file for core project     src/                   &lt;- Source folder for all your game's code  desktop/     build.gradle           &lt;- Gradle build file for desktop project     src/                   &lt;- Source folder for your desktop project, contains LWJGL launcher class  android/     build.gradle           &lt;- Gradle build file for android project     AndroidManifest.xml    &lt;- Android specific config     res/                   &lt;- contains icons for your app and other resources     src/                   &lt;- Source folder for your Android project, contains android launcher class  html/     build.gradle           &lt;- Gradle build file for the html project     src/                   &lt;- Source folder for your html project, contains launcher and html definition     webapp/                &lt;- War template, on generation the contents are copied to war. Contains startup url index page and web.xml  ios/     build.gradle           &lt;- Gradle build file for the iOS project     src/                   &lt;- Source folder for your iOS project, contains launcher   What is Gradle?  libGDX projects are Gradle projects, which makes managing dependencies and building considerably easier.   Gradle is a dependency management system and thus provides an easy way to pull in third-party libraries into your project, without having to manually download them. Instead, Gradle just needs you to provide it with the names and versions of the libraries you want to include in your application. This is all done in the Gradle configuration files. Adding, removing and changing the version of a third-party library is as easy as changing a few lines in that configuration file. The dependency management system will pull in the libraries you specified from a central repository (in our case Maven Central) and store them in a directory outside of your project. Find out more in our wiki.   In addition, Gradle is also a build system helping with building and packaging your application, without being tied to a specific IDE. This is especially useful if you use a build or continuous integration server, where IDEs aren’t readily available. Instead, the build server can call the build system, providing it with a build configuration so it knows how to build your application for different platforms. If you want to know more about deploying your application, take a look here.   Now you are ready to import the project into your IDE and run it.  ",
        
        "url": "/wiki/start/project-generation" },{
        "title": "Project Setup via Command Line",
        "excerpt":
        
        "This section describes how you can create a libGDX project from the command line. This is not required if you use the setup tool’s wizard. The following arguments are to be specified:      dir: the directory to write the project to, relative or absolute   name: the name of the application, lower-case with minuses is usually a good idea, e.g. mygame   package: the Java package under which your code will live, e.g. com.badlogic.mygame   mainClass: the name of the main ApplicationListener of your app, e.g. MyGame   sdkLocation: the location of your Android SDK, IntelliJ uses this if ANDROID_HOME is not set   excludeModules: the modules to exclude (lwjgl2; lwjgl3; Android; iOS; HTML) separated by ‘;’ and not case sensitive, e.g. Android;ios. Optional. Default it includes all the modules   extensions: the extensions to include (same name as in GUI: Bullet; Freetype; Tools; Controllers; Box2d; Box2dlights; Ashley; Ai) separated by ‘;’ and not case sensitive, e.g. box2d;box2dlights;Ai. Optional   Putting it all together, you can run the project generator on the command line as follows:   java -jar gdx-setup.jar --dir mygame --name mygame --package com.badlogic.mygame --mainClass MyGame --sdkLocation mySdkLocation [--excludeModules &lt;modules&gt;] [--extensions &lt;extensions&gt;]  ",
        
        "url": "/wiki/start/project-setup-via-command-line" },{
        "title": "Querying and Configuring Graphics (monitors, display modes, vsync, display cutouts)",
        "excerpt":
        
        "Configuration &amp; querying  libGDX has an elaborate API that lets you query monitors and display modes, and toggle vertical synchronization (vsync). This can be done either when configuring your application, or at runtime. Note: Display mode changes are not supported by Android, iOS or HTML5.   Querying and setting monitors &amp; display modes at configuration time  Querying monitors and display modes at configuration time is platform specific. The following subsections illustrate what you can do on each platform with regards to monitors and display modes.   LWJGL backend  Using the default LWJGL 2 backend, you can get the available display modes for the primary monitor as follows:   DisplayMode[] modes = LwjglApplicationConfiguration.getDisplayModes();   You can get the current display mode of the primary monitor (also known as desktop mode) as follows:   DisplayMode desktopMode = LwjglApplicationConfiguration.getDesktopDisplayMode();   Once you have a DisplayMode, you can set it on the LwjglApplicationConfiguration:   DisplayMode displayMode = LwjglApplicationConfiguration.getDesktopDisplayMode(); LwjglApplicationConfiguration config = new LwjglApplicationConfiguration(); config.setFromDisplayMode(displayMode); new LwjglApplication(new MyAppListener(), config);   Your app will be started in full-screen mode, using the resolution found in the DisplayMode object.   To start your app in windowed mode, simply specify the width and height of the window:   LwjglApplicationConfiguration config = new LwjglApplicationConfiguration(); config.width = 800; config.height = 600; new LwjglApplication(new MyAppListener(), config);   You can also position the window on the primary monitor by specifying its top-left corner coordinate relative to the monitors current display mode:   config.x = 100; config.y = 100;   To center your window, use -1 for the coordinates (the default).   Finally, you can also specify whether your app should start with vsync enabled or not:   config.vSyncEnabled = true;   Disabling vsync will allow your your app to run above your monitor’s refresh rate, provided foregroundFPS has been set accordingly.   LWJGL 3 backend  The LWJGL 3 backend is much more elaborate when it comes to monitors and display modes. Unlike the LWJGL 2 backend, it supports multi-monitor setups.   Querying all available monitors at configuration time works like this:  Monitor[] monitors = Lwjgl3ApplicationConfiguration.getMonitors();   To get the primary monitor, call:  Monitor primary = Lwjgl3ApplicationConfiguration.getPrimaryMonitor();   To get all supported display modes of a monitor, call:  DisplayMode[] displayModes = Lwjgl3ApplicationConfiguration.getDisplayModes(monitor);   To get the current display mode of a monitor, call:  DisplayMode desktopMode = Lwjgl3ApplicationConfiguration.getDisplayMode(monitor);   There are shorthands for getting the display modes of the primary monitor as well:  DisplayMode[] primaryDisplayModes = Lwjgl3ApplicationConfiguration.getDisplayModes(); DisplayMode primaryDesktopMode = Lwjgl3ApplicationConfiguration.getDisplayMode();   With a display mode in hand, you can set it on the Lwjgl3ApplicationConfiguration:  DisplayMode primaryMode = Lwjgl3ApplicationConfiguration.getDisplayMode(); Lwjgl3ApplicationConfiguration config = new Lwjgl3ApplicationConfiguration(); config.setFullscreenMode(primaryMode); new Lwjgl3ApplicationConfiguration(new MyAppListener(), config);  This will start your app in full-screen mode on the primary monitor, using that monitor’s current resolution. If you pass a display mode from a different monitor, the app will be started in full-screen mode on that montior. Note: it is recommended to always use the current display mode of a monitor. Other display modes may fail.   To start your app in windowed mode, call this method on the configuration:  Lwjgl3ApplicationConfiguration config = new Lwjgl3ApplicationConfiguration(); config.setWindowedMode(800, 600);   This will start your app in windowed mode on the primary monitor. To set the position of your window call:  config.setWindowPosition(100, 100);   This will position the window’s top-left corner at the coordinates 100,100 relative to the virtual surface all your monitors span. To check the position of a monitor, simply access it’s virtualX and virtualY members. These coordinates are also relative to the virtual surface your monitors span. By positioning your window relative to these coordinates, you can move a window to that specific monitor.   You can also specify if your window should be resizable and whether it has decoration (window title bar, borders):  config.setResizable(false); config.setDecorated(false);   The LWJGL 3 backend also allows you to specify how to deal with HDPI monitors. The operating system may report logical sizes and coordinates instead of pixel-based coordinates to you for drawing surface sizes or mouse events. E.g. on a Macbook Pro with a retina display running Mac OS X, the OS reports only half the width/height of the underlying pixel surface. The LWJGL 3 backend can report drawing surface sizes as returned by Gdx.graphics.getWidth()/getHeight() and mouse coordinates either in those logical coordinates, or in pixel coordinates. To configure this behaviour, set a HDPI mode:   config.setHdpiMode(HdpiMode.Logical);   This will report mouse coordinates and drawing surface sizes in logical coordinates. It is also the default for this backend. If you want to work in raw pixels, use HdpiMode.Pixels. Note that when using logical coordinates, you will have to convert these to pixel coordinates for OpenGL functions like glScissor, glViewport or glReadPixels. All libGDX classes calling these functions will take into account the HdpiMode you set. If you call these functions yourself, use HdpiUtils.   Querying and setting monitors &amp; display modes at runtime  libGDX provides an API via the Graphics interface that lets you query monitors, display modes and other related aspects at runtime. Once you know about possible configurations, you can set them, e.g. switch to full-screen mode, or toggle vsync.   Checking if display mode changes are supported  Only a subset of platforms supports display mode changes. Notably, Android and iOS do not support switching to arbitrary full-screen display modes. It is therefor good practice to check if the platform your application currently runs on supports display mode changes:  if(Gdx.graphics.supportsDisplayModeChange()) {    // change display mode if necessary }  Note that all display mode related functions in Graphics will simply not do anything on platforms that don’t support display mode changes.   Querying monitors  To query all connected monitors, use this method:  Monitor[] monitors = Gdx.graphics.getMonitors();   On Android, iOS, GWT and the LWJGL 2 backend, only the primary monitor will be reported. The LWJGL 3 backend reports all connected monitors.   To query the primary monitor, use:  Monitor primary = Gdx.graphics.getPrimaryMonitor();   To query the monitor the window is currently on, use:  Monitor currMonitor = Gdx.graphics.getMonitor();   It is good practice to toggle full-screen on the monitor the window is on, instead of say the primary monitor. This allows users to move the application window to another monitor, and then enable full-screen mode there.   Querying display modes  Once you have a Monitor instance, you can query its supported display modes:  DisplayMode[] modes = Gdx.graphics.getDisplayModes(monitor);   To get the current display mode, use this method:  DisplayMode currMode = Gdx.graphics.getDisplayMode(monitor);   Switching to full-screen mode  With a DisplayMode from a specific Monitor, you can switch to full-screen as follows:  Monitor currMonitor = Gdx.graphics.getMonitor(); DisplayMode displayMode = Gdx.graphics.getDisplayMode(currMonitor); if(!Gdx.graphics.setFullscreenMode(displayMode)) {    // switching to full-screen mode failed }  If the switch to full-screen mode failed, the backend will restore the last windowed-mode configuration.   Switching to windowed mode  To change the size of a window, or to switch from full-screen mode to windowed mode, use this method:  Gdx.graphics.setWindowedMode(800, 600);   This will set the window to windowed mode, centering it on the monitor it was on before the call to this method.   Querying display cutouts on mobile  On Android and iOS, displays don’t have to be a perfect rectangle but might have cutouts, round edges or overlaying status bars. You can query the areas that are not safe to use for your important game content with the Gdx.graphics.getSafeInsetLeft(), Gdx.graphics.getSafeInsetRight(), Gdx.graphics.getSafeInsetTop() and Gdx.graphics.getSafeInsetBottom() methods.   Desktop &amp; multi-window API of the LWJGL 3 backend  Some applications like editors or other desktop-only tools can benefit from multi-window setups. The LWJGL 3 backend provides an additional, non-cross-platform API to create multiple windows.   Every application starts with one window that is set up during configuration time:  Lwjgl3ApplicationConfiguration config = new Lwjgl3ApplicationConfiguration(); config.setWindowedMode(800, 600); new Lwjgl3ApplicationConfiguration(new MyMainWindowListener(), config);   In this example, the window is driven by MyMainWindowListener, a standard ApplicationListener. libGDX does not report events like iconification or focus loss directly. For this, the LWJGL 3 backend introduces a desktop specific interface called Lwjgl3WindowListener. You can provide an implementation of this interface to receive and react to such events:  config.setWindowListener(new Lwjgl3WindowListener() {    @Override    public void iconified() {       // the window is not visible anymore, potentially stop background       // work, mute audio, etc.    }     @Override    public void deiconified() {       // the window is visible again, start-up background work, unmute       // audio, etc.    }     @Override    public void focusLost() {       // the window lost focus, pause the game    }     @Override    public void focusGained() {       // the window received input focus, unpause the game    }     @Override    public boolean windowIsClosing() {       // if there's unsaved stuff, we may not want to close       // the window, but ask the user to save her work       if(isStuffUnsaved) {          // tell our app listener to show a save dialog          return false;       } else {          // OK, the window may close          return true;       } });   The LWJGL 3 backend does not report pause and resume events if the window loses focus. It will only report pause and resume events in case the app is iconfified/deiconified, or if the app is being closed.   To spawn additional windows, your code needs to cast Gdx.app to Lwjgl3Application. This is only possible if your project directly depends on the LWJGL 3 backend. You will not be able to share such code with other platforms. Once you have an Lwjgl3Application, you can create a new window like this:  Lwjgl3Application lwjgl3App = (Lwjgl3Application)Gdx.app; Lwjgl3WindowConfiguration windowConfig = new Lwjgl3WindowConfiguration(); windowConfig.setWindowListener(new MyWindowListener()); windowConfig.setTitle(\"My other window\"); Lwjgl3Window window = Lwjgl3App.newWindow(new MyOtherWindowAppListener(), windowConfig);   It is recommended to let every window have its own ApplicationListener. All windows of an application are updated on the same thread, one after the other. The LWJGL 3 backend will ensure that the statics Gdx.graphics and Gdx.input are setup for the window that’s currently being updated. This means that your ApplicationListener can essentially ignore the other windows, and pretend it’s the only listener in town.   There is one exception to this. When using Gdx.app.postRunnable(), the LWJGL 3 backend can not decide for which window the Runnable has been posted. E.g. the method may have been called from a worker thread, and by the time the Runnable is posted, a different window may currently be updated. To fix this, it is recommended to post window-specific Runnable instances directly to the window:   // e.g. in a worker thread window.postRunnable(new MyRunnable());   This ensures that the statics Gdx.graphics and Gdx.input will be setup for the specific window when the Runnable is executed.   The Lwjgl3Window class has additional methods that let you modify the window’s properties. You can fetch the current window in your ApplicationListener, then proceed to modify it:  // in ApplicationListener#render() Lwjgl3Window window = ((Lwjgl3Graphics)Gdx.graphics).getWindow(); window.setVisible(false); // hide the window window.iconifyWindow(); // iconify the window window.deiconifyWindow(); // deiconify window window.closeWindow(); // close the window, also disposes the ApplicationListener  ",
        
        "url": "/wiki/graphics/querying-and-configuring-graphics" },{
        "title": "Querying",
        "excerpt":
        
        "The Application interface provides various methods to query properties of the run-time environment.   Getting the Application Type  Sometimes it is necessary to implement certain functionality differently depending on the platform it is running on. The Application.getType() method returns the platform the application is currently running on:   switch (Gdx.app.getType()) {     case Android:         // android specific code         break;     case Desktop:         // desktop specific code         break;     case WebGl:         // HTML5 specific code         break;     default:         // Other platforms specific code }   On Android and iOS, one can also query the OS version the application is currently running on:   int androidVersion = Gdx.app.getVersion();   On Android, this will return the SDK level supported on the current device, e.g., 3 for Android 1.5; on iOS it will return the major version of the current OS.   Memory Consumption  For debugging and profiling purposes it is often necessary to know the memory consumption, for both the Java heap and the native heap:   long javaHeap = Gdx.app.getJavaHeap(); long nativeHeap = Gdx.app.getNativeHeap();   Both methods return the number of bytes currently in use on the respective heap.  ",
        
        "url": "/wiki/app/querying" },{
        "title": "Quick start",
        "excerpt":
        
        "This is a series of articles, originally written by Xoppa. You can find the original versions here:      Basic 3D using libGDX   Loading models using libGDX   Loading a scene with libGDX   Behind the 3D scenes - part1   Behind the 3D scenes - part2   Creating a shader with libGDX   Using materials with libGDX   3D frustum culling with libGDX   Interacting with 3D objects   Using collision shapes   3D collision detection   3D physics simulation   These articles give you a good quick overview on how to achieve basic tasks with the new 3D API. For more information, take a look at the Javadocs of the classes in the com.badlogic.gdx.graphics.g3d package or check out the tests and demos.  ",
        
        "url": "/wiki/graphics/3d/quick-start" },{
        "title": "Reading and writing JSON",
        "excerpt":
        
        "   Overview   Writing Object Graphs   Reading Object Graphs   Customizing Serialization   Serialization Methods   Event Based Parsing   Supported Classes   Overview   libGDX can perform automatic object to JSON serialization and JSON to object deserialization. Four small classes make up the API:      JsonWriter: A builder style API for emitting JSON.   JsonReader: Parses JSON and builds a DOM of JsonValue objects.   JsonValue: Describes a JSON object, array, string, float, long, boolean, or null.   Json: Reads and writes arbitrary object graphs using JsonReader and JsonWriter.   To use these classes outside of libgdx, see the JsonBeans project.   Writing Object Graphs   The Json class uses reflection to automatically serialize objects to JSON. For example, here are two classes (getters/setters and constructors omitted):   public class Person {    private String name;    private int age;    private ArrayList numbers; }  public class PhoneNumber {    private String name;    private String number; }   Example object graph using these classes:   Person person = new Person(); person.setName(\"Nate\"); person.setAge(31); ArrayList numbers = new ArrayList(); numbers.add(new PhoneNumber(\"Home\", \"206-555-1234\")); numbers.add(new PhoneNumber(\"Work\", \"425-555-4321\")); person.setNumbers(numbers);   The code to serialize this object graph:   Json json = new Json(); System.out.println(json.toJson(person));  {numbers:[{class:com.example.PhoneNumber,number:\"206-555-1234\",name:Home},{class:com.example.PhoneNumber,number:\"425-555-4321\",name:Work}],name:Nate,age:31}   That is compact, but hardly legible. The prettyPrint method can be used:   Json json = new Json(); System.out.println(json.prettyPrint(person));  { numbers: [ \t{ \t\tclass: com.example.PhoneNumber, \t\tnumber: \"206-555-1234\", \t\tname: Home \t}, \t{ \t\tclass: com.example.PhoneNumber, \t\tnumber: \"425-555-4321\", \t\tname: Work \t} ], name: Nate, age: 31 }   Note that the class for the PhoneNumber objects in the ArrayList numbers field appears in the JSON. This is required to recreate the object graph from the JSON because ArrayList can hold any type of object. Class names are only output when they are required for deserialization. If the field was ArrayList&lt;PhoneNumber&gt; numbers then class names would only appear when an item in the list extends PhoneNumber. If you know the concrete type or aren’t using generics, you can avoid class names being written by telling the Json class the types:   Json json = new Json(); json.setElementType(Person.class, \"numbers\", PhoneNumber.class); System.out.println(json.prettyPrint(person));  { numbers: [ \t{ \t\tnumber: \"206-555-1234\", \t\tname: Home \t}, \t{ \t\tnumber: \"425-555-4321\", \t\tname: Work \t} ], name: Nate, age: 31 }   When writing the class cannot be avoided, an alias can be given:   Json json = new Json(); json.addClassTag(\"phoneNumber\", PhoneNumber.class); System.out.println(json.prettyPrint(person));  { numbers: [ \t{ \t\tclass: phoneNumber, \t\tnumber: \"206-555-1234\", \t\tname: Home \t}, \t{ \t\tclass: phoneNumber, \t\tnumber: \"425-555-4321\", \t\tname: Work \t} ], name: Nate, age: 31 }   The Json class can write and read both JSON and a couple JSON-like formats. It supports “JavaScript”, where the object property names are only quoted when needed. It also supports a “minimal” format (the default), where both object property names and values are only quoted when needed.   Json json = new Json(); json.setOutputType(OutputType.json); json.setElementType(Person.class, \"numbers\", PhoneNumber.class); System.out.println(json.prettyPrint(person));  { \"numbers\": [ \t{ \t\t\"number\": \"206-555-1234\", \t\t\"name\": \"Home\" \t}, \t{ \t\t\"number\": \"425-555-4321\", \t\t\"name\": \"Work\" \t} ], \"name\": \"Nate\", \"age\": 31 }   Note: By default, the Json class will not write those fields which have values that are identical to a newly constructed instance. If you wish to disable this behavior and include all fields, call json.setUsePrototypes(false);.   Reading Object Graphs   The Json class uses reflection to automatically deserialize objects from JSON. Here is how to deserialize the JSON from the previous examples:   Json json = new Json(); String text = json.toJson(person); Person person2 = json.fromJson(Person.class, text);   The type passed to fromJson is the type of the root of the object graph. From this, the Json class determines the types of all the fields and all other objects encountered, recursively. The “knownType” and “elementType” of the root can be passed to toJson. This is useful if the type of the root object is not known:   Json json = new Json(); json.setOutputType(OutputType.minimal); String text = json.toJson(person, Object.class); System.out.println(json.prettyPrint(text)); Object person2 = json.fromJson(Object.class, text);  { class: com.example.Person, numbers: [ \t{ \t\tclass: com.example.PhoneNumber, \t\tnumber: \"206-555-1234\", \t\tname: Home \t}, \t{ \t\tclass: com.example.PhoneNumber, \t\tnumber: \"425-555-4321\", \t\tname: Work \t} ], name: Nate, age: 31 }   To read the JSON as a DOM of maps, arrays, and values, the JsonReader class can be used:   Json json = new Json(); String text = json.toJson(person, Object.class); JsonValue root = new JsonReader().parse(text);   The JsonValue describes a JSON object, array, string, float, long, boolean, or null.   Customizing Serialization   Usually automatic serialization is sufficient, however there are some classes where automatic serialization is not possible or custom serialization is desired. Serialization may be customized by having the class implement the Json.Serializable interface or by registering a Json.Serializer with the Json instance.   This example uses Json.Serializable to write a phone number as an object with a single field:   static public class PhoneNumber implements Json.Serializable {    private String name;    private String number;     public void write (Json json) {       json.writeValue(name, number);    }     public void read (Json json, JsonValue jsonMap) {       name = jsonMap.child().name();       number = jsonMap.child().asString();    } }  Json json = new Json(); json.setElementType(Person.class, \"numbers\", PhoneNumber.class); String text = json.prettyPrint(person); System.out.println(text); Person person2 = json.fromJson(Person.class, text);  { numbers: [ \t{ \t\tHome: \"206-555-1234\" \t}, \t{ \t\tWork: \"425-555-4321\" \t} ], name: Nate, age: 31 }   The class implementing Json.Serializable must have a zero argument constructor because object construction is done for you. In the write method, the surrounding JSON object has already been written. The read method always receives a JsonValue that represents that JSON object.   Json.Serializer provides more control over what is output, requiring writeObjectStart and writeObjectEnd to be called if you require a JSON object like Json.Serializable. Alternatively, a JSON array or a simple value (string, int, boolean) could be output instead of an object. Json.Serializer also allows the object creation to be customized:   Json json = new Json(); json.setSerializer(PhoneNumber.class, new Json.Serializer&lt;PhoneNumber&gt;() {    public void write (Json json, PhoneNumber number, Class knownType) {       json.writeObjectStart();       json.writeValue(number.name, number.number);       json.writeObjectEnd();    }     public PhoneNumber read (Json json, JsonValue jsonData, Class type) {       PhoneNumber number = new PhoneNumber();       number.setName(jsonData.child().name());       number.setNumber(jsonData.child().asString());       return number;    } }); json.setElementType(Person.class, \"numbers\", PhoneNumber.class); String text = json.prettyPrint(person); System.out.println(text); Person person2 = json.fromJson(Person.class, text);   Serialization Methods   Json has many methods to read and write data to the JSON. Write methods without a name string are used to write a value that is not a JSON object field (eg, a string or an object in a JSON array). Write methods that take a name string are used to write a field name and value for a JSON object.   writeObjectStart is used to start writing a JSON object, then values can be written using the write methods that take a name string. When the object is finished, writeObjectEnd must be called:   json.writeObjectStart(); json.writeValue(\"name\", \"value\"); json.writeObjectEnd();   The writeObjectStart methods that take an actualType and a knownType will write a class field to the JSON if the types differ. This enables the actual type to be known during deserialization. For example, the known type may be java.util.Map but the actual type is java.util.LinkedHashMap (which extends HashMap), so deserialization needs to know the actual type to create.   Writing arrays works in a similar manner, except the values should be written using the write methods that do not take a name string:   json.writeArrayStart(); json.writeValue(\"value1\"); json.writeValue(\"value2\"); json.writeArrayEnd();   The Json class can automatically write Java object fields and values. writeFields writes all fields and values for the specified Java object to the current JSON object:   json.writeObjectStart(); json.writeFields(someObject); json.writeObjectEnd();   The writeField method writes the value for a single Java object field:   json.writeObjectStart(); json.writeField(someObject, \"javaFieldName\", \"jsonFieldName\"); json.writeObjectEnd();   Many of the write methods take an “element type” parameter. This is used to specify the known type of objects in a collection. For example, for a list:   ArrayList list = new ArrayList(); list.add(someObject1); list.add(someObject2); list.add(someObject3); list.add(someOtherObject); ... json.writeObjectStart(); json.writeValue(\"items\", list); json.writeObjectEnd();  { \titems: [ \t\t{ class: com.example.SomeObject, value: 1 }, \t\t{ class: com.example.SomeObject, value: 2 }, \t\t{ class: com.example.SomeObject, value: 3 }, \t\t{ class: com.example.SomeOtherObject, value: four } \t] }   Here the known type of objects in the list is Object, so each object in the JSON for “items” has a class field that specifies Integer or String. By specifying the element type, Integer is used as the known type so only the last entry in the JSON for “items” has a class field:   json.writeObjectStart(); json.writeValue(\"items\", list, ArrayList.class, Integer.class); json.writeObjectEnd();  { \titems: [ \t\t{ value: 1 }, \t\t{ value: 2 }, \t\t{ value: 3 }, \t\t{ class: com.example.SomeOtherObject, value: four } \t] }   For maps, the element type is used for the values. The keys for maps are always strings, a limitation of how object fields are described using JSON.   Note that the Json class uses generics on Java field declarations to determine the element type where possible.   Supported Classes   Note that when using GWT, not all classes are serializable. Json supports the following:     POJOs   OrderedMap (but not ArrayMap)   Array   String   Float   Boolean   Make sure to provide your own de/serializers or mark objects you don’t intend to serialize with the ‘transient’ keyword.   Manual and Event Based Parsing   The JsonReader class reads JSON and has protected methods that are called as JSON objects, arrays, strings, floats, longs, and booleans are encountered. By default, these methods build a DOM out of JsonValue objects. These methods can be overridden to do your own event based JSON handling.   // read something  JsonValue fromJson = new JsonReader().parse(jsonAsString); nickName = fromJson.getString(\"nickName\"); // ...  // write something JsonValue toJson = new JsonValue(JsonValue.ValueType.object); toJson.addChild(\"name\", new JsonValue(\"some name\"); toJson.addChild(\"age\", 12); // ... toJson.toJson(JsonWriter.OutputType.json);   ",
        
        "url": "/wiki/utils/reading-and-writing-json" },{
        "title": "Reading and writing XML",
        "excerpt":
        
        "XmlReader(code) parses XML into a simple DOM. It can also do event based parsing.   XmlWriter (code) uses a stack based API to emit XML.   Relevant blog post  ",
        
        "url": "/wiki/utils/reading-and-writing-xml" },{
        "title": "Recording PCM audio",
        "excerpt":
        
        "You can access PCM data from the microphone on a PC or Android phone via the AudioRecorder (code) interface. To create an instance of that interface use:   AudioRecorder recorder = Gdx.audio.newAudioRecorder(22050, true);   This will create an AudioRecorder with a sampling rate of 22.05khz, in mono mode. If the recorder couldn’t be created, a GdxRuntimeException will be thrown.   Samples can be read as 16-bit signed PCM:   short[] shortPCM = new short[1024]; // 1024 samples recorder.readSamples(shortPCM, 0, shortPCM.length);   Stereo samples are interleaved as usual (first sample -&gt; left channel, second sample -&gt; right channel).   An AudioRecorder is a native resource and needs to be disposed of if no longer in use:   recorder.dispose();   Audio recording is not supported in the JavaScript/WebGL backend.  ",
        
        "url": "/wiki/audio/recording-pcm-audio" },{
        "title": "Reflection",
        "excerpt":
        
        "In order to utilize reflection in a cross-platform way, libGDX provides a small wrapper around Java’s reflection API. The wrapper consists mainly of two classes containing the static methods you will use to perform reflection operations:      ArrayReflection - encapsulates access to java.lang.reflect.Array   ClassReflection - encapsulates access to java.lang.Class   Other classes included in the wrapper provide access to Constructors, Fields, and Methods. These classes (for the most part) mirror their java.lang.reflect equivalent. and can be used in the same way.   Usage   In general, you will use the reflection wrapper the same way you would use Java’s reflection API, except you’d route the calls through the appropriate wrapper class instead of calling the methods directly.   Examples:                  Operation       Java       Wrapper                       Create a new instance of an array of a specified component type       Array.newInstance(clazz, size)       ArrayReflection.newInstance(clazz, size)                 Obtain the Class object for a class by name       Class.forName(\"java.lang.Object\")       ClassReflection.forName(\"java.lang.Object\")                 Create a new instance of a class       clazz.newInstance()       ClassReflection.newInstance(clazz)                 Get the fields of a class       clazz.getFields()       ClassReflection.getFields(clazz)           GWT   Because GWT does not allow for reflection in the same way as Java, extra steps are required to make reflection information available to your GWT application. In short, you must specify which classes you plan to use with reflection. When compiling the HTML project, libGDX takes that information and generates a reflection cache containing information about and providing access to the constructors, fields and methods of the specified classes. libGDX then uses this reflection cache to implement the reflection api.   Classes are specified by including a special configuration property in your GWT module definition (*.gwt.xml).   To include a single class:  &lt;extend-configuration-property name=\"gdx.reflect.include\" value=\"com.me.reflected.ReflectedClass\" /&gt;   To include an entire package:  &lt;extend-configuration-property name=\"gdx.reflect.include\" value=\"com.me.reflected\" /&gt;   You can also exclude classes of packages (for example when you include a package, but you don’t want all classes or subpackages in that package):  &lt;extend-configuration-property name=\"gdx.reflect.exclude\" value=\"com.me.reflected.NotReflectedClass\" /&gt;   Notes     You must specify the fully qualified name of the class or package.   You must specify each class or package in its own extend-configuration-property element.   Any classes referenced by those classes you include will automatically be included, so you need only include your own classes.   Nested classes cannot be included directly. If, e.g., you want to have CustomActor$CustomActorStyle available for reflection (maybe to be used in uiskin.json), just include the parent class (i.e. CustomActor in this example)   Visibility restrictions may cause the compiler to not include your class in the IReflectionCache. Public visibility is therefore recommended.   static fields cannot be accessed directly via field.get(Example.class);, instance must be passed in as a parameter field.get(new Example());  ",
        
        "url": "/wiki/utils/reflection" },{
        "title": "Release Process",
        "excerpt":
        
        "Note: This page is outdated and needs to be updated!      Setup pgp keys and settings.xml for Maven release, ask Mario   Disable the libGDX project on the build server   pull in the latest changes into your local libGDX repository   pull in the latest nightlies via ant -f fetch.xml   Make sure Version.java has the release version in it   Modify DependencyBank.java, update the libGDX version (and snapshot version) to the latest (must match Version.java),   mvn release:clean, this should print BUILD SUCCESSFUL, if not, you’ve done something wrong   mvn release:prepare -DdryRun=true to test the prepare, see http://www.jroller.com/robertburrelldonkin/entry/apache_using_dry_run_with   mvn release:prepare, enter the release version number, then make sure the new snapshot version is x.y.z+1 or x.y+1.0 if you released an API breaking change   mvn release:perform, pray that everything works   Log into http://oss.sonatype.com (ask Mario for username:password) and do what’s described at https://docs.sonatype.org/display/Repository/Sonatype+OSS+Maven+Repository+Usage+Guide#SonatypeOSSMavenRepositoryUsageGuide-8a.ReleaseIt   Enable the libGDX project on the build server and wait for the libGDX to finish (this will also updated gdx-setup.jar on the server, which will use the latest libGDX version we just deployed to sonatype/maven central)   Download nightly, rename to libgdx-x.y.z, upload to website /usr/share/nginx/html/releases   Increase the libGDX version in Version.java to match the new snapshot version   Update the latest libGDX release version in forum for user registration   If something fails  If mvn release:prepare fails  Fix the issue! Do not continue. If you do, you’ll be in for a world of pain.   Issue #1: Javadoc for an artifact wasn’t build box2d-GWT had no source files in the src/ directory, only in the emulation directory. In such cases, place a dummy class in the src/ directory.   If mvn release:perform or closing and releasing the staging repository on Sonatype fails  Great, now you have a tag in Git and the versions in your pom.xml files are all kinds of fucked up! Do this:      Open all pom.xml files (find . | grep pom.xml for a list) and change the versions from x.y.z-SNAPSHOT xold.yold.zold-SNAPSHOT. E.g. if you failed to release 1.0.0, set the versions to 1.0.0-SNAPSHOT, just like before the release. You’ll only have to change the version of the root pom.xml, and the versions specified for the parent in the other pom.xml files.   git tag -d x.y.z, git push origin :x.y.z to kill the motherfucking tag in the master repository   Drop the staging repository on SonaType if necessary   Start from the beginning, good luck  ",
        
        "url": "/wiki/misc/release-process" },{
        "title": "Rendering shapes",
        "excerpt":
        
        "ShapeRenderer API (Javadoc provides an example in the top level!)   What can the ShapeRenderer do?   You can use ShapeRenderer to draw simple shapes. Types of shapes include rectangle and ellipse. The shapes can be outlined or filled. You can also set the color to use for each shape draw. It is very similar to use like sprite batch.   Example code that is taken from an actual project:   public class MyGame extends Game {         public final static float WIDTH = 100;         public final static float HEIGHT = 16 * WIDTH / 9;          FitViewport viewport; \tOrthographicCamera camera;         ShapeRenderer shape;          @Override \tpublic void create () { \t\tshape = new ShapeRenderer(); \t\tcamera = new OrthographicCamera(); \t\tviewport = new FitViewport(WIDTH, HEIGHT, camera);                 ...         }          @Override \tpublic void resize(int width, int height) { \t\tviewport.update(width, height); \t}          ... }  public class Box {         ... }  public class ScreenPlay implements Screen {         final Game g;         Array&lt;Box&gt; boxes;         ...                  @Override \tpublic void render(float delta) {               for(Box box : boxes) { \t\t\tg.shape.setProjectionMatrix(g.camera.combined); \t\t\tg.shape.begin(ShapeType.Line); \t\t\tg.shape.setColor(Color.RED); \t\t\tg.shape.rect(box.x, box.y, box.width, box.height); \t\t\tg.shape.end(); \t\t\t \t\t\tg.shape.setProjectionMatrix(g.camera.combined); \t\t\tg.shape.begin(ShapeType.Filled); \t\t\tg.shape.setColor(Color.BLUE); \t\t\tg.shape.ellipse(box.x, box.y, box.width, box.height); \t\t\tg.shape.end(); \t\t}\t         }          ...  }   Alternatives   One of the downsides of using a ShapeRenderer is that it uses its own Mesh, meaning if you want to alternate between ShapeRenderer and a Batch you need to start and end (and flush) each one before switching, which can significantly drop performance. An alternative is to use the third party library ShapeDrawer, which uses a user-provided Batch to draw shapes. It has most of the functionality of ShapeRenderer, plus some extras like line joining/bevelling. It also doesn’t draw over itself when drawing shapes so it can be used with transparent colours, and doesn’t need to be flushed when switching from drawing lines to filled shapes.   Typical usage looks something like this:   // batch drawing shapeDrawer.setColor(Color.RED); shapeDrawer.line(0, 0, 100, 100); // batch drawing shapeDrawer.setColor(Color.BLUE); shapeDrawer.filledCircle(50, 50, 20); // batch drawing  ",
        
        "url": "/wiki/graphics/opengl-utils/rendering-shapes" },{
        "title": "Saved game serialization",
        "excerpt":
        
        "JSON serialization   The JSON class can automatically convert Java objects to and from JSON.   Binary serialization   Kryo can be used to automatically and efficiently serialize game state.   libGDX Kryo Serializers   Kryo can handle most POJOs and other classes, but some classes need special handling. Below are a few serializers for libGDX classes.   Note that classes like Texture should not be serialized in most cases. It would be better to have a String instead of a Texture object in your object graph. After serializing you would process the objects and look up the texture using the string path.   kryo.register(Array.class, new Serializer&lt;Array&gt;() { \t{ \t\tsetAcceptsNull(true); \t}  \tprivate Class genericType;  \tpublic void setGenerics (Kryo kryo, Class[] generics) { \t\tif (generics != null &amp;&amp; kryo.isFinal(generics[0])) genericType = generics[0]; \t\telse genericType = null; \t}  \tpublic void write (Kryo kryo, Output output, Array array) { \t\tint length = array.size; \t\toutput.writeInt(length, true); \t\tif (length == 0) { \t\t\tgenericType = null; \t\t\treturn; \t\t} \t\tif (genericType != null) { \t\t\tSerializer serializer = kryo.getSerializer(genericType); \t\t\tgenericType = null; \t\t\tfor (Object element : array) \t\t\t\tkryo.writeObjectOrNull(output, element, serializer); \t\t} else { \t\t\tfor (Object element : array) \t\t\t\tkryo.writeClassAndObject(output, element); \t\t} \t}  \tpublic Array read (Kryo kryo, Input input, Class&lt;Array&gt; type) { \t\tArray array = new Array(); \t\tkryo.reference(array); \t\tint length = input.readInt(true); \t\tarray.ensureCapacity(length); \t\tif (genericType != null) { \t\t\tClass elementClass = genericType; \t\t\tSerializer serializer = kryo.getSerializer(genericType); \t\t\tgenericType = null; \t\t\tfor (int i = 0; i &lt; length; i++) \t\t\t\tarray.add(kryo.readObjectOrNull(input, elementClass, serializer)); \t\t} else { \t\t\tfor (int i = 0; i &lt; length; i++) \t\t\t\tarray.add(kryo.readClassAndObject(input)); \t\t} \t\treturn array; \t} });  kryo.register(IntArray.class, new Serializer&lt;IntArray&gt;() { \t{ \t\tsetAcceptsNull(true); \t}  \tpublic void write (Kryo kryo, Output output, IntArray array) { \t\tint length = array.size; \t\toutput.writeInt(length, true); \t\tif (length == 0) return; \t\tfor (int i = 0, n = array.size; i &lt; n; i++) \t\t\toutput.writeInt(array.get(i), true); \t}  \tpublic IntArray read (Kryo kryo, Input input, Class&lt;IntArray&gt; type) { \t\tIntArray array = new IntArray(); \t\tkryo.reference(array); \t\tint length = input.readInt(true); \t\tarray.ensureCapacity(length); \t\tfor (int i = 0; i &lt; length; i++) \t\t\tarray.add(input.readInt(true)); \t\treturn array; \t} });  kryo.register(FloatArray.class, new Serializer&lt;FloatArray&gt;() { \t{ \t\tsetAcceptsNull(true); \t}  \tpublic void write (Kryo kryo, Output output, FloatArray array) { \t\tint length = array.size; \t\toutput.writeInt(length, true); \t\tif (length == 0) return; \t\tfor (int i = 0, n = array.size; i &lt; n; i++) \t\t\toutput.writeFloat(array.get(i)); \t}  \tpublic FloatArray read (Kryo kryo, Input input, Class&lt;FloatArray&gt; type) { \t\tFloatArray array = new FloatArray(); \t\tkryo.reference(array); \t\tint length = input.readInt(true); \t\tarray.ensureCapacity(length); \t\tfor (int i = 0; i &lt; length; i++) \t\t\tarray.add(input.readFloat()); \t\treturn array; \t} });  kryo.register(Color.class, new Serializer&lt;Color&gt;() { \tpublic Color read (Kryo kryo, Input input, Class&lt;Color&gt; type) { \t\tColor color = new Color(); \t\tColor.rgba8888ToColor(color, input.readInt()); \t\treturn color; \t}  \tpublic void write (Kryo kryo, Output output, Color color) { \t\toutput.writeInt(Color.rgba8888(color)); \t} });  ",
        
        "url": "/wiki/utils/saved-game-serialization" },{
        "title": "Scene2d.ui",
        "excerpt":
        
        "Overview   scene2d is libGDX’s 2D scene graph. At its core, it provides basic 2D scene graph functionality: actors, groups, drawing, events, and actions. This is a lot of utility that applications can leverage, but it is reasonably low level. For games this is fine because most actors are application specific. For building UIs, the scene2d.ui package provides common UI widgets and other classes built on top of scene2d.   It is highly recommended to read or least skim the scene2d documentation before continuing.   Check out libGDX.info for examples showcasing Scene2d actors, scenes, Stages Images etc..      Widget and WidgetGroup   Layout   Stage setup   Skin   Drawable   ChangeEvents   Clipping   Rotation and scale   Layout widgets            Table       Container       Stack       ScrollPane       SplitPane       Tree       VerticalGroup       HorizontalGroup           Widgets            Label       Image       Button       TextButton       ImageButton       CheckBox       ButtonGroup       TextField       TextArea       List       SelectBox       ProgressBar       Slider       Window       Touchpad       Dialog           Widgets without scene2d.ui   Drag and Drop   Usage without touch or mouse   Examples   Widget and WidgetGroup   UIs often have many UI widgets to be sized and positioned on the screen. Doing this manually is time consuming, makes code difficult to read and maintain, and doesn’t easily adapt to different screen sizes. The Layout interface defines methods that allow for more intelligent layout for actors.   The Widget and WidgetGroup classes extend Actor and Group respectively, and they both implement Layout. These two classes are the basis for actors that will participate in layout. UI widgets should extend WidgetGroup if they have child actors, otherwise they should extend Widget.   Layout   UI widgets do not set their own size and position. Instead, the parent widget sets the size and position of each child. Widgets provide a minimum, preferred, and maximum size that the parent can use as hints. Some parent widgets, such as Table and Container, can be given constraints on how to size and position the children. To give a widget a specific size in a layout, the widget’s minimum, preferred, and maximum size are left alone and size constraints are set in the parent.   Before each widget is drawn, it first calls validate. If the widget’s layout is invalid, its layout method will be called so that the widget (and any child widgets) can cache information needed for drawing at their current size. The invalidate and invalidateHierarchy methods both invalidate the layout for a widget.   invalidate should be called when the widget’s state has changed and the cached layout information needs to be recalculated, but the widget’s minimum, preferred, and maximum size are unaffected. This means that the widget needs to be laid out again, but the widget’s desired size hasn’t changed so the parent is unaffected.   invalidateHierarchy should be called when the widget’s state has changed that affects the widget’s minimum, preferred, or maximum size. This means that the parent’s layout may be affected by the widget’s new desired size. invalidateHierarchy calls invalidate on the widget and every parent up to the root.   Stage setup   Most scene2d.ui layouts will use a table that is the size of the stage. All other widgets and nested tables are placed in this table.   Here is an example of the most basic scene2d.ui application with a root table:   private Stage stage; private Table table;  public void create () { \tstage = new Stage(); \tGdx.input.setInputProcessor(stage);  \ttable = new Table(); \ttable.setFillParent(true); \tstage.addActor(table);  \ttable.setDebug(true); // This is optional, but enables debug lines for tables.  \t// Add widgets to the table here. }  public void resize (int width, int height) { \tstage.getViewport().update(width, height, true); }  public void render () { \tGdx.gl.glClear(GL20.GL_COLOR_BUFFER_BIT); \tstage.act(Gdx.graphics.getDeltaTime()); \tstage.draw(); }  public void dispose() { \tstage.dispose(); }   Note that setFillParent is used on the root table, causing it to be sized to its parent (in this case, the stage) when validated. Normally a widget’s size is set by its parent and setFillParent must not be used. setFillParent is for convenience only when the widget’s parent does not set the size of its children (such as the stage).   Tables automatically adapt to various screen resolutions, so this sets up a stage that uses pixel coordinates. See stage viewport setup for setting up a stage that scales.   Skin   Most UI widgets are made up of a few configurable resources: images, fonts, colors, etc. All the resources needed to render a widget is called a “style”. Each widget defines its own style class (usually a static member class) and has constructors for setting the initial style and a setStyle method for changing the style later.   Skin files from the libGDX tests can be used as a starting point. You will need: uiskin.png, uiskin.atlas, uiskin.json, and default.fnt. This enables you to quickly get started using scene2d.ui and replace the skin assets later.   Styles can be configured using JSON or with code:   TextureRegion upRegion = ... TextureRegion downRegion = ... BitmapFont buttonFont = ...  TextButtonStyle style = new TextButtonStyle(); style.up = new TextureRegionDrawable(upRegion); style.down = new TextureRegionDrawable(downRegion); style.font = buttonFont;  TextButton button1 = new TextButton(\"Button 1\", style); table.add(button1);  TextButton button2 = new TextButton(\"Button 2\", style); table.add(button2);   Note the same style can be used for multiple widgets. Also note that all images needed by UI widgets are actually implementations of the Drawable interface.   The Skin class can be used to more conveniently define the styles and other resources for UI widgets. See the Skin documentation for more information. It is very strongly recommended to use Skin for convenience, even if not defining styles via JSON.   Drawable   The Drawable interface provides a draw method that takes a SpriteBatch, position, and size. The implementation can draw anything it wants: texture regions, sprites, animations, etc. Drawables are used extensively for all images that make up widgets. Implementations are provided to draw texture regions, sprites, nine patches, and to tile a texture region. Custom implementations can draw anything they like.   Drawable provides a minimum size, which can be used as a hint for the smallest it should be drawn. It also provides top, right, bottom, and left sizes, which can be used as a hint for how much padding should be around content drawn on top of the drawable.   By default, NinePatchDrawable uses the top, right, bottom, and left sizes of the corresponding nine patch texture regions. However, the drawable sizes are separate from the nine patch sizes. The drawable sizes can be changed to draw content on the nine patch with more or less padding than the actual nine patch regions.   Creating drawables is very common and somewhat tedious. It is recommended to use the skin methods to automatically obtain a drawable of the appropriate type.   ChangeEvents   Most widgets fire a ChangeEvent when something changes. This is a generic event, what actually changed depends on each widget. Eg, for a button the change is that the button was pressed, for a slider the change is the slider position, etc.   ChangeListener should be used to detect these events:   actor.addListener(new ChangeListener() { \tpublic void changed (ChangeEvent event, Actor actor) { \t\tSystem.out.println(\"Changed!\"); \t} });   ChangeListener should be used when possible instead of ClickListener, eg on buttons. ClickListener reacts to input events on the widget and only knows if the widget has been clicked. The click will still be detected if the widget is disabled and doesn’t handle the widget being changed by a key press or programmatically. Also, for most widgets the ChangeEvent can be cancelled, allowing the widget to revert the change.   Clipping   Clipping is most easily done using setClip(true) on a Table. Actors in the table will be clipped to the table’s bounds. Culling is done so actors completely outside of the table’s bounds are not drawn at all.   Actors added to a table using the add Table methods get a table cell and will be sized and positioned by the table. Like any group, actors can still be added to a table using the addActor method. The table will not size and position actors added this way. This can be useful when using a table solely for clipping.   Rotation and scale   As described previously, a scene2d group that has transform enabled causes a SpriteBatch flush before drawing its children. A UI often has dozens, if not hundreds, of groups. Flushing for each group would severely limit performance, so most scene2d.ui groups have transform set to false by default. Rotation and scale is ignored when the group’s transform is disabled.   Transforms can be enabled as needed, with some caveats. Not all widgets support all features when rotation or scaling is applied. Eg, transform can be enabled for a Table and then it can be rotated and scaled. Children will be drawn rotated and scaled, input is routed correctly, etc. However, other widgets may perform drawing without taking rotation and/or scale into account. A workaround for this problem is to wrap a widget in a table or container with transform enabled and set the rotation and scale on the table or container, not on the widget:   TextButton button = new TextButton(\"Text Button\", skin); Container wrapper = new Container(button); wrapper.setTransform(true); wrapper.setOrigin(wrapper.getPrefWidth() / 2, wrapper.getPrefHeight() / 2); wrapper.setRotation(45); wrapper.setScaleX(1.5f);   Note that scene2d.ui groups that perform layout, such as Table, will use the unscaled and unrotated bounds of a transformed widget when computing the layout.   Widgets that perform clipping, such as ScrollPane, use glScissor which uses a screen aligned rectangle. These widgets cannot be rotated.   If excessive batch flushes are occurring due to transform being enabled on many groups, CpuSpriteBatch can be used for the stage. This does transformations using the CPU to avoid flushing the batch.   Layout widgets   Table   The Table class (code) sizes and positions its children using a logical table, similar to HTML tables. Tables are intended to be used extensively in scene2d.ui to layout widgets, as they are easy to use and much more powerful than manually sizing and positioning widgets. Table-based layouts don’t rely on absolute positioning and therefore automatically adjust to different widget sizes and screen resolutions.   It is highly recommended to read the Table documentation before building a UI using scene2d.ui.   Container   The Container class (code) is equivalent to a Table with only a single child, but is more lightweight. Container has all of the constraints of a table cell and is useful for setting the size and alignment of a single widget. If you implement with Scene2D, Container presents to you a much better personal development experience if you encounter trouble adjusting values for alignments and sizes for whatever an object holds and supports. While the advantages of this class generally work best within a Table object, you may still find a benefit in certain scenarios even outside of such code.   Stack   Stack (code) is a WidgetGroup that lays out each child to be the size of the stack. This is useful when it is necessary to have widgets stacked on top of each other. The first widget added to the stack is drawn on the bottom, and the last widget added is drawn on the top.   ScrollPane   ScrollPane (code) scrolls a child widget using scrollbars and/or mouse or touch dragging. Scrolling is automatically enabled when the widget is larger than the scroll pane. If the widget is smaller than the scroll pane in one direction, it is sized to the scroll pane in that direction. ScrollPane has many settings for if and how touches control scrolling, fading scrollbars, etc. ScrollPane has drawables for the background, horizontal scrollbar and knob, and vertical scrollbar and knob. If touches are enabled (the default), all the drawables are optional.   Note: ScrollPane doesn’t support well children that dynamically change their size or move while dragging them. Having a children in your scrollpane that move while being dragged will make the ScrollPane flicker.   SplitPane   SplitPane (code) contains two widgets and is divided in two either horizontally or vertically. The user may resize the widgets with a draggable splitter. The child widgets are always sized to fill their half of the splitpane. SplitPane has a drawable for the draggable splitter.   Tree   Tree (code) displays a hierarchy of nodes. Each node may have child nodes and can be expanded or collapsed. Each node has an actor, allowing complete flexibility over how each item is displayed. Tree has drawables for the plus and minus icons next to each node’s actor.   VerticalGroup   A VerticalGroup (code) is equivalent to a Table with only a single column, but is more lightweight. VerticalGroup allows widgets to be inserted in the middle and removed, while Table does not.   HorizontalGroup   A HorizontalGroup (code) is equivalent to a Table with only a single row, but is more lightweight. HorizontalGroup allows widgets to be inserted in the middle and removed, while Table does not.   Widgets   Label   Label (code) displays text using a bitmap font and a color. The text may contain newlines. Word wrap may be enabled, in which case the width of the label should be set by the parent. The lines of text can be aligned relative to each other, and also all of the text aligned within the label widget.   The labels that use the same font will be the same size, though they may have different colors. Bitmap fonts don’t typical scale well, especially at small sizes. It is suggested to use a separate bitmap font for each font size. The bitmap font image should be packed into the skin’s atlas to reduce texture binds.   To create a label which contain text with different colors, the BitmapFont used to create the label should have markup enabled by setting it in the BitmapFont.BitmapFontData object passed via the constructor. If you’re using the AssetManager, you can pass this data by passing an appropriate BitmapFontParameter object when you call load.   Image   Image (code) simply displays a drawable. The drawable can be a texture, texture region, ninepatch, sprite, etc. The drawable may be scaled and aligned within the image widget bounds in various ways.   For a tutorial on using Image (Create, rotate, resize and creating images with repeating texture see this Image tutorial)   Button   Button (code) by itself it is just an empty button, but it extends table so other widgets can be added to it. It has an up background that is normally displayed, and a down background that is displayed when pressed. It has a checked state which is toggled each time it is clicked, and when checked it will use the checked background instead of up, if defined. It also has pressed/unpressed offsets, which offset the entire button contents when pressed/unpressed.   TextButton   TextButton (code) extends Button and contains a label. TextButton adds to Button a bitmap font and a colors for the text in the up, down, and checked states.   TextButton extends Button which extends Table, so widgets can be added to the TextButton using the Table methods.   ImageButton   ImageButton (code) extends Button and contains an image widget. ImageButton adds to Button a drawables for the image widget in the up, down, and checked states.   Note that ImageButton extends Button, which already has a background drawable for the up, down, and checked states. ImageButton is only needed when it is desired to have a drawable (such as an icon) on top of the button background.   ImageButton extends Button which extends Table, so widgets can be added to the ImageButton using the Table methods.   CheckBox   CheckBox (code) extends TextButton and adds an image widget to the left of the label. It has a drawable for the image widget for the checked and unchecked states.   ButtonGroup   ButtonGroup (code) is not an actor and has no visuals. Buttons are added to it and it enforces a minimum and maximum number of checked buttons. This allows for buttons (button, text button, checkbox, etc) to be used as “radio” buttons.   TextField   TextField (code) is a single line text entry field. It has drawables for the background, text cursor, and text selection, a font and font color for the entered text, and a font and font color for the message displayed when the text field is empty. Password mode can be enabled, where it will display asterisks instead of the entered text.   TextArea   TextArea (code) is similiar to a TextField, but allows multiple line text entry.   List   List (code) is a list box that displays textual items and highlights the selected item. List has a font, selected item background drawable, and a font color for selected and unselected items. A list does not scroll on its own, but is often put in a scrollpane.   SelectBox   SelectBox (code) is a drop-down list, it allows one of a number of values to be chosen from a list. When inactive, the selected value is displayed. When activated, it shows the list of values that may be selected. SelectBox has drawables for the background, list background, and selected item background, a font and font color, and a setting for how much spacing is used between items.   ProgressBar   ProgressBar (code) is a widget that visually displays the progress of some activity or a variable value within a given range. The progress bar has a range (min, max) and a stepping between each value it represents. The percentage of completeness typically starts out as an empty progress bar and gradually becomes filled in as the task or value increases towards upper limit.   A progress bar can be setup to be of horizontal or vertical orientation, although the increment direction is always the same. For horizontal progress bar, is grows to the right, for vertical, upwards. Animation for changes to the progress bar value can be enabled to make the bar fill more smoothly over time.   Slider   Slider (code) is a horizontal indicator that allows a user to set a value. The slider has a range (min, max) and a stepping between each value the slider represents. Slider has drawables for the background, the slider knob, and for the portion of the slider before and after the knob.   A slider with touches disabled, a drawable before the knob, and without the knob can be used as a substitute for progress bar, as both widgets share the same codebase and use same visual style. Animation for changes to the slider value can be enabled to make the progress bar fill more smoothly.   Window   Window (code) is a table with a title bar area above the contents that displays a title. It can optionally act as a modal dialog, preventing touch events to widgets below. Window has a background drawable and a font and font color for the title.   Touchpad   Touchpad (code) is an onscreen joystick that moves in a circular area. It has a background drawable and a drawable for the knob that the user drags around. If you want to implement a “follow mode” for your joystick element, check out the example here.   Dialog   Dialog (code) is a window with a content table and a button table underneath. Anything can be added to the dialog, but convenience methods are provided to add a label to the content table and buttons to the button table.   Widgets without scene2d.ui   Widgets can be used as simple actors in scene2d, without using tables or the rest of scene2d.ui. Widgets have a default size and can be positioned absolutely, the same as any actor. If a widget’s size is changed, the invalidate Widget method must be called so the widget will relayout at the new size.   Some widgets, such as Table, don’t have a default size after construction because their preferred size is based on the widgets they will contain. After the widgets have been added, the pack method can be used to set the width and height of the widget to its preferred width and height. This method also calls invalidate if the widget’s size was changed, and then calls validate so that the widget adjusts itself to the new size.   Drag and Drop (DragAndDrop class)  It should be noted that to make a drag start/source actor, a table and to have that table and all of its contents trigger a drag, one must enable Table.setTouchable(Enabled). It is set to ChildrenOnly by default.   Usage without touch or mouse   Scene2d.ui is mainly designed with touch or mouse control in mind. Stage has a setKeyboardFocus and a setScrollFocus methods to set the Actor receiving scroll and key events. However, it does not support a full type focus and is therefore not operable with keys only. A focusing system is needed for games designed with controller-only or keyboard-only interface. gdx-controllerutils project’s scene2d module includes a ControllerMenuStage adding this to scene2d.ui. View the documentation.   Examples   For now, please see these test programs:      UISimpleTest   TableLayoutTest   UITest   ImageTest   LabelTest   ScrollPaneTest  ",
        
        "url": "/wiki/graphics/2d/scene2d/scene2d-ui" },{
        "title": "Scene2d",
        "excerpt":
        
        "Overview   scene2d is a 2D scene graph for building applications and UIs using a hierarchy of actors. If you’re looking for the UI component of scene2d, see Scene2d.ui   Check out LibGDX.info for Scene2d examples (Image, Label etc…)   It provides the following features:           Rotation and scale of a group is applied to all child actors. Child actors always work in their own coordinate system, parent transformations are applied transparently.            Simplified 2D drawing via SpriteBatch. Each actor draws in its own un-rotated and unscaled coordinate system where 0,0 is the bottom left corner of the actor.            Hit detection of rotated and scaled actors. Each actor determines if it is hit using its own un-rotated and unscaled coordinate system.            Routing of input and other events to the appropriate actor. The event system is flexible, enabling parent actors to handle events before or after children.            Action system for easy manipulation of actors over time. Actions can be chained and combined for complex effects.       scene2d is well equipped for laying out, drawing, and handling input for game menus, HUD overlays, tools, and other UIs. The scene2d.ui package provides many actors and other utilities specifically for building UIs.   Scene graphs have the drawback that they couple model and view. Actors store data that is often considered model data in games, such as their size and position. Actors are also the view, as they know how to draw themselves. This coupling makes MVC separation difficult. When used solely for UIs or for apps that don’t care about MVC, the coupling is not an issue.   scene2d has three classes at its core:           The Actor class is a node in the graph which has a position, rectangular size, origin, scale, rotation, and color.            The Group class is an actor that may have child actors.            The Stage class has a camera, SpriteBatch, and a root group and handles drawing the actors and distributing input events.       Stage   Stage is an InputProcessor. When it receives input events, it fires them on the appropriate actors. If the stage is being used as a UI on top of other content (eg, a HUD), an InputMultiplexer can be used to first give the stage a chance to handle an event. If an actor in the stage handles an event, stage’s InputProcessor methods will return true, indicating the event has been handled and should not continue on to the next InputProcessor.   Stage has an act method that takes a delta time since last frame. This causes the act method on every actor in the scene to be called, allowing the actors to take some action based on time. By default, the Actor act method updates all actions on the actor. Calling act on the stage is optional, but actor actions and enter/exit events will not occur if it is omitted.   Viewport   The stage’s viewport is determined by a Viewport instance. The viewport manages a Camera and controls how the stage is displayed on the screen, the stage’s aspect ratio (whether it is stretched) and whether black bars appear (letterboxing). The viewport also converts screen coordinates to and from stage coordinates.   The viewport is specified in the stage constructor or by using setViewport. If running where the application window can be resized (eg, on the desktop), the stage’s viewport should be set when the application window is resized.   Here is an example of the most basic scene2d application with no actors, using a ScreenViewport. With this viewport, each unit in the stage corresponds to 1 pixel. This means the stage is never stretched, but more or less of the stage is visible depending on the size of the screen or window. This is often useful for UI applications.   private Stage stage;  public void create () { \tstage = new Stage(new ScreenViewport()); \tGdx.input.setInputProcessor(stage); }  public void resize (int width, int height) { \t// See below for what true means. \tstage.getViewport().update(width, height, true); }  public void render () { \tfloat delta = Gdx.graphics.getDeltaTime(); \tGdx.gl.glClear(GL20.GL_COLOR_BUFFER_BIT); \tstage.act(delta); \tstage.draw(); }  public void dispose () { \tstage.dispose(); }   Passing true when updating the viewport changes the camera position so it is centered on the stage, making 0,0 the bottom left corner. This is useful for UIs, where the camera position is not usually changed. When managing the camera position yourself, pass false or omit the boolean. If the stage position is not set, by default 0,0 will be in the center of the screen.   Here is an example of using StretchViewport. The stage’s size of 640x480 will be stretched to the screen size, potentially changing the stage’s aspect ratio.   \tstage = new Stage(new StretchViewport(640, 480));   Here is an example of using FitViewport. The stage’s size of 640x480 is scaled to fit the screen without changing the aspect ratio, then black bars are added on either side to take up the remaining space (letterboxing).   \tstage = new Stage(new FitViewport(640, 480));   Here is an example of using ExtendViewport. The stage’s size of 640x480 is first scaled to fit without changing the aspect ratio, then the stage’s shorter dimension is increased to fill the screen. The aspect ratio is not changed and there are no black bars, but the stage may be longer in one direction.   \tstage = new Stage(new ExtendViewport(640, 480));   Here is an example of using ExtendViewport with a maximum size. As before, the stage’s size of 640x480 is first scaled to fit without changing the aspect ratio, then the stage’s shorter dimension is increased to fill the screen. However, the stage’s size won’t be increased beyond the maximum size of 800x480. This approach allows you to show more of the world to support many different aspect ratios without showing black bars.   \tstage = new Stage(new ExtendViewport(640, 480, 800, 480));   Most viewports that show black bars use glViewport, so the stage cannot draw within the black bars. The glViewport can be set to the full screen to draw in the black bars outside of the stage.   // Set the viewport to the whole screen. Gdx.gl.glViewport(0, 0, Gdx.graphics.getWidth(), Gdx.graphics.getHeight());  // Draw anywhere on the screen.  // Restore the stage's viewport. stage.getViewport().update(Gdx.graphics.getWidth(), Gdx.graphics.getHeight(), true);   See Viewport for more information.   Drawing   When draw is called on the stage, it calls draw on every actor in the stage. Actors’ draw method can be overridden to perform drawing:   public class MyActor extends Actor { \tTextureRegion region;  \tpublic MyActor () { \t\tregion = new TextureRegion(...);                 setBounds(region.getRegionX(), region.getRegionY(), \t\t\tregion.getRegionWidth(), region.getRegionHeight()); \t}  \t@Override \tpublic void draw (Batch batch, float parentAlpha) { \t\tColor color = getColor(); \t\tbatch.setColor(color.r, color.g, color.b, color.a * parentAlpha); \t\tbatch.draw(region, getX(), getY(), getOriginX(), getOriginY(), \t\t\tgetWidth(), getHeight(), getScaleX(), getScaleY(), getRotation()); \t} }   This draw method draws a region using the position, origin, size, scale, and rotation of the actor. The Batch passed to draw is configured to draw in the parent’s coordinates, so 0,0 is the bottom left corner of the parent. This makes drawing simple, even if the parent is rotated and scaled. Batch begin has already been called. If the parentAlpha is combined with this actor’s alpha as shown, child actors will be influenced by the parent’s translucency. Note the color of the Batch may be changed by other actors and should be set by each actor before it draws.   If setVisible(false) is called on an actor, its draw method will not be called. It will also not receive input events.   If an actor needs to perform drawing differently, such as with a  ShapeRenderer, the Batch should be ended and then begun again at the end of the method. Of course, this causes the batch to be flushed, so should be used judiciously. The transformation and projection matrices from the Batch can be used:   private ShapeRenderer renderer = new ShapeRenderer();  public void draw (Batch batch, float parentAlpha) { \tbatch.end();  \trenderer.setProjectionMatrix(batch.getProjectionMatrix()); \trenderer.setTransformMatrix(batch.getTransformMatrix()); \trenderer.translate(getX(), getY(), 0);  \trenderer.begin(ShapeType.Filled); \trenderer.setColor(Color.BLUE); \trenderer.rect(0, 0, getWidth(), getHeight()); \trenderer.end();  \tbatch.begin(); }   Group transform   When a group is rotated or scaled, the children draw as normal and the Batch’s transform draws them correctly rotated or scaled. Before a group draws, the Batch is flushed so the transform can be set. This flush may become a performance bottleneck if there are many dozens of groups. If the actors in a group are not rotated or scaled, then setTransform(false) can be used for the group. When this is done, each child’s position will be offset by the group’s position for drawing, causing the children to appear in the correct location even though the Batch has not been transformed. This cannot be used for a group that has rotation or scale.   Hit detection   The Actor hit method receives a point and returns the deepest actor at that point, or null if no actor was hit. Here is the default hit method:   public Actor hit (float x, float y, boolean touchable) { \tif (touchable &amp;&amp; getTouchable() != Touchable.enabled) return null; \treturn x &gt;= 0 &amp;&amp; x &lt; width &amp;&amp; y &gt;= 0 &amp;&amp; y &lt; height ? this : null; }   The coordinates are given in the actor’s coordinate system. This simply returns the actor if the point is inside the actor’s bounds. More sophisticated checks could be used, eg if the actor was round. The touchable boolean parameter indicates if the actor’s touchability should be respected. This enables hit detection for purposes other than touch on actors that are not touchable.   When hit is called on the stage, hit is called on the stage’s root group, which in turn calls hit on each child. The first non-null actor found is returned as the actor deepest in the hierarchy that contains the given point.   Event system   scene2d uses a generic event system. Each actor has a list of listeners that are notified for events on that actor. Events are propagated in two phases. First, during the “capture” phase an event is given to each actor from the root down to the target actor. Only capture listeners are notified during this phase. This gives parents a chance to intercept and potentially cancel events before children see them. Next, during the “normal” phase the event is given to each actor from the target up to the root. Only normal listeners are notified during this phase. This allows actors to handle an event themselves or let the parent have a try at it.   The event provided to each actor when it is notified contains state about the event. The target is the actor that the event originated on. The listener actor is the actor that the listener is attached to. The event also has a few important methods. If stop is called on the event, any remaining listeners for the current actor are still notified, but after those no other actors will receive the event. This can be used to prevent children (during the capture phase) or parents (during the normal phase) from seeing the event. If cancel is called on the event, it stops propagation the same as stop and also prevents any default action that would have been taken by the code that fired the event. E.g., if the event is for a check-box being checked, cancelling the event could prevent the check-box from being checked.   For example, imagine a group (a button) which has a child (a label). When the label is clicked, capture listeners are fired. Usually there are none. Next, the label’s normal listeners are notified. The label is both the target and the listener actor. If the event was not stopped, the button gets the event and its normal listeners are notified. The label is the target and the button is the listener actor. This continues up to the root. This system allows a single listener on a parent to handle events on its children.   InputListener   EventListeners are added to actors to be notified about events. EventListener is an interface with a handle(Event) method. Classes that implement the EventListener interface use instanceof to determine whether they should handle the event. For most types of events, specific listener classes are provided for convenience. For example, InputListener is provided for receiving and handling InputEvents. An actor just needs to add an InputListener to start receiving input events. InputListener has several methods that may be overridden, and two are shown below:   actor.setBounds(0, 0, texture.getWidth(), texture.getHeight());  actor.addListener(new InputListener() { \tpublic boolean touchDown (InputEvent event, float x, float y, int pointer, int button) { \t\tSystem.out.println(\"down\"); \t\treturn true; \t} \t \tpublic void touchUp (InputEvent event, float x, float y, int pointer, int button) { \t\tSystem.out.println(\"up\"); \t} });   Note that the actor must specify its bounds in order to receive input events within those bounds.   To handle touch and mouse events, override touchDown, touchDragged, and touchUp. The touchDragged and touchUp events will only be received if touchDown returns true. Also, the touchDragged and touchUp events will be received even if they do not occur over the actor. This simplifies the most common touch event use cases.   To handle enter and exit events when a touch and or the mouse enters or exits an actor, override enter or exit.   To handle mouse movement when no mouse button is down (which only occurs on the desktop), override the mouseMoved method.   To handle mouse scrolling (which only occurs on the desktop), override the scrolled method. This will only be called on the actor with scroll focus, which is set and cleared by calling setScrollFocus on the stage.   To handle key input, override the keyDown, keyUp, and keyTyped methods. These will only be called on the actor with keyboard focus, which is set and cleared by calling setKeyboardFocus on the stage.   If setTouchable(false) or setVisible(false) is called on an actor, it will not receive input events.   Other listeners   Other listeners are provided for common handling of input events. ClickListener has a boolean that is true when a touch or mouse button is pressed over the actor and a clicked method that is called when the actor is clicked. ActorGestureListener detects tap, longPress, fling, pan, zoom, and pinch gestures on an actor.   actor.addListener(new ActorGestureListener() { \tpublic boolean longPress (Actor actor, float x, float y) { \t\tSystem.out.println(\"long press \" + x + \", \" + y); \t\treturn true; \t}  \tpublic void fling (InputEvent event, float velocityX, float velocityY, int button) { \t\tSystem.out.println(\"fling \" + velocityX + \", \" + velocityY); \t}  \tpublic void zoom (InputEvent event, float initialDistance, float distance) { \t\tSystem.out.println(\"zoom \" + initialDistance + \", \" + distance); \t} });   Actions   Each actor has a list of actions. These are updated each frame by the Actor act method. Many types of actions are included with libgdx. These can be instantiated, configured, and added to an actor. When the action is complete, it will automatically be removed from the actor.   MoveToAction action = new MoveToAction(); action.setPosition(x, y); action.setDuration(duration); actor.addAction(action);   Check out LibGDX.info for a tutorial on Actions   Action pooling   To avoid allocating a new action each time it is needed, a pool can be used:   Pool&lt;MoveToAction&gt; pool = new Pool&lt;MoveToAction&gt;() { \tprotected MoveToAction newObject () { \t\treturn new MoveToAction(); \t} }; MoveToAction action = pool.obtain(); action.setPool(pool); action.setPosition(x, y); action.setDuration(duration); actor.addAction(action);   When the action is complete, it is removed from the actor and put back in the pool for reuse. The above code is quite verbose though. The Actions class (note the plural) provides convenience methods. It can provide pooled actions:   MoveToAction action = Actions.action(MoveToAction.class); action.setPosition(x, y); action.setDuration(duration); actor.addAction(action);   Even better, the Actions class has methods that return a pooled and configured action.   actor.addAction(Actions.moveTo(x, y, duration));   The Actions class has many of these convenience methods for completely configuring any of the built in actions using a single method call. This can be made even simpler by using a static import, which allows the static methods to be referenced on Actions without specifying “Actions.” each time. Note that Eclipse will not add a static import for you, you must add it yourself.   import static com.badlogic.gdx.scenes.scene2d.actions.Actions.*; ... actor.addAction(moveTo(x, y, duration));   Complex actions   More complex actions can be built by running actions at the same time or in sequence. ParallelAction has a list of actions and runs them at the same time. SequenceAction has a list of actions and runs them one after another. Use of the static import for the Actions class makes defining complex actions very easy:   actor.addAction(sequence(moveTo(200, 100, 2), color(Color.RED, 6), delay(0.5f), rotateTo(180, 5)));   Action completion   To run code when an action is complete, a sequence with a RunnableAction can be used:   actor.addAction(sequence(fadeIn(2), run(new Runnable() { \tpublic void run () { \t\tSystem.out.println(\"Action complete!\"); \t} })));   Interpolation   The tweening curve can be set for actions that manipulate an actor over time. This is done by giving the action an instance of Interpolation. The Interpolation class has many static fields for convenience, or you can write your own. See InterpolationTest for an interactive demo of each interpolation.   MoveToAction action = Actions.action(MoveToAction.class); action.setPosition(x, y); action.setDuration(duration); action.setInterpolation(Interpolation.bounceOut); actor.addAction(action);   The Actions class has methods that can take an interpolation and a static import can be used to more easily access the Interpolation static fields. Here the bounceOut and swing interpolations are used:   import static com.badlogic.gdx.scenes.scene2d.actions.Actions.*; import static com.badlogic.gdx.math.Interpolation.*; ... actor.addAction(parallel(moveTo(250, 250, 2, bounceOut), color(Color.RED, 6), delay(0.5f), rotateTo(180, 5, swing))); actor.addAction(forever(sequence(scaleTo(2, 2, 0.5f), scaleTo(1, 1, 0.5f), delay(0.5f))));   External Links      netthreads A fully documented scene2d example game.   gdx-ui-app A library on top of scene2d for easier development.   Should I use scene2d for my game?   Street Race game tutorial  ",
        
        "url": "/wiki/graphics/2d/scene2d/scene2d" },{
        "title": "Set Up a Dev Env",
        "excerpt":
        
        "If this is your first time using libGDX, you’re at the right place. The following steps detail how you can get your first libGDX project up and running.   {% include setup_flowchart.html current=’0’ %}   Before you can get started with libGDX, you need to set up an IDE (Integrated Development Environment). It is basically an editor for your java files, which makes developing java applications considerably more convenient in various ways. If you already have an IDE installed, you can skip to the next step.   The java world offers a lot of different IDEs. All of them will have minor advantages and disadvantages, but in the end they all do their job, so feel free to choose whichever you like most.   (1.) Android Studio  For newcomers wanting to not only target desktop, but mobile platforms as well, we recommend Android Studio.      JDK: is provided by Android Studio   IDE itself: Android Studio   Android: is offered out-of-the-box   For iOS: RoboVM OSS IntelliJ plugin   (2.) IDEA          JDK 8+: there are different distributions, but Adoptium should fit your needs       Since Gradle does not support JDK 18 yet, libGDX projects will not work with it either. As a consequence, you are advised to use JDK 8-17!       IDE itself: IntelliJ IDEA (the “Community” edition is sufficient)   For Android: Android SDK   For iOS: RoboVM OSS IntelliJ plugin   (3.) Eclipse          JDK 8+: there are different distributions, but Adoptium should fit your needs       Since Gradle does not support JDK 18 yet, libGDX projects will not work with it either. As a consequence, you are advised to use JDK 8-17!       IDE itself: Eclipse   Android: not officially supported, but you may have success with Andmore or tinkering around with an older ADT version   For iOS: RoboVM OSS Eclipse plugin   (4.) Other IDEs  Of course you can also use any other IDE for Java, e.g. NetBeans, Visual Studio Code or even AIDE. However, as those are not commonly used in the libGDX community, it may prove difficult to get any help if IDE-specific issues arise!     NetBeans requires the NetBeans Gradle Plugin; Android and iOS are not officially supported   Visual Studio Code requires extensions to support Java; see the Coding Pack for Java; Android and iOS are not officially supported   AIDE only supports Android development; libGDX’s JAR files can be found here   (5.) No IDE  It is also possible to develop libGDX applications entirely without any IDE, just using a simple editor like Notepad or Vim. This is not recommended, because IDEs provide some very convenient features, such as code completion and error checking. However, if you insist on doing so: libGDX applications are Gradle applications, so they can be built and executed via the commandline.           JDK 8+: there are different distributions, but Adoptium should fit your needs       Since Gradle does not support JDK 18 yet, libGDX projects will not work with it either. As a consequence, you are advised to use JDK 8-17!       For Android: Android SDK   Set the ANDROID_HOME environment variable, or use gradle.properties       Now that you have a development environment, you can create your very first libGDX project. libGDX offers a setup tool for that, which generates all the necessary files. To get started with it, take a look here.  ",
        
        "url": "/wiki/start/setup" },{
        "title": "Shaders",
        "excerpt":
        
        "If you want to work with OpenGL ES 2.0, you should know some shader basics. libGDX comes with a standard shader that will take care of rendering things via SpriteBatch. However, if you’d like to render a Mesh in OpenGL ES 2.0 you will have to supply a valid shader yourself. Basically, in OpenGL ES 2.0, everything is rendered with shaders. That’s why it’s called a programmable pipeline.   The thought of dabbling around in shaders might scare some people away from using ES 2.0, but it’s well worth reading up on it as shaders allow you to do some pretty incredible things. And understanding the basics is actually quite straightforward.   What are shaders?   Shaders in OpenGL are little programs written in a C-like language called GLSL (OpenGL Shading Language) that runs on the GPU and processes the data necessary to render things. A shader can simply be viewed as a processing stage on the GPU. It receives a set of inputs which you can do a set of operations on and finally sends them back out. Think of this like function parameters and return values.   Typically, when rendering something in OpenGL ES 2.0 the data will be sent through the vertex shader first and then through the fragment shader.   Vertex shaders   As the name implies, vertex shaders are responsible for performing operations on vertices. More specifically, each execution of the program operates on exactly one vertex. This is an important concept to understand. Everything you do in the vertex shader happens only on exactly one vertex.   Here’s a simple vertex shader:   attribute vec4 a_position;  uniform mat4 u_projectionViewMatrix;  void main() {     gl_Position =  u_projectionViewMatrix * a_position; }    That doesn’t look too bad, now does it? First, you have a vertex attribute called a_position. This attribute is a vec4 which means it’s a vector with 4 dimensions. In this sample, it holds the position information of the vertex.   Next, you have the u_projectionViewMatrix. This is a 4x4 matrix that holds the view and projection transform data. If those terms sound fuzzy to you I’d recommend reading up on those topics here. It’s incredibly useful to understand it.   Inside the main method, we execute the operations on the vertex. In this case, all the shader does is multiply the vertex position with the matrix and assigns it to gl_Position. gl_Position is a predefined keyword by OpenGL and can’t be used for anything else but passing through the processed vertex.   Fragment shaders  A fragment shader functions in a very similar way to a vertex shader. But instead of processing it on a vertex it processes it once for each fragment. For simplicity’s sake think of a fragment as one pixel. Now, you might notice that this is a very significant difference.   Let’s say a triangle covers an area of 300 pixels. The vertex shader for this triangle would be executed 3 times. The fragment shader though would be executed 300 times. So when writing shaders, keep this in mind. Everything done in the fragment shader will be exponentially more expensive!   Here’s a very basic fragment shader:   void main() {     gl_FragColor = vec4(1.0, 0.0, 0.0, 1.0); }   This fragment shader will simply render every fragment with solid red. gl_FragColor is another pre-defined keyword. It’s used to output the final color for the fragment. Notice how we use vec4(x, y, z, w) to define a vector inside the shader. In this case, the vector is used to define the color of the fragment.   A simple ShaderProgram   Now that we have a basic understanding of what shaders do and how they work, let’s create one in libGDX. This is done with the ShaderProgram class. A ShaderProgram is made up of a vertex shader and a fragment shader. You can either load from a file or just pass in a string and keep the shader code inside your Java files.   This is the shader setup we’ll be working with:   String vertexShader = \"attribute vec4 a_position;\\n\" +                       \"attribute vec4 a_color;\\n\" +                       \"attribute vec2 a_texCoord0;\\n\" +                       \"uniform mat4 u_projTrans;\\n\" +                       \"varying vec4 v_color;\\n\" +                       \"varying vec2 v_texCoords;\\n\" +                       \"void main() {\\n\" +                           \"v_color = vec4(1, 1, 1, 1);\\n\" +                           \"v_texCoords = a_texCoord0;\\n\" +                           \"gl_Position =  u_projTrans * a_position;\\n\" +                       \"}\"; String fragmentShader = \"#ifdef GL_ES\\n\" +                         \"precision mediump float;\\n\" +                         \"#endif\\n\" +                         \"varying vec4 v_color;\\n\" +                         \"varying vec2 v_texCoords;\\n\" +                         \"uniform sampler2D u_texture;\\n\" +                         \"void main() {\\n\" +                             \"gl_FragColor = v_color * texture2D(u_texture, v_texCoords);\\n\" +                         \"}\";   This is fairly standard setup for a shader that uses a position attribute, a color attribute and a texture coordinate attribute. Notice the 2 varying. They are outputs that we pass through to the fragment shader.   In the fragment shader, we have a sampler2D; this is a special uniform used for textures. As you can see in the main function, we multiply the vertex color with the color from the texture lookup to produce the final output color.   Here is a list of attributes that are in libGDX’s own library for those who wish to easily attach shaders to SpriteBatches and other parts of libGDX:     a_position   a_normal   a_color   a_texCoord, requires a number at the end, i.e. a_texCoord0, a_texCoord1, etc.   a_tangent   a_binormal   To create the ShaderProgram we do the following:   ShaderProgram shader = new ShaderProgram(vertexShader, fragmentShader);   You can ensure that the shader compiled properly via shader.isCompiled(). A compile log can be spit out using shader.getLog().   We also create a matching mesh and load the texture:   mesh = new Mesh(true, 4, 6, VertexAttribute.Position(), VertexAttribute.ColorUnpacked(), VertexAttribute.TexCoords(0)); mesh.setVertices(new float[] {     -0.5f, -0.5f, 0, 1, 1, 1, 1, 0, 1,     0.5f, -0.5f, 0, 1, 1, 1, 1, 1, 1,     0.5f, 0.5f, 0, 1, 1, 1, 1, 1, 0,     -0.5f, 0.5f, 0, 1, 1, 1, 1, 0, 0 }); mesh.setIndices(new short[] {0, 1, 2, 2, 3, 0}); texture = new Texture(Gdx.files.internal(\"bobrgb888-32x32.png\"));   In the render method we then simply call shader.bind() and pass the uniforms in and then render the mesh with the shader:   texture.bind(); shader.bind(); shader.setUniformMatrix(\"u_projTrans\", matrix); shader.setUniformi(\"u_texture\", 0); mesh.render(shader, GL20.GL_TRIANGLES);   And that’s it!   The good thing about shaders in OpenGL ES 2.0 is that you have a huge library of shaders available to you. Pretty much anything that’s done in WebGL can be easily ported over to run on mobiles. Go and experiment!   A simple demo of a shockwave effect shader with libGDX is available on LibGDX.info  ",
        
        "url": "/wiki/graphics/opengl-utils/shaders" },{
        "title": "Extending the Simple Game",
        "excerpt":
        
        "In this tutorial we will be extending the simple game “Drop”, made in the previous tutorial. We will be adding a menu screen and a couple of features to make this game a little more fully featured.   Let’s get started with an introduction to a few more advanced classes in libGDX.   The Screen interface  Screens are fundamental to any game with multiple components. Screens contain many of the methods you are used to from ApplicationListener objects, and include a couple of new methods: show and hide, which are called when the Screen gains or loses focus, respectively. Screens are responsible for handling (i.e., processing and rendering) one aspect of your game: a menu screen, a settings screen, a game screen, etc.   The Game Class  The Game class is responsible for handling multiple screens and provides some helper methods for this purpose, alongside an implementation of ApplicationListener for you to use. Together, Screen and Game objects are used to create a simple and powerful structure for games.   We will start by creating a Drop class, which extends Game and whose create() method will be the entry point to our game. Let’s take a look at some code:   package com.badlogic.drop;  import com.badlogic.gdx.Game; import com.badlogic.gdx.graphics.g2d.BitmapFont; import com.badlogic.gdx.graphics.g2d.SpriteBatch;   public class Drop extends Game {  \tpublic SpriteBatch batch; \tpublic BitmapFont font;  \tpublic void create() { \t\tbatch = new SpriteBatch(); \t\tfont = new BitmapFont(); // use libGDX's default Arial font \t\tthis.setScreen(new MainMenuScreen(this)); \t}  \tpublic void render() { \t\tsuper.render(); // important! \t}  \tpublic void dispose() { \t\tbatch.dispose(); \t\tfont.dispose(); \t}  }   We start the application with instantiating a SpriteBatch and a BitmapFont. It is a bad practice to create multiple objects that can be shared instead (see DRY). The SpriteBatch object is used to render objects onto the screen, such as textures; and the BitmapFont object is used, along with a SpriteBatch, to render text onto the screen. We will touch more on this in the Screen classes.   Next, we set the Screen of the Game to a MainMenuScreen object, with a Drop instance as its first and only parameter.   A common mistake is to forget to call super.render() with a Game implementation. Without this call, the Screen that you set in the create() method will not be rendered if you override the render method in your Game class!   Finally, another reminder to dispose of your (heavy) objects! Some further reading on this can be found here.   The Main Menu  Now, let’s get into the nitty-gritty of the MainMenuScreen class.   package com.badlogic.drop;  import com.badlogic.gdx.Gdx; import com.badlogic.gdx.Screen; import com.badlogic.gdx.graphics.OrthographicCamera;  public class MainMenuScreen implements Screen {  \tfinal Drop game;  \tOrthographicCamera camera;  \tpublic MainMenuScreen(final Drop game) { \t\tthis.game = game;  \t\tcamera = new OrthographicCamera(); \t\tcamera.setToOrtho(false, 800, 480); \t}           //...Rest of class omitted for succinctness.  }   In this code snippet, we make the constructor for the MainMenuScreen class, which implements the Screen interface. The Screen interface does not provide any sort of create() method, so we instead use a constructor. The only parameter for the constructor necessary for this game is an instance of Drop, so that we can call upon its methods and fields if necessary.   Next, the final “meaty” method in the MainMenuScreen class: render(float)   public class MainMenuScreen implements Screen {          //public MainMenuScreen(final Drop game)....  \t@Override \tpublic void render(float delta) { \t\tScreenUtils.clear(0, 0, 0.2f, 1);  \t\tcamera.update(); \t\tgame.batch.setProjectionMatrix(camera.combined);  \t\tgame.batch.begin(); \t\tgame.font.draw(game.batch, \"Welcome to Drop!!! \", 100, 150); \t\tgame.font.draw(game.batch, \"Tap anywhere to begin!\", 100, 100); \t\tgame.batch.end();  \t\tif (Gdx.input.isTouched()) { \t\t\tgame.setScreen(new GameScreen(game)); \t\t\tdispose(); \t\t} \t}          // Rest of class still omitted...  }    The code here is fairly straightforward, except for the fact that we need to call game’s SpriteBatch and BitmapFont instances instead of creating our own. game.font.draw(SpriteBatch, String, float, float), is how text is rendered to the screen. libGDX comes with a pre-made font, Arial, so that you can use the default constructor and still get a font.   We then check to see if the screen has been touched, if it has, then we check to set the games screen to a GameScreen instance, and then dispose of the current instance of MainMenuScreen. The rest of the methods that are needed to implement in the MainMenuScreen are left empty, so I’ll continue to omit them (there is nothing to dispose of in this class).   The Game Screen  Now that we have our main menu finished, it’s time to finally get to making our game. We will be lifting most of the code from the original game as to avoid redundancy, and avoid having to think of a different game idea to implement as simply as Drop is.   package com.badlogic.drop;  import java.util.Iterator;  import com.badlogic.gdx.Gdx; import com.badlogic.gdx.Input.Keys; import com.badlogic.gdx.Screen; import com.badlogic.gdx.audio.Music; import com.badlogic.gdx.audio.Sound; import com.badlogic.gdx.graphics.OrthographicCamera; import com.badlogic.gdx.graphics.Texture; import com.badlogic.gdx.math.MathUtils; import com.badlogic.gdx.math.Rectangle; import com.badlogic.gdx.math.Vector3; import com.badlogic.gdx.utils.Array; import com.badlogic.gdx.utils.ScreenUtils; import com.badlogic.gdx.utils.TimeUtils;  public class GameScreen implements Screen { \tfinal Drop game;  \tTexture dropImage; \tTexture bucketImage; \tSound dropSound; \tMusic rainMusic; \tOrthographicCamera camera; \tRectangle bucket; \tArray&lt;Rectangle&gt; raindrops; \tlong lastDropTime; \tint dropsGathered;  \tpublic GameScreen(final Drop game) { \t\tthis.game = game;  \t\t// load the images for the droplet and the bucket, 64x64 pixels each \t\tdropImage = new Texture(Gdx.files.internal(\"droplet.png\")); \t\tbucketImage = new Texture(Gdx.files.internal(\"bucket.png\"));  \t\t// load the drop sound effect and the rain background \"music\" \t\tdropSound = Gdx.audio.newSound(Gdx.files.internal(\"drop.wav\")); \t\trainMusic = Gdx.audio.newMusic(Gdx.files.internal(\"rain.mp3\")); \t\trainMusic.setLooping(true);  \t\t// create the camera and the SpriteBatch \t\tcamera = new OrthographicCamera(); \t\tcamera.setToOrtho(false, 800, 480);  \t\t// create a Rectangle to logically represent the bucket \t\tbucket = new Rectangle(); \t\tbucket.x = 800 / 2 - 64 / 2; // center the bucket horizontally \t\tbucket.y = 20; // bottom left corner of the bucket is 20 pixels above \t\t\t\t\t\t// the bottom screen edge \t\tbucket.width = 64; \t\tbucket.height = 64;  \t\t// create the raindrops array and spawn the first raindrop \t\traindrops = new Array&lt;Rectangle&gt;(); \t\tspawnRaindrop();  \t}  \tprivate void spawnRaindrop() { \t\tRectangle raindrop = new Rectangle(); \t\traindrop.x = MathUtils.random(0, 800 - 64); \t\traindrop.y = 480; \t\traindrop.width = 64; \t\traindrop.height = 64; \t\traindrops.add(raindrop); \t\tlastDropTime = TimeUtils.nanoTime(); \t}  \t@Override \tpublic void render(float delta) { \t\t// clear the screen with a dark blue color. The \t\t// arguments to clear are the red, green \t\t// blue and alpha component in the range [0,1] \t\t// of the color to be used to clear the screen. \t\tScreenUtils.clear(0, 0, 0.2f, 1);  \t\t// tell the camera to update its matrices. \t\tcamera.update();  \t\t// tell the SpriteBatch to render in the \t\t// coordinate system specified by the camera. \t\tgame.batch.setProjectionMatrix(camera.combined);  \t\t// begin a new batch and draw the bucket and \t\t// all drops \t\tgame.batch.begin(); \t\tgame.font.draw(game.batch, \"Drops Collected: \" + dropsGathered, 0, 480); \t\tgame.batch.draw(bucketImage, bucket.x, bucket.y, bucket.width, bucket.height); \t\tfor (Rectangle raindrop : raindrops) { \t\t\tgame.batch.draw(dropImage, raindrop.x, raindrop.y); \t\t} \t\tgame.batch.end();  \t\t// process user input \t\tif (Gdx.input.isTouched()) { \t\t\tVector3 touchPos = new Vector3(); \t\t\ttouchPos.set(Gdx.input.getX(), Gdx.input.getY(), 0); \t\t\tcamera.unproject(touchPos); \t\t\tbucket.x = touchPos.x - 64 / 2; \t\t} \t\tif (Gdx.input.isKeyPressed(Keys.LEFT)) \t\t\tbucket.x -= 200 * Gdx.graphics.getDeltaTime(); \t\tif (Gdx.input.isKeyPressed(Keys.RIGHT)) \t\t\tbucket.x += 200 * Gdx.graphics.getDeltaTime();  \t\t// make sure the bucket stays within the screen bounds \t\tif (bucket.x &lt; 0) \t\t\tbucket.x = 0; \t\tif (bucket.x &gt; 800 - 64) \t\t\tbucket.x = 800 - 64;  \t\t// check if we need to create a new raindrop \t\tif (TimeUtils.nanoTime() - lastDropTime &gt; 1000000000) \t\t\tspawnRaindrop();  \t\t// move the raindrops, remove any that are beneath the bottom edge of \t\t// the screen or that hit the bucket. In the later case we increase the \t\t// value our drops counter and add a sound effect. \t\tIterator&lt;Rectangle&gt; iter = raindrops.iterator(); \t\twhile (iter.hasNext()) { \t\t\tRectangle raindrop = iter.next(); \t\t\traindrop.y -= 200 * Gdx.graphics.getDeltaTime(); \t\t\tif (raindrop.y + 64 &lt; 0) \t\t\t\titer.remove(); \t\t\tif (raindrop.overlaps(bucket)) { \t\t\t\tdropsGathered++; \t\t\t\tdropSound.play(); \t\t\t\titer.remove(); \t\t\t} \t\t} \t}  \t@Override \tpublic void resize(int width, int height) { \t}  \t@Override \tpublic void show() { \t\t// start the playback of the background music \t\t// when the screen is shown \t\trainMusic.play(); \t}  \t@Override \tpublic void hide() { \t}  \t@Override \tpublic void pause() { \t}  \t@Override \tpublic void resume() { \t}  \t@Override \tpublic void dispose() { \t\tdropImage.dispose(); \t\tbucketImage.dispose(); \t\tdropSound.dispose(); \t\trainMusic.dispose(); \t}  }   This code is almost 95% the same as the original implementation, except now we use a constructor instead of the create() method of the ApplicationListener, and pass in a Drop object, like in the MainMenuScreen class. We also start playing the music as soon as the Screen is set to GameScreen. Moreover, we added a string to the top left corner of the game, which tracks the number of raindrops collected.   Note that the dispose() method of the GameScreen class is not called automatically, see the Screen API. It is your responsibility to take care of that. You can call this method from the dispose() method of the Game class, if the GameScreen class passes a reference to itself to the Game class. It is important to do this, else GameScreen assets might persist and occupy memory even after exiting the application.   And that’s it, you have the complete game finished. That is all there is to know about the Screen interface and abstract Game Class, and all there is to creating multifaceted games with multiple states. The full Java code can be found here. If you are developing in Kotlin, take a look here for the full code.   The Future  After this tutorial you should have a basic understanding how libGDX works and what to expect going forward. Some things can still be improved, like using the Memory Management classes to recycle all the Rectangles we have the garbage collector clean up each time we delete a raindrop. OpenGL is also not too fond if we hand it too many different images in a batch (in our case it’s OK as we only had two images). Usually one would put all those images into a single Texture, also known as a TextureAtlas. In addition, taking a look at Viewports will most certainly prove useful. Viewports help dealing with different screen sizes/resolutions and decide, whether the screen’s content needs to be stretched/should keep its aspect ratio, etc.   To continue learning about libGDX we highly recommend reading our wiki and checking out the demos and tests in our main GitHub repository. If you have any questions, join our official Discord server, we are always glad to help!   The best practice is to get out there and do it, so farewell and happy coding!  ",
        
        "url": "/wiki/start/simple-game-extended" },{
        "title": "Simple text input",
        "excerpt":
        
        "If an application needs to ask the user for a string, e.g a user name or a password, it can do so by using a simple dialog box that is customizable to some extent.   On the desktop a Swing dialog will be opened, prompting the user to enter a string. ⚠ In the LWJGL3 backend is this method not implemented. (See here)   On Android a standard Android dialog will be opened, again prompting the user for input.   To receive the input or a notification that the user canceled the input, one has to implement the TextInputListener interface:   public class MyTextInputListener implements TextInputListener {    @Override    public void input (String text) {    }     @Override    public void canceled () {    } }   The input() method will be called when the user enters a text string. The canceled() method will be called if the user closed the dialog on the desktop or pressed the back button on Android.   To bring up the dialog, simple invoke the following method with your listener:   MyTextInputListener listener = new MyTextInputListener(); Gdx.input.getTextInput(listener, \"Dialog Title\", \"Initial Textfield Value\", \"Hint Value\");   The methods of the listener will be called on the rendering thread, right before the ApplicationListener.render() method is called.                  Prev       Next          ",
        
        "url": "/wiki/input/simple-text-input" },{
        "title": "Skin Composer",
        "excerpt":
        
        "Skin Composer is a handy tool to make the process of creating Scene2D skins easier. Its extensive wiki provides information on how to use it.   Video series:      Features:     Live preview of all widgets with configurable options   Specify tinted, tiled, and custom created Drawables right in the editor   Includes BitmapFont editor and Image font generator like Shoebox   Freetype support and custom serializer to generate fonts from Json   Nine-Patch editor with batch functions for multiple images   Integrated support for TenPatch allowing for smart-resizing, animated UI’s.   Implement your own classes allowing for extended Skin functionality   Create basic Scene2D layouts with Scene Composer   VisUI template and sample projects included   Scene Composer  Later versions of Skin Composer also include a Scene Composer, a WYSIWYG editor that can export JSON or Java files to be loaded directly in-game. It allows easily prototype Scene2D layouts and seeing your Skin in action.  ",
        
        "url": "/wiki/tools/skin-composer" },{
        "title": "Skin",
        "excerpt":
        
        "   Overview   Resources   Convenience methods   Conversions   Modifying resources   Widget styles   Skin JSON            Color       BitmapFont       TintedDrawable           Overview   The Skin class stores resources for UI widgets to use. It is a convenient container for texture regions, ninepatches, fonts, colors, etc. Skin also provides convenient conversions, such as retrieving a texture region as a ninepatch, sprite, or drawable.   Skin files from the libGDX tests can be used as a starting point. You will need: uiskin.png, uiskin.atlas, uiskin.json, and default.fnt. This enables you to quickly get started using scene2d.ui and replace the skin assets later.   Resources in a skin typically come from a texture atlas, widget styles and other objects defined using JSON, and objects added to the skin via code. Even when JSON is not used, it is still recommended to use Skin with a texture atlas and objects added via code. This is much more convenient to obtain instances of drawables and serves as a central place to obtain UI resources.   Useful resources:     Ready to use skins.   Skin Composer is a UI tool for creating and editing skins.   Basic skin Label tutorial    Resources   Each resource in the skin has a name and type. The regions from a texture atlas can be made available as resources in the skin. Texture regions can be retrieved as a ninepatch, sprite, tiled drawable, or drawable.   TextureAtlas atlas = ... Skin skin = new Skin(); skin.addRegions(atlas); ... TextureRegion hero = skin.get(\"hero\", TextureRegion.class);   Resources can also be defined for a skin using JSON (see below) or added using code:   Skin skin = new Skin(); skin.add(\"logo\", new Texture(\"logo.png\")); ... Texture logo = skin.get(\"logo\", Texture.class);   Convenience Methods   There are convenience methods to retrieve resources for commons types.   Skin skin = ... Color red = skin.getColor(\"red\"); BitmapFont font = skin.getFont(\"large\"); TextureRegion region = skin.getRegion(\"hero\"); NinePatch patch = skin.getPatch(\"header\"); Sprite sprite = skin.getSprite(\"footer\"); TiledDrawable tiled = skin.getTiledDrawable(\"pattern\"); Drawable drawable = skin.getDrawable(\"enemy\");   These methods are identical to passing in the appropriate class, but allow for slightly more concise code.   Conversions   All styles for UI widgets use a Drawable when they need an image. This allows a texture region, ninepatch, sprite, etc to be used anywhere in the UI. Skin makes it easy to convert textures and texture regions to drawables and other types:   Skin skin = new Skin(); skin.add(\"logo\", new Texture(\"logo.png\")); ... Texture texture = skin.get(\"logo\", Texture.class); TextureRegion region = skin.getRegion(\"logo\"); NinePatch patch = skin.getPatch(\"logo\"); Sprite sprite = skin.getSprite(\"logo\"); TiledDrawable tiled = skin.getTiledDrawable(\"logo\"); Drawable drawable = skin.getDrawable(\"logo\");   A texture region can be retrieved as a ninepatch, sprite, tiled drawable, or drawable. The first time a conversion is made, a new object is allocated and stored in the skin. Subsequent retrievals will return the stored object.   When converting a texture region to a drawable, the skin will choose the most appropriate drawable for that region. If the region is an AtlasRegion with ninepatch split information, then a NinePatchDrawable is returned. If the region is an AtlasRegion that has been rotated or whitespace stripped, then a SpriteDrawable is returned so the region will be drawn correctly. Otherwise, a TextureRegionDrawable is returned.   Modifying resources   Resources obtained from the skin are not new instances, the same object is returned each time. If the object is modified, the changes will be reflected throughout the application. If this is not desired, a copy of the object should be made.   The newDrawable method copies a drawable. The new drawable’s size information can be changed without affecting the original. The method can also tint a drawable.   Skin skin = ... ... Drawable redDrawable = skin.newDrawable(\"whiteRegion\", Color.RED);   Note the new drawable is not stored in the skin. To store it in the skin it must be explicitly added with a name like any other resource.   Widget styles   Skin is a useful container for providing texture regions and other resources that UI widgets need. It can also store the UI widget styles that define how widgets look.   TextButtonStyle buttonStyle = skin.get(\"bigButton\", TextButtonStyle.class); TextButton button = new TextButton(\"Click me!\", buttonStyle);   All widgets have convenience methods for passing the skin and the style name:   TextButton button = new TextButton(\"Click me!\", skin, \"bigButton\");   If the style name is omitted, the name “default” is used:   TextButton button = new TextButton(\"Click me!\", skin);   Skin JSON   A skin can be populated programmatically. Alternatively, JSON can be used to describe named objects in the skin. This makes it convenient to define the UI widget styles. Note the JSON does not describe texture regions, ninepatche splits, or other information which comes from the texture atlas. However, the JSON may reference the regions, ninepatches, and other resources in the skin by name. The JSON looks like this:   { \tclassName: { \t\tname: resource, \t\t... \t}, \tclassName: { \t\tname: resource, \t\t... \t}, \t... }   className is the fully qualified Java class name for the objects. name is the name of each resource. resource is the JSON for the actual resource object. The JSON corresponds exactly to the names of the fields in the resource’s class. Here is a real example:   { \tcom.badlogic.gdx.graphics.Color: { \t\twhite: { r: 1, g: 1, b: 1, a: 1 }, \t\tred: { r: 1, g: 0, b: 0, a: 1 }, \t\tyellow: { r: 0.5, g: 0.5, b: 0, a: 1 } \t}, \tcom.badlogic.gdx.graphics.g2d.BitmapFont: { \t\tmedium: { file: medium.fnt } \t}, \tcom.badlogic.gdx.scenes.scene2d.ui.TextButton$TextButtonStyle: { \t\tdefault: { \t\t\tdown: round-down, up: round, \t\t\tfont: medium, fontColor: white \t\t}, \t\ttoggle: { \t\t\tdown: round-down, up: round, checked: round-down, \t\t\tfont: medium, fontColor: white, checkedFontColor: red \t\t}, \t\tgreen: { \t\t\tdown: round-down, up: round, \t\t\tfont: medium, fontColor: { r: 0, g: 1, b: 0, a: 1 } \t\t} \t} }   First, some colors and a font are defined. Next, some text button styles are defined. The fields down, up, and checked are of type Drawable. An object is expected but a string is found in the JSON, so the string is used as a name to look up the drawable in the skin. The same thing happens for the font and colors, except for the “green” text button style, which defines a new color inline.   Note that order is important. A resource must be declared in the JSON above where it is referenced. Also note that the JSON that libGDX uses differentiates from the standard, where quotes are not used to define keys or values.   Skin files from the libGDX tests can be used as a starting point: uiskin.png, uiskin.atlas, uiskin.json, and default.fnt.   Loading and configuring a freetype font via the skin json file requires some additional steps. Either use Scene Composer or a library like freetype-skin.   Color   Colors are defined in JSON as shown above. If the r, g, or b properties are omitted, 0 is used. If a is omitted, 1 is used.   Alternatively, you can specify the color by hex value:  com.badlogic.gdx.graphics.Color: { \tskyblue: { hex: 489affff } }   BitmapFont   A bitmap font is declared in the JSON like this:   { \tcom.badlogic.gdx.graphics.g2d.BitmapFont: { \t\tmedium: { file: medium.fnt,                   scaledSize: -1, //integer height of capital letters, default -1 for unscaled                   markupEnabled: false,                   flip : false} \t} }   To find the font’s BMFont file, first the skin looks in the directory containing the skin file. If not found, it uses the specified path as an internal path.   To find the font’s image file, first the skin looks for a texture region with the same name as the font file, without the file extension. If not found, it will look in the directory containing the font file for an image with the same name as the font file, but with a “png” file extension.   TintedDrawable   It is very useful to tint regions various colors. For example, the regions for a white button can be tinted to have a button of any color. Drawables can be tinted in code using the newDrawable method. The Skin.TintedDrawable class provides a way to tint drawables in JSON:   { \tcom.badlogic.gdx.graphics.Color: { \t\tgreen: { r: 0, g: 1, b: 0, a: 1 } \t}, \tcom.badlogic.gdx.scenes.scene2d.ui.Skin$TintedDrawable: { \t\tround-green: { name: round, color: green } \t} }   This makes a copy of the drawable named “round”, tints it green, and adds it to the skin as a drawable under the name “round-green”.  ",
        
        "url": "/wiki/graphics/2d/scene2d/skin" },{
        "title": "Smaato in libGDX",
        "excerpt":
        
        "Summary      Introduction   Configuration   Recap   Banner   Interstitial   Rewarded   Test Ads   Introduction   This article will show you how to add Smaato ads to your libGDX Android projects. You will be able to monetize your games with Banner, Interstitial and Rewarded Ads. This tutorial implies that you are already familiar with libGDX basics and how Android Views are handled. But there will be a short recap.   Just for a reference, the latest documentation is available on Smaato official website.   You will need:     A libGDX Android Game Project   Your Smaato account   Configuration  Add the following repository setup to your project’s main build.gradle file:   buildscript {     repositories {         mavenCentral()         google()         jcenter()         maven { url \"https://s3.amazonaws.com/smaato-sdk-releases/\" }     } } ... allprojects {     repositories {         google()         jcenter()         maven {             url \"https://s3.amazonaws.com/smaato-sdk-releases/\"         }     } }  Set the compile options to Java 8 and minSdkVersion to at least version 16, in android module build.gradle file:  android {     defaultConfig {         minSdkVersion 16         ...     }     compileOptions {         sourceCompatibility JavaVersion.VERSION_1_8         targetCompatibility JavaVersion.VERSION_1_8     } }   Add required dependencies to your application module build.gradle file under project(“:android”) –&gt; dependencies.   implementation 'com.smaato.android.sdk:smaato-sdk:21.5.3   If you’re using Proguard in your project, please add the following lines to your Proguard config file:  -keep public class com.smaato.sdk.** { *; } -keep public interface com.smaato.sdk.** { *; }   Add the following permissions to application AndroidManifest.xml file:  &lt;uses-permission android:name=\"android.permission.INTERNET\" /&gt; &lt;uses-permission android:name=\"android.permission.ACCESS_NETWORK_STATE\" /&gt; &lt;uses-permission android:name=\"android.permission.ACCESS_COARSE_LOCATION\" /&gt;   If your application targets Android 5.0 (API level 21) or higher, then you need to add the following line to your application AndroidManifest.xml file:  &lt;uses-feature android:name=\"android.hardware.location.network\" /&gt;   The latest version of Smaato NextGenSDK has been validated against com.android.tools.build:gradle:3.5.4 and gradle-wrapper gradle-5.6.3-bin   Main build.gradle:  buildscript {     ...     dependencies {         classpath 'com.android.tools.build:gradle:3.5.4'     } }   The gradle-wrapper.properties:  distributionUrl=https\\://services.gradle.org/distributions/gradle-5.6.3-bin.zip distributionBase=GRADLE_USER_HOME distributionPath=wrapper/dists zipStorePath=wrapper/dists zipStoreBase=GRADLE_USER_HOME   Recap  LibGDX Android game is created as a View. In order to add ads that are constantly displayed (like Banners) another view holding that should be used. In other words, a Relative Layout having both views should be created. You can see that in the following code snippet:   public class AndroidLauncher extends AndroidApplication {      @Override     protected void onCreate(Bundle savedInstanceState) {         super.onCreate(savedInstanceState);          RelativeLayout layout = new RelativeLayout(this);         layout.setLayoutParams(new RelativeLayout.LayoutParams(MATCH_PARENT, MATCH_PARENT));         layout.addView(createGameView());         createAdView(layout);         setContentView(layout);     }      private View createGameView() {         View gameView = initializeForView(new MyGame(this), new AndroidApplicationConfiguration());         RelativeLayout.LayoutParams params = new RelativeLayout.LayoutParams(MATCH_PARENT, WRAP_CONTENT);         params.addRule(RelativeLayout.ALIGN_PARENT_TOP, RelativeLayout.TRUE);         params.addRule(RelativeLayout.CENTER_HORIZONTAL, RelativeLayout.TRUE);         gameView.setLayoutParams(params);         return gameView;     } }  The only missing piece here is the createAdView method that will be created in the next section.   Banner  In order to add Banner ads we will use com.smaato.sdk.banner.widget.BannerView. As mentioned previously, the Banner view will reside in the same Relative Layout as the Game view.  public class AndroidLauncher extends AndroidApplication {     private static final String PUBLISHER_ID = \"1100042525\";     private static final String BANNER_AD_SPACE_ID = \"130635694\";      private BannerView smaatoBanner;      @Override     protected void onCreate(Bundle savedInstanceState) {         super.onCreate(savedInstanceState);          RelativeLayout layout = new RelativeLayout(this);         layout.setLayoutParams(new RelativeLayout.LayoutParams(MATCH_PARENT, MATCH_PARENT));         layout.addView(createGameView());         createAdView(layout);         setContentView(layout);     }      private void createAdView(RelativeLayout layout) {         SmaatoSdk.init(getApplication(), PUBLISHER_ID);         createSmaatoBanner();         layout.addView(smaatoBanner);     }      private void createSmaatoBanner() {         smaatoBanner = new BannerView(this);          RelativeLayout.LayoutParams params = new RelativeLayout.LayoutParams(MATCH_PARENT, WRAP_CONTENT);         params.addRule(RelativeLayout.ALIGN_PARENT_BOTTOM, RelativeLayout.TRUE);         params.addRule(RelativeLayout.CENTER_HORIZONTAL, RelativeLayout.TRUE);         smaatoBanner.setLayoutParams(params);         smaatoBanner.setBackgroundColor(Color.TRANSPARENT);         smaatoBanner.setVisibility(View.INVISIBLE);     }      @Override     public void onDestroy() {         if (smaatoBanner != null) smaatoBanner.destroy();         super.onDestroy();     }      public void showBannerAds() {         runOnUiThread(() -&gt; {             smaatoBanner.setVisibility(View.VISIBLE);             smaatoBanner.setEnabled(true);             smaatoBanner.loadAd(BANNER_AD_SPACE_ID, BannerAdSize.XX_LARGE_320x50);         });     }      public void hideBannerAds() {         runOnUiThread(() -&gt; {             smaatoBanner.setVisibility(View.GONE);             smaatoBanner.setEnabled(false);         });     } }  Let me walk you through this code snippet:     BannerView smaatoBanner is kept as an instance variable, so an ad can be shown and hidden depending on the game lifecycle   BannerView should be always destroyed in order to release resources properly   The banner will be placed according to RelativeLayout.LayoutParams rules   showBannerAds/hideBannerAds are supposed to be called as per the game lifecycle   PUBLISHER_ID and BANNER_AD_SPACE_ID can be used during testing. Please, refer to Test Ads section for all available test ads configurations.   Interstitial  Unlike Banners, Interstitial ads are not constantly displayed on the screen. Such ads should be only shown at natural transition points in the flow of an app, like finishing/failing of a level or switching between the screens. Hence there is no need to have an additional view for Interstitial ads as they will typically occupy the whole screen and will be destroyed upon closing.   public class AndroidLauncher extends AndroidApplication {     private static final String PUBLISHER_ID = \"1100042525\";     private static final String INTERSTITIAL_AD_SPACE_ID = \"130626426\";      private InterstitialAd smaatoInterstitial;      @Override     protected void onCreate(Bundle savedInstanceState) {         super.onCreate(savedInstanceState);          RelativeLayout layout = new RelativeLayout(this);         layout.setLayoutParams(new RelativeLayout.LayoutParams(MATCH_PARENT, MATCH_PARENT));         layout.addView(createGameView());         createAd();         setContentView(layout);     }      private void createAd() {         SmaatoSdk.init(getApplication(), PUBLISHER_ID);         requestSmaatoInterstitialAd();     }      public void showInterstitial() {         runOnUiThread(() -&gt; {             if (smaatoInterstitial == null) {                 requestSmaatoInterstitialAd();                 return;             }             smaatoInterstitial.showAd(AndroidLauncher.this);             requestSmaatoInterstitialAd();         });     }      private void requestSmaatoInterstitialAd() {         Interstitial.loadAd(INTERSTITIAL_AD_SPACE_ID, new EventListener() {             @Override             public void onAdLoaded(@NonNull InterstitialAd interstitialAd) {                 smaatoInterstitial = interstitialAd;             }              @Override             public void onAdFailedToLoad(@NonNull InterstitialRequestError interstitialRequestError) {                 log(AndroidLauncher.class.getName(), \"Smaato Interstitial failed to load; error: \" + interstitialRequestError.getInterstitialError());             }              @Override             public void onAdError(@NonNull InterstitialAd interstitialAd, @NonNull InterstitialError interstitialError) {             }              @Override             public void onAdOpened(@NonNull InterstitialAd interstitialAd) {             }              @Override             public void onAdClosed(@NonNull InterstitialAd interstitialAd) {             }              @Override             public void onAdClicked(@NonNull InterstitialAd interstitialAd) {             }              @Override             public void onAdImpression(@NonNull InterstitialAd interstitialAd) {             }              @Override             public void onAdTTLExpired(@NonNull InterstitialAd interstitialAd) {             }         });     } }  Let me walk you through this code snippet:     InterstitialAd smaatoInterstitial is kept as an instance variable, so an ad can be shown depending on the game lifecycle   It is recommended to call requestSmaatoInterstitialAd on a game load, in order to have an ad preloaded from the start   There are multiple useful callbacks of Interstitial.loadAd that can be used depending on your use case   showInterstitial is supposed to be called as per the game lifecycle   Rewarded  Rewarded ads are visually similar to Interstitial ads as they typically occupy the whole screen. But workflow-wise they are completely different - such ads encourage users to interact with the ad content in exchange for in-app rewards, like extra lives, free coins or energy.   public class AndroidLauncher extends AndroidApplication {     private static final String PUBLISHER_ID = \"1100042525\";     private static final String REWARDED_INTERSTITIAL_AD_SPACE_ID = \"130626428\";      private RewardedInterstitialAd smaatoRewardedInterstitial;      @Override     protected void onCreate(Bundle savedInstanceState) {         super.onCreate(savedInstanceState);          RelativeLayout layout = new RelativeLayout(this);         layout.setLayoutParams(new RelativeLayout.LayoutParams(MATCH_PARENT, MATCH_PARENT));         layout.addView(createGameView());         createAd();         setContentView(layout);     }      private void createAd() {         SmaatoSdk.init(getApplication(), PUBLISHER_ID);         requestSmaatoRewardedInterstitialAd();     }      public void showRewarded() {         runOnUiThread(() -&gt; {             if (smaatoRewardedInterstitial == null) {                 requestSmaatoRewardedInterstitialAd();                 return;             }             smaatoRewardedInterstitial.showAd();             requestSmaatoRewardedInterstitialAd();         });     }      private void requestSmaatoRewardedInterstitialAd() {         RewardedInterstitial.loadAd(REWARDED_INTERSTITIAL_AD_SPACE_ID, new EventListener() {             @Override             public void onAdLoaded(@NonNull RewardedInterstitialAd rewardedInterstitialAd) {                 smaatoRewardedInterstitial = rewardedInterstitialAd;             }              @Override             public void onAdFailedToLoad(@NonNull RewardedRequestError rewardedRequestError) {             }              @Override             public void onAdError(@NonNull RewardedInterstitialAd rewardedInterstitialAd, @NonNull RewardedError rewardedError) {             }              @Override             public void onAdClosed(@NonNull RewardedInterstitialAd rewardedInterstitialAd) {             }              @Override             public void onAdClicked(@NonNull RewardedInterstitialAd rewardedInterstitialAd) {             }              @Override             public void onAdStarted(@NonNull RewardedInterstitialAd rewardedInterstitialAd) {             }              @Override             public void onAdReward(@NonNull RewardedInterstitialAd rewardedInterstitialAd) {             }              @Override             public void onAdTTLExpired(@NonNull RewardedInterstitialAd rewardedInterstitialAd) {             }         });     } }  One more time, let me walk you through this code snippet:     RewardedInterstitialAd smaatoRewardedInterstitial is kept as an instance variable, so an ad can be shown whenever a user initiates a rewarded ad flow in your game   It is recommended to call requestSmaatoRewardedInterstitialAd on a game load, in order to have an ad preloaded from the start   There are multiple useful callbacks of RewardedInterstitial.loadAd that you can be used depending on your use case   showRewarded is supposed to be called whenever a user initiates a rewarded ad flow in your game   Test Ads                  Adspace ID       Type                       130626424       Rich Media                 130635694       Static Image                 130635706       MRAID                 130626426       Rich Media / Video                 130626427       Video                 130626428       Rewarded           Please, use Publisher Id 1100042525 with each of those adspaces.   Don’t forget to change the values to the production ones before the release!  ",
        
        "url": "/wiki/third-party/smaato-in-libgdx" },{
        "title": "Sound effects",
        "excerpt":
        
        "Sound effects are small audio samples, usually no longer than a few seconds, that are played back on specific game events such as a character jumping or shooting a gun.   Sound effects can be stored in various formats, including MP3, OGG and WAV. Which format you should use, depends on your specific needs, as each format has its own advantages and disadvantages. For example, WAV files are quite large compared to other formats, OGG files don’t work on RoboVM (iOS) nor with Safari (GWT), and MP3 files have issues with seemless looping.   Note: On Android, a Sound instance can not be over 1mb in size (uncompressed raw PCM size, not the file size). If you have a bigger file, use Music instead.   Sound effects are represented by the Sound interface. Loading a sound effect works as follows:   Sound sound = Gdx.audio.newSound(Gdx.files.internal(\"data/mysound.mp3\"));   This loads an audio file called \"mysound.mp3\" from the internal directory data.   Once we have the sound loaded we can play it back:   sound.play(1.0f);   This will playback the sound effect once, at full volume. The play method on a single Sound instance can be called many times in a row, e.g. for a sequence of shots in a game, and will be overlaid accordingly.   More fine-grained control is available. Every call to Sound.play() returns a long which identifies that sound instance. Using this handle we can modify that specific playback instance:   long id = sound.play(1.0f); // play new sound and keep handle for further manipulation sound.stop(id);             // stops the sound instance immediately sound.setPitch(id, 2);      // increases the pitch to 2x the original pitch  id = sound.play(1.0f);      // plays the sound a second time, this is treated as a different instance sound.setPan(id, -1, 1);    // sets the pan of the sound to the left side at full volume sound.setLooping(id, true); // keeps the sound looping sound.stop(id);             // stops the looping sound    note: These modifier methods have limited functionality in the JavaScript/WebGL back-end for now. As of 1.9.6 setPan() is only working when flash is supported and enabled GwtApplicationConfiguration.preferFlash = true   note: setPan() method does not work with stereo sounds   Once you no longer need a Sound, make sure you dispose of it:   sound.dispose();   Accessing the sound after you disposed of it will result in undefined errors.   Multiple sounds cause freezes on Android  As stated on an Audio topic the Android has many issues with audio in general. One of them, is that waiting for sound ID might take quite a lot time. The sounds in Libgdx are playing synchronously by default. It causes main loop to be frozen for significant time if you play a lot of sounds at once. Especially this issue noticeable on Android 10.   The solution is to make them playing asynchronously. However it will cause inability to use sounds methods where ID is required. More info provided here - Audio#audio-on-android  ",
        
        "url": "/wiki/audio/sound-effects" },{
        "title": "Spritebatch, Textureregions, and Sprites",
        "excerpt":
        
        "This page gives a brief overview of how images are drawn using OpenGL and how libGDX simplifies and optimizes the task through the SpriteBatch class.   Drawing images   An image that has been decoded from its original format (e.g., PNG) and uploaded to the GPU is called a texture. To draw a texture, geometry is described and the texture is applied by specifying where each vertex in the geometry corresponds on the texture. For example, the geometry could be a rectangle and the texture could be applied so that each corner of the rectangle corresponds to a corner of the texture. A rectangle that is a subset of a texture is called a texture region.   To do the actual drawing, first the texture is bound (i.e., made the current texture), then the geometry is given to OpenGL to draw. The size and position on the screen that the texture is drawn is determined by both the geometry and how the OpenGL viewport is configured. Many 2D games configure the viewport to match the screen resolution. This means that the geometry is specified in pixels, which makes it easy to draw textures in the appropriate size and position on the screen.   It is very common to draw a texture mapped to rectangular geometry. It is also very common to draw the same texture or various regions of that texture many times. It would be inefficient to send each rectangle one at a time to the GPU to be drawn. Instead, many rectangles for the same texture can be described and sent to the GPU all at once. This is what the SpriteBatch class does.   SpriteBatch is given a texture and coordinates for each rectangle to be drawn. It collects the geometry without submitting it to the GPU. If it is given a texture different than the last texture, then it binds the last texture, submits the collected geometry to be drawn, and begins collecting geometry for the new texture.   Changing textures every few rectangles that are drawn prevents SpriteBatch from batching much geometry. Also, binding a texture is a somewhat expensive operation. For these reasons, it is common to store many smaller images in a larger image and then draw regions of the larger image to both maximize geometry batching and avoid texture changes. See TexturePacker for more information.   SpriteBatch   Using SpriteBatch (source) in an application looks like this:    public class Game implements ApplicationAdapter { \tprivate SpriteBatch batch;  \tpublic void create () { \t\tbatch = new SpriteBatch(); \t}  \tpublic void render () { \t\tGdx.gl.glClear(GL20.GL_COLOR_BUFFER_BIT); // This cryptic line clears the screen. \t\tbatch.begin(); \t\t// Drawing goes here! \t\tbatch.end(); \t} }   All SpriteBatch drawing calls must be made between the begin and end methods. Non-SpriteBatch drawing cannot occur between begin and end.   SpriteBatch assumes the active texture unit is 0. When using custom shaders and binding textures yourself, you can reset this with the following code:   Gdx.gl.glActiveTexture(GL20.GL_TEXTURE0);   Texture   The Texture class decodes an image file and loads it into GPU memory. The image file should be placed in the “assets” folder. The image’s dimensions should be powers of two (16x16, 64x256, etc) for compatibility and performance reasons.   private Texture texture; ... texture = new Texture(Gdx.files.internal(\"image.png\")); ... batch.begin(); batch.draw(texture, 10, 10); batch.end();   Here a texture is created and passed to a SpriteBatch to be drawn. The texture will be drawn in a rectangle positioned at 10,10 with a width and height equal to the size of the texture. SpriteBatch has many methods for drawing a texture:                  Method signature       Description                       draw(Texture texture, float x, float y)       Draws the texture using the texture’s width and height                 draw(Texture texture, float x, float y, int srcX, int srcY, int srcWidth, int srcHeight)       Draws a portion of the texture.                 draw(Texture texture, float x, float y, float width, float height, int srcX, int srcY, int srcWidth, int srcHeight, boolean flipX, boolean flipY)       Draws a portion of a texture, stretched to the width and height, and optionally flipped.                 draw(Texture texture, float x, float y, float originX, float originY, float width, float height, float scaleX, float scaleY, float rotation, int srcX, int srcY, int srcWidth, int srcHeight, boolean flipX, boolean flipY)       This monster method draws a portion of a texture, stretched to the width and height, scaled and rotated around an origin, and optionally flipped.                 draw(Texture texture, float x, float y, float width, float height, float u, float v, float u2, float v2)       This draws a portion of a texture, stretched to the width and height. This is a somewhat advanced method as it uses texture coordinates from 0-1 rather than pixel coordinates.                 draw(Texture texture, float[] spriteVertices, int offset, int length)       This is an advanced method for passing in the raw geometry, texture coordinates, and color information. This can be used to draw any quadrilateral, not just rectangles.           TextureRegion   The TextureRegion class (source) describes a rectangle inside a texture and is useful for drawing only a portion of the texture.   private TextureRegion region; ... texture = new Texture(Gdx.files.internal(\"image.png\")); region = new TextureRegion(texture, 20, 20, 50, 50); ... batch.begin(); batch.draw(region, 10, 10); batch.end();   Here the 20, 20, 50, 50 describes the portion of the texture, which is then drawn at 10,10. The same can be achieved by passing the Texture and other parameters to SpriteBatch, but TextureRegion makes it convenient to have a single object that describes both.   SpriteBatch has many methods for drawing a texture region:                  Method signature       Description                       draw(TextureRegion region, float x, float y)       Draws the region using the width and height of the region.                 draw(TextureRegion region, float x, float y, float width, float height)       Draws the region, stretched to the width and height.                 draw(TextureRegion region, float x, float y, float originX, float originY, float width, float height, float scaleX, float scaleY, float rotation)       Draws the region, stretched to the width and height, and scaled and rotated around an origin.           Sprite   The Sprite class (source) describes both a texture region, the geometry where it will be drawn, and the color it will be drawn.   private Sprite sprite; ... texture = new Texture(Gdx.files.internal(\"image.png\")); sprite = new Sprite(texture, 20, 20, 50, 50); sprite.setPosition(10, 10); sprite.setRotation(45); ... batch.begin(); sprite.draw(batch); batch.end();   Here the 20, 20, 50, 50 describes the portion of the texture, which is rotated 45 degrees and then drawn at 10,10. The same can be achieved by passing the Texture or a TextureRegion and other parameters to SpriteBatch, but Sprite makes it convenient to have a single object that describes everything. Also, because Sprite stores the geometry and only recomputes it when necessary, it is slightly more efficient if the scale, rotation, or other properties are unchanged between frames.   Note that Sprite mixes model information (position, rotation, etc) with view information (the texture being drawn). This makes Sprite inappropriate when applying a design pattern that wishes to strictly separate the model from the view. In that case, using Texture or TextureRegion may make more sense.   Also note that there is no Sprite constructor that is related to the position of the Sprite. calling Sprite(Texture, int, int, int, int) does not edit the position. It is necessary to call Sprite#setPosition(float,float) or else the sprite will be drawn at the default position of 0,0.   Tinting   When a texture is drawn, it can be tinted a color:   private Texture texture; private TextureRegion region; private Sprite sprite; ... texture = new Texture(Gdx.files.internal(\"image.png\")); region = new TextureRegion(texture, 20, 20, 50, 50); sprite = new Sprite(texture, 20, 20, 50, 50); sprite.setPosition(100, 10); sprite.setColor(0, 0, 1, 1); ... batch.begin(); batch.setColor(1, 0, 0, 1); batch.draw(texture, 10, 10); batch.setColor(0, 1, 0, 1); batch.draw(region, 50, 10); sprite.draw(batch); batch.end();   This shows how to draw a texture, region, and sprite with a tint color. The color values here are described using RGBA values between 1 and 0. Alpha is ignored if blending is disabled.   Blending   Blending is enabled by default. This means that when a texture is drawn, translucent portions of the texture are merged with pixels already on the screen at that location.   When blending is disabled, anything already on the screen at that location is replaced by the texture. This is more efficient, so blending should always be disabled unless it is needed. E.g., when drawing a large background image over the whole screen, a performance boost can be gained by first disabling blending:   Gdx.gl.glClear(GL20.GL_COLOR_BUFFER_BIT); // This cryptic line clears the screen. batch.begin(); batch.disableBlending(); backgroundSprite.draw(batch); batch.enableBlending(); // Other drawing here. batch.end();   Note: Be sure to clear the screen each frame. If this is not done, a texture with alpha can be drawn on top of itself hundreds of times, making it appear opaque. Also, some GPU architectures perform better when the screen is cleared each frame, even if opaque images are being drawn over the entire screen.   Viewport   SpriteBatch manages its own projection and transformation matrixes. When a SpriteBatch is created, it uses the current application size to setup an orthographic projection using a y-up coordinate system. When begin is called, it sets up the viewport.   Performance tuning   SpriteBatch has a constructor that sets the maximum number of sprites that can be buffered before sending to the GPU. If this is too low, it will cause extra calls to the GPU. If this is too high, the SpriteBatch will be using more memory than is necessary.   SpriteBatch has a public int field named maxSpritesInBatch. This indicates the highest number of sprites that were sent to the GPU at once over the lifetime of the SpriteBatch. Setting a very large SpriteBatch size and then checking this field can help determine the optimum SpriteBatch size. It should be sized equal to or slightly more than maxSpritesInBatch. This field may be set to zero to reset it at any time.   SpriteBatch has a public int field named renderCalls. After end is called, this field indicates how many times a batch of geometry was sent to the GPU between the last calls to begin and end. This occurs when a different texture must be bound, or when the SpriteBatch has cached enough sprites to be full. If the SpriteBatch is sized appropriately and renderCalls is large (more than maybe 15-20), it indicates that many texture binds are occurring.   SpriteBatch has an additional constructor that takes a size and a number of buffers. This is an advanced feature that causes vertex buffer objects (VBOs) to be used rather than the usual vertex arrays (VAs). A list of buffers is kept, and each render call uses the next buffer in the list (wrapping around). When maxSpritesInBatch is low and renderCalls is large, this may provide a small performance boost.  ",
        
        "url": "/wiki/graphics/2d/spritebatch-textureregions-and-sprites" },{
        "title": "Starter classes and configuration",
        "excerpt":
        
        "   Desktop (LWJGL3)   Desktop (LWJGL)   Android            Game Activity       Game Fragment       Manifest configuration       Live Wallpapers       Screen Savers (aka Daydreams)           iOS/Robovm   HTML5/GWT   For each target platform, a starter class has to be written. This class instantiates a back-end specific Application implementation and the ApplicationListener that implements the application logic. The starter classes are platform-dependent, so let’s have a look at how to instantiate and configure these for each backend.   This article assumes you have followed the instructions in Project Setup as well as Importing &amp; Running a Project and you therefore have already set up the project in your IDE.   Desktop (LWJGL3)   Since libGDX version 1.10.1, this has been the default desktop backend. You can find more information here.   Opening the DesktopLauncher.java class in my-gdx-game shows the following:   package com.me.mygdxgame;  import com.badlogic.gdx.backends.lwjgl3.Lwjgl3Application; import com.badlogic.gdx.backends.lwjgl3.Lwjgl3ApplicationConfiguration;  public class DesktopLauncher {    public static void main(String[] args) {       Lwjgl3ApplicationConfiguration config = new Lwjgl3ApplicationConfiguration();       config.setTitle(\"my-gdx-game\");       config.setWindowedMode(480, 320);        new Lwjgl3Application(new MyGdxGame(), config);    } }   First an Lwjgl3ApplicationConfiguration is instantiated. This class lets one specify various configuration settings, such as the initial screen resolution, whether to use OpenGL ES 2.0 or 3.0 and so on. Refer to the Javadocs of this class for more information.   Once the configuration object is set, an Lwjgl3Application is instantiated. The MyGdxGame() class is the ApplicationListener implementing the game logic.   From there on a window is created and the ApplicationListener is invoked as described in The Life-Cycle   Common issues:      On macOS, there is another step required to get LWJGL 3 apps running. Either add the com.badlogicgames.gdx:gdx-lwjgl3-glfw-awt-macos dependency to your desktop project set the VM Options to -XstartOnFirstThread. The latter can typically be done in the Launch/Run Configurations of your IDE, as is described here. Alternatively, if you’re starting your project via Gradle, add this line to run task of the desktop Gradle file:      jvmArgs = ['-XstartOnFirstThread']          A fourth approach is to just programatically restart the JVM if the argument is not present (see here for a simple example). Lastly, if you want to deploy your game by packaging a JRE with it (which is the recommended way to distribute your later game), jpackage or packr allow you to set the JVM arguments.       If you are using gdx-tools and the lwjgl3 backend in the same project, you need to modify your gdx-tools dependency like this:     compile (\"com.badlogicgames.gdx:gdx-tools:$gdxVersion\") {    exclude group: 'com.badlogicgames.gdx', module: 'gdx-backend-lwjgl' }           Desktop (LWJGL)   In version 1.10.1, libGDX switched its default desktop backend to LWJGL 3. If you want to upgrade, please take a look here.   Opening the DesktopLauncher.java class in my-gdx-game shows the following:   package com.me.mygdxgame;  import com.badlogic.gdx.backends.lwjgl.LwjglApplication; import com.badlogic.gdx.backends.lwjgl.LwjglApplicationConfiguration;  public class DesktopLauncher {    public static void main(String[] args) {       LwjglApplicationConfiguration cfg = new LwjglApplicationConfiguration();       cfg.title = \"my-gdx-game\";       cfg.useGL30 = false;       cfg.width = 480;       cfg.height = 320;        new LwjglApplication(new MyGdxGame(), cfg);    } }   First an LwjglApplicationConfiguration is instantiated. This class lets one specify various configuration settings, such as the initial screen resolution, whether to use OpenGL ES 2.0 or 3.0 and so on. Refer to the Javadocs of this class for more information.   Once the configuration object is set, an LwjglApplication is instantiated. The MyGdxGame() class is the ApplicationListener implementing the game logic.   From there on a window is created and the ApplicationListener is invoked as described in The Life-Cycle   Common issues:           When using a JDK of version 8 or later, an “illegal reflective access” warning is shown. This is nothing to be worried about. If it bothers you, downgrade the used JDK or switch to the LWJGL 3 backend.            If an error like Process 'command 'C:/.../java.exe'' finished with non-zero exit value -1 is shown, this can safely be ignored. A workaround is disabling forceExit: config.forceExit = false;.       Android   Game Activity   Android applications do not use a main() method as the entry-point, but instead require an Activity. Open the MainActivity.java class in the my-gdx-game-android project:   package com.me.mygdxgame;  import android.os.Bundle;  import com.badlogic.gdx.backends.android.AndroidApplication; import com.badlogic.gdx.backends.android.AndroidApplicationConfiguration;  public class MainActivity extends AndroidApplication {     @Override     public void onCreate(Bundle savedInstanceState) {         super.onCreate(savedInstanceState);          AndroidApplicationConfiguration cfg = new AndroidApplicationConfiguration();          initialize(new MyGdxGame(), cfg);     } }   The main entry-point method is the Activity’s onCreate() method. Note that MainActivity derives from AndroidApplication, which itself derives from Activity. As in the desktop starter class, a configuration instance is created (AndroidApplicationConfiguration). Once configured, the AndroidApplication.initialize() method is called, passing in the ApplicationListener (MyGdxGame) as well as the configuration. Refer to the AndroidApplicationConfiguration Javadocs for more information on what configuration settings are available.   Android applications can have multiple activities. libGDX games should usually only consist of a single activity. Different screens of the game are implemented within libGDX, not as separate activities. The reason for this is that creating a new Activity also implies creating a new OpenGL context, which is time consuming and also means that all graphical resources have to be reloaded.   Game Fragment   A libGDX game can be hosted in an Android Fragment instead of using a complete Activity. This allows it to take up a portion of the screen in an Activity or be moved between layouts. To create a libGDX fragment, subclass AndroidFragmentApplication and implement the onCreateView() with the following initialization:      @Override     public View onCreateView(LayoutInflater inflater, ViewGroup container, Bundle savedInstanceState) {         return initializeForView(new MyGdxGame());     }   That code depends on some other changes to the -android project:     Add AndroidX Fragment Library to the -android project and its build path if you haven’t already added it. This is needed in order to extend FragmentActivity later.   Change the AndroidLauncher Activity to extend FragmentActivity, not AndroidApplication.   Implement AndroidFragmentApplication.Callbacks on the AndroidLauncher Activity.   Create a class that extends AndroidFragmentApplication which is the Fragment implementation for libGDX.   Add the initializeForView() code in the Fragment’s onCreateView method.   Finally, replace the AndroidLauncher activity content with the libGDX Fragment.   For example:  // 2. Change AndroidLauncher activity to extend FragmentActivity, not AndroidApplication // 3. Implement AndroidFragmentApplication.Callbacks on the AndroidLauncher activity public class AndroidLauncher extends FragmentActivity implements AndroidFragmentApplication.Callbacks {    @Override    protected void onCreate (Bundle savedInstanceState)    {       super.onCreate(savedInstanceState);        // 6. Finally, replace the AndroidLauncher activity content with the libGDX Fragment.       GameFragment fragment = new GameFragment();       FragmentTransaction trans = getSupportFragmentManager().beginTransaction();       trans.replace(android.R.id.content, fragment);       trans.commit();    }     // 4. Create a Class that extends AndroidFragmentApplication which is the Fragment implementation for libGDX.    public static class GameFragment extends AndroidFragmentApplication    {       // 5. Add the initializeForView() code in the Fragment's onCreateView method.       @Override       public View onCreateView(LayoutInflater inflater, ViewGroup container, Bundle savedInstanceState)       {  return initializeForView(new MyGdxGame());   }    }     @Override    public void exit() {} }   Manifest configuration  Besides the AndroidApplicationConfiguration, an Android application is also configured via the AndroidManifest.xml file, found in the root directory of the Android project. This might look something like this:   &lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt; &lt;manifest xmlns:android=\"http://schemas.android.com/apk/res/android\"     package=\"com.me.mygdxgame\"&gt;      &lt;application         android:icon=\"@drawable/ic_launcher\"         android:label=\"@string/app_name\" &gt;         &lt;activity             android:name=\".MainActivity\"             android:label=\"@string/app_name\"             android:screenOrientation=\"landscape\"             android:configChanges=\"keyboard|keyboardHidden|orientation\"&gt;             &lt;intent-filter&gt;                 &lt;action android:name=\"android.intent.action.MAIN\" /&gt;                 &lt;category android:name=\"android.intent.category.LAUNCHER\" /&gt;             &lt;/intent-filter&gt;         &lt;/activity&gt;     &lt;/application&gt;  &lt;/manifest&gt;   Screen Orientation &amp; Configuration Changes  In addition to the targetSdkVersion, the screenOrientation and configChanges attributes of the activity element should always be set.   The screenOrientation attribute specifies a fixed orientation for the application. One may omit this if the application can work with both landscape and portrait mode.   The configChanges attribute is crucial and should always have the values shown above. Omitting this attribute means that the application will be restarted every time a physical keyboard is slid out/in or if the orientation of the device changes. If the screenOrientation attribute is omitted, a libGDX application will receive calls to ApplicationListener.resize() to indicate the orientation change. API clients can then re-layout the application accordingly.   Permissions  If an application needs to be able to write to the external storage of a device (e.g. SD-card), needs internet access, uses the vibrator or wants to record audio, the following permissions need to be added to the AndroidManifest.xml file:   \t&lt;uses-permission android:name=\"android.permission.RECORD_AUDIO\"/&gt; \t&lt;uses-permission android:name=\"android.permission.WRITE_EXTERNAL_STORAGE\"/&gt; \t&lt;uses-permission android:name=\"android.permission.VIBRATE\"/&gt;   Users are generally suspicious of applications with many permissions, so choose these wisely.   For wake locking to work, AndroidApplicationConfiguration.useWakeLock needs to be set to true.   If a game doesn’t need accelerometer or compass access, it is advised to disable these by setting the useAccelerometer and useCompass fields of AndroidApplicationConfiguration to false.   If your game needs the gyroscope sensor, you have to set useGyroscope to true in AndroidApplicationConfiguration (It’s disabled by default, to save energy).   Please refer to the Android Developer’s Guide for more information on how to set other attributes like icons for your application.   Live Wallpapers  A libGDX core application can also be used as an Android Live Wallpaper. The project setup is very similar to an Android game, but AndroidLiveWallpaperService is used in place of AndroidApplication. Live Wallpapers are Android Services, not Activities.   Note: Due to synchronization issues, you cannot combine games and live wallpapers in the same app. However, Live Wallpapers and Screen Savers can safely coexist in the same app.   First, extend AndroidLiveWallpaperService and override onCreateApplication() (instead of onCreate() like you would do with a game Activity):   public class MyLiveWallpaper extends AndroidLiveWallpaperService {     @Override     public void onCreateApplication() {         AndroidApplicationConfiguration cfg = new AndroidApplicationConfiguration();          initialize(new MyGdxGame(), cfg);     } }   You can optionally subscribe to Live Wallpaper-specific events by implementing AndroidWallpaperListener with your ApplicationListener class. AndroidWallpaperListener is not available from the core module, so you can either follow the strategy outlined in Interfacing With Platform-Specific Code, or you can manage it just from the android module by subclassing your ApplicationListener like this:   public class MyLiveWallpaper extends AndroidLiveWallpaperService {      static class MyLiveWallpaperListener extends MyGdxGame implements AndroidWallpaperListener {         @Override         public void offsetChange (float xOffset, float yOffset, float xOffsetStep,                                   float yOffsetStep, int xPixelOffset, int yPixelOffset) {             // Called when the home screen is scrolled. Not all launchers support this.         }          @Override         public void previewStateChange (boolean isPreview) {             // Called when switched between being previewed and running as the wallpaper.         }          @Override         public void iconDropped (int x, int y) {             // Called when an icon is dropped on the home screen.         }     }      @Override     public void onCreateApplication() {         AndroidApplicationConfiguration cfg = new AndroidApplicationConfiguration();          initialize(new MyLiveWallpaperListener(), cfg);     } }   Since libGDX 1.9.12, you can also report the dominant colors of the wallpaper to the OS. Starting with Android 8.1, this is used by some Android launchers and lock screens for styling, such as changing the text color of the clock. You can create a method like this to report the colors, and access it from the core module using the strategy from Interfacing With Platform-Specific Code:   public void notifyColorsChanged (Color primaryColor, Color secondaryColor, Color tertiaryColor) {     Application app = Gdx.app;     if (Build.VERSION.SDK_INT &gt;= 27 &amp;&amp; app instanceof AndroidLiveWallpaper) {         ((AndroidLiveWallpaper) app).notifyColorsChanged(primaryColor, secondaryColor, tertiaryColor);     } }   In additional to the service class, you must also create an xml file in the Android res/xml directory to define some Live Wallpaper properties: its thumbnail and description shown in the wallpaper picker, and an optional settings Activity. Let’s call this file livewallpaper.xml.   &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt; &lt;wallpaper     xmlns:android=\"http://schemas.android.com/apk/res/android\"       android:thumbnail=\"@drawable/ic_launcher\"     android:description=\"@string/description\"     android:settingsActivity=\"com.mypackage.MyLiveWallpaperSettingsActivity\"/&gt;   Finally, you’ll need to add things to your AndroidManifest.xml files. Here’s an example for a Live Wallpaper with a simple settings Activity. The key elements here are the uses-feature and service blocks. The label and icon set on the service appear in the Android application settings. The settings Activity and the Live Wallpaper service must both be set with exported true so they can be accessed by the Live Wallpaper picker.   &lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt; &lt;manifest xmlns:android=\"http://schemas.android.com/apk/res/android\"         package=\"com.mypackage\"&gt;     &lt;uses-feature android:name=\"android.software.live_wallpaper\" /&gt;     &lt;application android:icon=\"@drawable/icon\" android:label=\"@string/app_name\"&gt;         &lt;activity android:name=\".MyLiveWallpaperSettingsActivity\"             android:label=\"@string/app_name\"             android:exported=\"true\" /&gt;         &lt;service android:name=\".LiveWallpaper\"             android:label=\"@string/app_name\"             android:icon=\"@drawable/icon\"             android:exported=\"true\"             android:permission=\"android.permission.BIND_WALLPAPER\"&gt;             &lt;intent-filter&gt;                 &lt;action android:name=\"android.service.wallpaper.WallpaperService\" /&gt;             &lt;/intent-filter&gt;             &lt;meta-data android:name=\"android.service.wallpaper\"                 android:resource=\"@xml/livewallpaper\" /&gt;         &lt;/service&gt;\t\t\t\t  \t     &lt;/application&gt; &lt;/manifest&gt;   Live Wallpapers have some limitations concerning touch input. In general only one pointer will be reported. If you want full multi-touch events you can set the AndroidApplicationConfiguration.getTouchEventsForLiveWallpaper field to true.   Screen Savers (aka Daydreams)  A libGDX core application can also be used as an Android Screen Saver. Screen Savers were once known as Daydreams, so many of the related classes have the term “Daydream” in their names. Screen Savers have no relation to Google’s Daydream VR platform.   The project setup is very similar to an Android game, but AndroidDaydream is used in  place of AndroidApplication. Screen Savers are Android Services, not Activities.   First, extend AndroidDaydream and override onAttachedToWindow() (instead of onCreate() like you would do with a game Activity). It must call through to super. You can also call setInteractive() from this method to enable/disable touch. A non-interactive Screen Saver immediately closes when the screen is touched.   public class MyScreenSaver extends AndroidDaydream {     @Override     public void onAttachedToWindow() {         super.onAttachedToWindow();         setInteractive(true);          AndroidApplicationConfiguration cfg = new AndroidApplicationConfiguration();         initialize(new MyGdxGame(), cfg);     } }   In additional to the service class, you must also create an xml file in the Android res/xml directory to define the only Screensaver setting: an optional settings Activity. Let’s call this file screensaver.xml.   &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt; &lt;dream xmlns:android=\"http://schemas.android.com/apk/res/android\"     android:settingsActivity=\"com.badlogic.gdx.tests.android/.MyScreenSaverSettingsActivity\" /&gt;   Finally, you’ll need to add things to your AndroidManifest.xml files. Here’s an example for a Screen Saver with a simple settings Activity. Note that a settings Activity is optional. The key element is the service block. The settings Activity and the Screen Saver service must both be set with exported true so they can be accessed by the Screen Saver picker.   &lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt; &lt;manifest xmlns:android=\"http://schemas.android.com/apk/res/android\"         package=\"com.mypackage\"&gt;     &lt;application android:icon=\"@drawable/icon\" android:label=\"@string/app_name\"&gt;         &lt;activity android:name=\".MyScreenSaverSettingsActivity\"             android:label=\"@string/app_name\"             android:exported=\"true\" /&gt;         &lt;service android:name=\".MyScreenSaver\"             android:label=\"@string/app_name\"             android:icon=\"@drawable/icon\"             android:exported=\"true\" &gt;             &lt;intent-filter&gt;                 &lt;action android:name=\"android.service.dreams.DreamService\" /&gt;                 &lt;category android:name=\"android.intent.category.DEFAULT\" /&gt;             &lt;/intent-filter&gt;             &lt;meta-data android:name=\"android.service.dream\"                 android:resource=\"@xml/screensaver\" /&gt;         &lt;/service&gt;\t\t\t  \t     &lt;/application&gt; &lt;/manifest&gt;   iOS/Robovm   [ToDo]   HTML5/GWT  The main entry-point for an HTML5/GWT application is a GwtApplication. Open GwtLauncher.java in the my-gdx-game-html5 project:   package com.me.mygdxgame.client;  import com.me.mygdxgame.MyGdxGame; import com.badlogic.gdx.ApplicationListener; import com.badlogic.gdx.backends.gwt.GwtApplication; import com.badlogic.gdx.backends.gwt.GwtApplicationConfiguration;  public class GwtLauncher extends GwtApplication {    @Override    public GwtApplicationConfiguration getConfig () {       GwtApplicationConfiguration cfg = new GwtApplicationConfiguration(480, 320);       return cfg;    }     @Override    public ApplicationListener createApplicationListener () {       return new MyGdxGame();    } }   The main entry-point is composed of two methods, GwtApplication.getConfig() and GwtApplication.createApplicationListener(). The former has to return a GwtApplicationConfiguration instance, which specifies various configuration settings for the HTML5 application. The GwtApplication.createApplicatonListener() method returns the ApplicationListener to run.   Module Files  GWT needs the actual Java code for each jar/project that is referenced. Additionally, each of these jars/projects needs to have one module definition file, having the suffix gwt.xml.   In the example project setup, the module file of the html5 project looks like this:   &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt; &lt;!DOCTYPE module PUBLIC \"-//Google Inc.//DTD Google Web Toolkit trunk//EN\" \"http://google-web-toolkit.googlecode.com/svn/trunk/distro-source/core/src/gwt-module.dtd\"&gt; &lt;module&gt;    &lt;inherits name='com.badlogic.gdx.backends.gdx_backends_gwt' /&gt;    &lt;inherits name='MyGdxGame' /&gt;    &lt;entry-point class='com.me.mygdxgame.client.GwtLauncher' /&gt;    &lt;set-configuration-property name=\"gdx.assetpath\" value=\"../my-gdx-game-android/assets\" /&gt; &lt;/module&gt;   This specifies two other modules to inherit from (gdx-backends-gwt and the core project) as well as the entry-point class (GwtLauncher above) and a path relative to the html5 project’s root directory, pointing to the assets directory.   Both the gdx-backend-gwt jar and the core project have a similar module file, specifying other dependencies. You cannot use jars/projects which do not contain a module file and source!   For more information on modules and dependencies refer to the GWT Developer Guide.   GWT Specifics   The HTML backend has a number of caveats. Be sure to check out the  comprehensive HTML Backend Guide!   GWT does not support Java reflection for various reasons. libGDX has an internal emulation layer that will generate reflection information for a select few internal classes. This means that if you use the Json serialization capabilities of libGDX, you’ll run into issues. You can fix this by specifying for which packages and classes reflection information should be generated for. To do so, take a look at the Reflection Guide.   A libGDX HTML5 application preloads all assets found in the gdx.assetpath. During this loading process, a loading screen is displayed which is implemented via GWT widget. If you want to customize this loading screen, you can simply overwrite the GwtApplication.getPreloaderCallback() method (in GwtLauncher in the above example). An example can be found here.  ",
        
        "url": "/wiki/app/starter-classes-and-configuration" },{
        "title": "Streaming music",
        "excerpt":
        
        "For any sound that’s longer than a few seconds it is preferable to stream it from disk instead of fully loading it into RAM. libGDX provides a Music interface that lets you do that.   To load a Music instance we can do the following:   Music music = Gdx.audio.newMusic(Gdx.files.internal(\"data/mymusic.mp3\"));   This loads an MP3 file called \"mymusic.mp3\" from the internal directory data.   Playing back the music instance works as follows:   music.play();   Of course you can set various playback attributes of the Music instance:   music.setVolume(0.5f);                 // sets the volume to half the maximum volume music.setLooping(true);                // will repeat playback until music.stop() is called music.stop();                          // stops the playback music.pause();                         // pauses the playback music.play();                          // resumes the playback boolean isPlaying = music.isPlaying(); // obvious :) boolean isLooping = music.isLooping(); // obvious as well :) float position = music.getPosition();  // returns the playback position in seconds   Music instances are heavy on some backends (such as Android), you should usually not have more than about 10 loaded and more than 1 or 2 playing at the same time.   A Music instance needs to be disposed if it is no longer needed, to free up resources.   music.dispose();  ",
        
        "url": "/wiki/audio/streaming-music" },{
        "title": "Table",
        "excerpt":
        
        "Overview   Table is a WidgetGroup that sets the position and size of its children using a logical table, similar to HTML tables. Tables are intended to be used extensively in scene2d.ui to layout widgets, as they are easy to use and much more powerful than manually sizing and positioning widgets. Table-based layouts don’t rely on absolute positioning and therefore automatically adjust to different widget sizes and screen resolutions.   Table is a fork of TableLayout for libGDX (by the same author). TableLayout provides the same functionality as Table but for other UI toolkits (Swing, Android, etc).      Quickstart   Root table   Debugging   Adding cells   Logical table   Cell properties            Expand       Alignment       Fill       Widget size       Padding       Spacing       Colspan       Uniform           Defaults            Cell defaults       Column defaults       Row defaults           Values   Overlapping widgets   Inserting cells   Notes   Quickstart       Label nameLabel = new Label(\"Name:\", skin);     TextField nameText = new TextField(\"\", skin);     Label addressLabel = new Label(\"Address:\", skin);     TextField addressText = new TextField(\"\", skin);          Table table = new Table();     table.add(nameLabel);     table.add(nameText).width(100);     table.row();     table.add(addressLabel);     table.add(addressText).width(100);      This code adds 4 cells to the table which are arranged in two columns and two rows. The add method returns a Cell, which has methods to control layout. The table sizes and positions its children, so setting the width of the text fields to 100 is done on the table cell, not on the text fields themselves.   Root table   When doing UI layout, a UI widget does not set its own size. Instead, it provides a minimum, preferred, and maximum size. The widget’s parent uses its own size along with these hints to size the widget. Many layouts will use a single table at the root which has a fixed size, often the whole screen. Widgets and nested tables are added to the root table, resulting in a responsive layout that doesn’t rely on fixed positions or fixed sizes.   In libgdx the setFillParent method can be used to easily size the root table to the stage (but should generally only be used on the root table):       Table table = new Table();     table.setFillParent(true);     stage.addActor(table);   Debugging   Table can draw debug lines to visualize what is happening in the layout. It is enabled following way:  table.setDebug(true); // turn on all debug lines (table, cell, and widget)   Adding cells   Widgets are added to a table with the add method. This adds a cell to the current row. To move to the next row, call the row method.       table.add(nameLabel);     table.add(nameText);     table.row();     table.add(addressLabel);     table.add(addressText);   The add method returns a Cell, which has properties that control the layout. Every method on the cell returns the cell, allowing calls to be chained.       table.add(nameText).padLeft(10).width(100);   Logical table   The cells make up a logical table, but it is not sized to the table widget.      The outer blue rectangle shows the size of the table widget. The inner blue rectangle shows the size of the logical table, which is aligned to center by default. The alignment can be changed using methods on the table. The table methods return the table, so can be chained just like the cell methods.       table.right().bottom();      Cell properties   Expand   To make the logical table take up the entire size of the table widget, TableLayout needs to be told which cells will receive the extra space.       table.add(nameLabel).expandX();     table.add(nameText).width(100);     table.row();     table.add(addressLabel);     table.add(addressText).width(100);      The red lines show the cell bounds and the green lines show the widget bounds. Note that the left column has received all of the extra space in the x direction. Only one cell needs to have expand to cause the entire column or row to expand. If multiple columns expand, the extra space is distributed evenly.       table.add(nameLabel).expandX();     table.add(nameText).width(100).expandX();     table.row();     table.add(addressLabel);     table.add(addressText).width(100);      Expand also works in the y direction via the expandY method. The expand method causes expand to happen in both directions.       table.add(nameLabel).expand();     table.add(nameText).width(100);     table.row();     table.add(addressLabel);     table.add(addressText).width(100);      Alignment   Similar to aligning the logical table, a widget can be aligned inside the cell.       table.add(nameLabel).expand().bottom().right();     table.add(nameText).width(100).top();     table.row();     table.add(addressLabel);     table.add(addressText).width(100);      Fill   The fill method causes a widget to be sized to the cell. Like expand, there are also fillX and fillY methods.       table.add(nameLabel).expand().bottom().fillX();     table.add(nameText).width(100).top();     table.row();     table.add(addressLabel);     table.add(addressText).width(100);      Note the red cell lines are drawn on top of the green widget lines.   Widget size   By default, the table attempts to size widgets to their preferred size. If the widgets don’t fit, they are sized between their preferred size and their minimum size, with widgets that have a larger preferred size receiving more space. If the widgets don’t fit at their minimum size then the layout is broken and widgets may overlap. The fill methods won’t make a widget larger than the widget’s maximum size.   It is a common beginner’s mistake to attempt to size or position a widget in a table by setting the size on the widget. The table sizes and positions its children using their preferred, minimum, or maximum size, so the table will overwrite any size or position set previously.   Widgets do not provide setters for their preferred, minimum, or maximum size. These values are typically computed by the widget, so would be confusing if they could also be set explicitly. Also, it is not advised to subclass a widget to change these values. Instead, set the preferred, minimum, or maximum size on the cell containing the widget and your value will be used instead of the widget’s value.       table.add(nameLabel);     table.add(nameText).minWidth(100);     table.row();     table.add(addressLabel);     table.add(addressText).prefWidth(999);      Here the prefWidth of 999 is larger than the table, so it is sized down to fit.   width is a shortcut method for setting minWidth, prefWidth, and maxWidth to the same value. height is a shortcut method for setting minHeight, prefHeight, and maxHeight to the same value. The size method takes a width and a height and sets all six properties.   Padding   Padding is extra space around the edges of a cell.       table.add(nameLabel);     table.add(nameText).width(100).padBottom(10);     table.row();     table.add(addressLabel);     table.add(addressText).width(100).pad(10);      Note that padding between cells combines, so there are 20 pixels between the text fields. The debug lines don’t necessarily show which cell the padding comes from, since it is not important for the layout of the table.   Padding can also be applied to the edges of the table.       table.pad(10);   Spacing   Like padding, spacing is extra space around the edges of a cell. However, spacing between cells does not combine, instead the larger of the two is used. Also, spacing is not applied at the edge of the table. Spacing makes it easy to have consistent space between cells, eg for forms.       table.add(nameLabel);     table.add(nameText).width(100).spaceBottom(10);     table.row();     table.add(addressLabel);     table.add(addressText).width(100).space(10);      Note that the spacing between cells doesn’t combine, so there are 10 pixels between the text fields. Also note that there is no spacing under the bottom text field because spacing isn’t applied around the edge of the table.   Colspan   A cell can span multiple columns.       table.add(nameLabel);     table.add(nameText).width(100).spaceBottom(10);     table.row();     table.add(addressLabel).colspan(2);      Note that there is no rowspan. To acheive this, use a nested table.   Uniform   Cells with uniform set to true will be the same size.       table.add(nameLabel).uniform();     table.add(nameText).width(100).uniform();     table.row();     table.add(addressLabel);     table.add(addressText).width(100);      Defaults   Cell defaults   Often many cells have the same properties, so setting the default properties for all cells can greatly reduce the code needed for a layout. The defaults method on the table returns a cell whose properties are the defaults for all cells.       table.defaults().width(100);     table.add(nameLabel);     table.add(nameText);     table.row();     table.add(addressLabel);     table.add(addressText);      Column defaults   The columnDefaults method on the table returns a cell whose properties are the defaults for all cells in that column. Any properties set here will override the cell default properties. Columns are indexed starting at 0.       table.columnDefaults(1).width(150);     table.add(nameLabel);     table.add(nameText);     table.row();     table.add(addressLabel);     table.add(addressText);      Row defaults   When the row method is called, it returns a cell whose properties are the defaults for all cells in that row. Any properties set here will override both the cell default properties and the column default properties. Note it is allowed to call row before any cells are added. This allows the first row to have row default properties.       table.row().height(50);     table.add(nameLabel);     table.add(nameText);     table.row().height(100);     table.add(addressLabel);     table.add(addressText);      Values   All values for sizes, padding, etc are actually Value instances. When numeric values are used, Value.Fixed is used. Value allows for more flexibility, eg to base a size or padding on another widget.       table.add(nameLabel);     table.add(nameText).pad(new Value.Fixed(10));     table.row();     table.add(addressLabel).width(Value.percentWidth(0.33f));     table.add(addressText);   Overlapping widgets   Table excels at sizing and positioning widgets that do not overlap. To layer widgets on top of each other, a Stack can be used.   Inserting cells   Table allows a cell’s widget to be changed or removed (by setting it to null), but Table currently does not allow cells to be inserted in the middle or removed. To do that, the Table needs to be rebuilt: call clearChildren to remove all children and cells, then add them all to the Table again.   If inserting or removing cells is needed, VerticalGroup or HorizontalGroup can be used.   Notes      By default, positions and sizes are rounded to integers. This may cause problems when using small values. Use Table#setRound(false) to change it.  ",
        
        "url": "/wiki/graphics/2d/scene2d/table" },{
        "title": "Taking a Screenshot",
        "excerpt":
        
        "Screenshots are easy in libGDX!   Post processing to guarantee clarity   This will guarantee your screenshots look like just like what the user expects:   Pixmap pixmap = Pixmap.createFromFrameBuffer(0, 0, Gdx.graphics.getBackBufferWidth(), Gdx.graphics.getBackBufferHeight()); ByteBuffer pixels = pixmap.getPixels();  // This loop makes sure the whole screenshot is opaque and looks exactly like what the user is seeing int size = Gdx.graphics.getBackBufferWidth() * Gdx.graphics.getBackBufferHeight() * 4; for (int i = 3; i &lt; size; i += 4) { \tpixels.put(i, (byte) 255); }  PixmapIO.writePNG(Gdx.files.external(\"mypixmap.png\"), pixmap, Deflater.DEFAULT_COMPRESSION, true); pixmap.dispose();   The GWT backend has some limitations in this regard, which require additional steps outlined here.   No post-processing   If you have no layered transparency, here’s a more compact and efficient way:   Pixmap pixmap = Pixmap.createFromFrameBuffer(0, 0, Gdx.graphics.getWidth(), Gdx.graphics.getHeight()); PixmapIO.writePNG(Gdx.files.external(\"mypixmap.png\"), pixmap, Deflater.DEFAULT_COMPRESSION, true); pixmap.dispose();  ",
        
        "url": "/wiki/graphics/taking-a-screenshot" },{
        "title": "Texture Compression",
        "excerpt":
        
        "If you need texture compression, offline mipmap generation, or cubemaps, the default texture formats like PNG won’t be sufficient. Luckily libGDX provides 2 options for this ETC1 files and KTX/ZKTX textures.   Note that for the GWT backend ETC1 and KTX/ZKTX is currently not supported.   Before going into details, there are 2 types of compression to be aware of ;     compression used to store the texture on disk (zip, png, jpg,…), which is useful to reduce the size of your package,   compression used to store the texture in memory (ETC1, ETC2, S3TC,…), which improves your game performance and minimizes the memory footprint of your application at runtime (hence reducing the risk that Android performs an App restart on resume).   OpenGL ES 2.0 has only one mandatory texture compression format on Android: ETC1 (this format is not available on iOS). It allows to decrease the size of any RGB8 image by a 6x factor. The main drawback is that it is a lossy compression format. The other is that it is limited to RGB8. To support alpha channel, you have to to store alpha separately :     either in another texture which can be ETC1 compressed as well,   or in the same texture, putting the color part of your image in the top, and the alpha part in the bottom.   The video memory savings will then drops from 6x to 4x, but are still worth the effort. An example is given in KTXTest.   ETC1 File Format   ETC1 file format is a very simple format specific to libGDX (see this blog post). It gives a straight forward way to support 2D texture ETC1 compressed. The drawback is that it won’t give you the ability to use mipmaps or cubemaps.   Compression  Compressing a Pixmap loaded from a file and writing it to our custom ETC1 file format is pretty simple:  Pixmap pixmap = new Pixmap(Gdx.files.absolute(\"image.png\")); ETC1.encodeImagePKM(pixmap).write(Gdx.files.absolute(\"image.etc1\"));  You can also use the ETC1Compressor tool in the gdx-tools project which can convert entire directory hierarchies.   Loading  Once you have your ETC1 compressed image in a file, you can easily load it like any other image file:  Texture texture = new Texture(Gdx.files.internal(\"image.etc1\"));   KTX/ZKTX Format   KTX file format is a standard dedicated to storing OpenGL textures. Its main advantage is that it supports most features of OpenGL Textures (all compression formats, with or without mipmaps, cubemaps, texture arrays,…).   The ZKTX format is just a zipped KTX to limit the size of the file on disk.   Preparing your file  The KTXProcessor tool in the gdx-tools project provides a simple way to prepare your textures:  usage : KTXProcessor input_file output_file [-etc1|-etc1a] [-mipmaps]   input_file  is the texture file to include in the output KTX or ZKTX file.               for cube map, just provide 6 input files corresponding to the faces in the following order : X+, X-, Y+, Y-, Z+, Z-   output_file is the path to the output file, its type is based on the extension which must be either KTX or ZKTX    options:     -etc1    input file will be packed using ETC1 compression, dropping the alpha channel     -etc1a   input file will be packed using ETC1 compression, doubling the height and placing the alpha channel in the bottom half     -mipmaps input file will be processed to generate mipmaps    examples:     KTXProcessor in.png out.ktx                                        Create a KTX file with the provided 2D texture     KTXProcessor in.png out.zktx                                       Create a Zipped KTX file with the provided 2D texture     KTXProcessor in.png out.zktx -mipmaps                              Create a Zipped KTX file with the provided 2D texture, generating all mipmap levels     KTXProcessor px.ktx nx.ktx py.ktx ny.ktx pz.ktx nz.ktx out.zktx    Create a Zipped KTX file with the provided cubemap textures     KTXProcessor in.ktx out.zktx                                       Convert a KTX file to a Zipped KTX file   There are also lots of third party tools to prepare KTX texture files:     The OpenGL SDK provides tools to create KTX file,   The Mali SDK provides tools to create KTX file including alpha channel processing.   Loading  Once you have your KTX or ZKTX compressed image in a file, you can easily load it like any other image file:   Texture texture = new Texture(Gdx.files.internal(\"image.zktx\"));  ",
        
        "url": "/wiki/graphics/2d/texture-compression" },{
        "title": "Texture packer",
        "excerpt":
        
        "   TexturePacker            Running TexturePacker       Directory structure       Configuration       Settings       Texture filter options       NinePatches       Indexes       Packing       Automatic packing           TextureAtlas   TexturePacker   In OpenGL, a texture is bound, some drawing is done, another texture is bound, more drawing is done, etc. Binding the texture is relatively expensive, so it is ideal to store many smaller images on a larger image, bind the larger texture once, then draw portions of it many times. libGDX has a TexturePacker class which is a command line application that packs many smaller images on to larger images. It stores the locations of the smaller images so they are easily referenced by name in your application using the TextureAtlas class.   TexturePacker uses multiple packing algorithms but the most important is based on the maximal rectangles algorithm. It also uses brute force, packing with numerous heuristics at various sizes and then choosing the most efficient result.   Running TexturePacker   GUIs   If you prefer to pack your textures using a GUI, you can use Texture Packer GUI. If you are using Scene2d Skins, you probably already use Skin Composer and can use its user-friendly interface to add your textures to the Skin’s atlas.   From source  The TexturePacker class is in the gdx-tools project. It can be run from source via Eclipse:   import com.badlogic.gdx.tools.texturepacker.TexturePacker; public class MyPacker { \tpublic static void main (String[] args) throws Exception { \t\tTexturePacker.process(inputDir, outputDir, packFileName); \t} }   If you use gradle and the TexturePacker class is not found, add gdx-tools to your build.gradle file.   You can also run texturePacker as a gradle task if you make the following updates to your gradle files. First, you will need to update your ‘main’ build.gradle:   buildscript {   dependencies {     // ... other dependencies trimmed ...     classpath \"com.badlogicgames.gdx:gdx-tools:$gdxVersion\"     } }   If you want to use specific version, just replace the $gdxVersion variable with the version of your choice   // Store the parameters you want to pass the texturePacker here... project.ext.texturePacker = [ \"android/assets/input/path/\", \"android/assets/output/path/\", \"atlas_name\" ]  // Import the texture packer import com.badlogic.gdx.tools.texturepacker.TexturePacker  // Add a new task that packs the textures for you task texturePacker {   doLast {     if (project.ext.has('texturePacker')) {       logger.info \"Calling TexturePacker: \"+texturePacker       TexturePacker.process(texturePacker[0], texturePacker[1], texturePacker[2])     }   } }   In this way, running ./gradlew texturePacker desktop:run will perform the texture packing before the desktop:run task is started. And if the textures have not changed, then all one has to do is omit the texturePacker argument.   TexturePacker can also be run from the standalone nightly:   // OS X / Linux java -cp runnable-texturepacker.jar com.badlogic.gdx.tools.texturepacker.TexturePacker inputDir [outputDir] [packFileName]  // WINDOWS java -cp runnable-texturepacker.jar com.badlogic.gdx.tools.texturepacker.TexturePacker inputDir [outputDir] [packFileName]   Note that TexturePacker runs significantly faster with Java 1.7+, especially when packing hundreds of input images.   Directory structure   TexturePacker can pack all images for an application in one shot. Given a directory, it recursively scans for image files. For each directory of images TexturePacker encounters, it packs the images on to a larger texture, called a page. If the images in a directory don’t fit on the max size of a single page, multiple pages will be used.   Images in the same directory go on the same set of pages. If all images fit on a single page, no subdirectories should be used because with one page the app will only ever perform one texture bind. Otherwise, subdirectories can be used to segregate related images to minimize texture binds. Eg, an application may want to place all the “game” images in a separate directory from the “pause menu” images, since these two sets of images are drawn serially: all the game images are drawn (one bind), then the pause menu is drawn on top (another bind). If the images were in a single directory that resulted in more than one page, each page could contain a mix of game and pause menu images. This would cause multiple texture binds to render the game and pause menu instead of just one each.   Subdirectories are also useful to group images with related texture settings. Settings like runtime memory format (RGBA, RGB, etc) and filtering (nearest, linear, etc) are per texture. Images that need different per texture settings need to go on separate pages, so should be placed in separate subdirectories.   To use subdirectories for organization without TexturePacker outputting a set of pages for each subdirectory, see the combineSubdirectories setting.   To avoid subdirectory paths being used in image names in the atlas file, see the flattenPaths setting.   Configuration   Each directory may contain a “pack.json” file, which is a JSON representation of the TexturePacker.Settings class. Each subdirectory inherits all the settings from its parent directory. Any settings set in the subdirectory override those set in the parent directory.   Below is a JSON example with every available setting and the default value for each. All settings do not need to be specified, any or all may be omitted. If a setting is not specified for a directory or any parent directory, the default value is used.   { \tpot: true, \tpaddingX: 2, \tpaddingY: 2, \tbleed: true, \tbleedIterations: 2, \tedgePadding: true, \tduplicatePadding: false, \trotation: false, \tminWidth: 16, \tminHeight: 16, \tmaxWidth: 1024, \tmaxHeight: 1024, \tsquare: false, \tstripWhitespaceX: false, \tstripWhitespaceY: false, \talphaThreshold: 0, \tfilterMin: Nearest, \tfilterMag: Nearest, \twrapX: ClampToEdge, \twrapY: ClampToEdge, \tformat: RGBA8888, \talias: true, \toutputFormat: png, \tjpegQuality: 0.9, \tignoreBlankImages: true, \tfast: false, \tdebug: false, \tcombineSubdirectories: false, \tflattenPaths: false, \tpremultiplyAlpha: false, \tuseIndexes: true, \tlimitMemory: true, \tgrid: false, \tscale: [ 1 ], \tscaleSuffix: [ \"\" ], \tscaleResampling: [ bicubic ], \tatlasExtension: .atlas, \tprettyPrint: true, \tlegacyOutput: true }   Note that this is libgdx’s “minimal” JSON format, so double quotes are optional in most cases.   Settings                  Field       Description       Default                       pot       If true, output pages will have power of two dimensions.       true                 paddingX       The number of pixels between packed images on the x-axis.       2                 paddingY       The number of pixels between packed images on the y-axis.       2                 bleed       If true, RGB values for transparent pixels are set based on the RGB values of the nearest non-transparent pixels. This prevents filtering artifacts when RGB values are sampled for transparent pixels.       true                 bleedIterations       The amount of bleed iterations that should be performed. Use greater values such as 4 or 8 if you’re having artifacts when downscaling your textures.       2                 edgePadding       If true, half of the paddingX and paddingY will be used around the edges of the packed texture.       true                 duplicatePadding       If true, edge pixels are copied into the padding. paddingX/Y should be &gt;= 2.       false                 rotation       If true, TexturePacker will attempt more efficient packing by rotating images 90 degrees. Applications must take special care to draw these regions properly.       false                 minWidth       The minimum width of output pages.       16                 minHeight       The minimum height of output pages.       16                 maxWidth       The maximum width of output pages. 1024 is safe for all devices. Extremely old devices may have degraded performance over 512.       1024                 maxHeight       The maximum height of output pages. 1024 is safe for all devices. Extremely old devices may have degraded performance over 512.       1024                 square       If true, output pages are forced to have the same width and height.       false                 stripWhitespaceX       If true, blank pixels on the left and right edges of input images will be removed. Applications must take special care to draw these regions properly.       false                 stripWhitespaceY       If true, blank pixels on the top and bottom edges of input images will be removed. Applications must take special care to draw these regions properly.       false                 alphaThreshold       From 0 to 255. Alpha values below this are treated as zero when whitespace is stripped.       0                 filterMin       The minification filter for the texture.       Nearest                 filterMag       The magnification filter for the texture.       Nearest                 wrapX       The wrap setting in the x direction for the texture.       ClampToEdge                 wrapY       The wrap setting in the y direction for the texture.       ClampToEdge                 format       The format the texture will use in-memory.       RGBA8888                 alias       If true, two images that are pixel for pixel the same will only be packed once.       true                 outputFormat       The image type for output pages, “png” or “jpg”.       png                 jpegQuality       From 0 to 1. The quality setting if outputFormat is “jpg”.       0.9                 ignoreBlankImages       If true, texture packer won’t add regions for completely blank images.       true                 fast       If true, the texture packer will not pack as efficiently but will execute much faster.       false                 debug       If true, lines are drawn on the output pages to show the packed image bounds.       false                 combineSubdirectories       If true, the directory containing the settings file and all subdirectories are packed as if they were in the same directory. Any settings files in the subdirectories are ignored.       false                 flattenPaths       If true, subdirectory prefixes are stripped from image file names. Image file names should be unique.       false                 premultiplyAlpha       If true, the RGB will be multiplied by the alpha. See here for more information.       false                 useIndexes       If false, image names are used without stripping any image index suffix.       true                 limitMemory       If true, only one image is in memory at any given time, but each image will be read twice. If false, all images are kept in memory during packing but are only read once.       true                 grid       If true, images are packed in a uniform grid, in order.       false                 scale       For each scale, the images are scaled and an entire atlas is output.       [ 1 ]                 scaleSuffix       For each scale, the suffix to use for the output files. If omitted, files for multiple scales will be output with the same name to a subdirectory for each scale.       [ \"\" ]                 scaleResampling       For each scale, the type of interpolation used for resampling the source to the scaled size. One of nearest, bilinear or bicubic.       [ bicubic ]                 atlasExtension       The file extension to be appended to the atlas filename.       .atlas                 prettyPrint       If true, removes all whitespace except newlines.       true                 legacyOutput       If true, the atlas uses a less efficient output format. Exists for backwards-compatibility reasons.       true           Texture filter options   Texture packer use the filters specified in the Texture.TextureFilter enum. The options for filterMin and filterMag are as following:  Nearest: no filtering, no mipmaps  Linear: filtering, no mipmaps  MipMap &amp; MipMapLinearLinear: filtering, smooth transition between mipmaps  MipMapNearestNearest: no filtering, sharp switching between mipmaps  MipMapLinearNearest: filtering, sharp switching between mipmaps  MipMapNearestLinear: no filtering, smooth transition between mipmaps    NinePatches   If an image file name ends with “.9” just before the file extension, it is considered a ninepatch. See ninepatches. The image must have a 1px transparent border. The upper and left edge may optionally have one contiguous line of black pixels which denote the split information, ie what part of the ninepatch will stretch. The bottom and right edge may optionally have one contiguous line of black pixels which denote the padding information, ie how content on top of the NinePatch should be inset. When this image is packed, the 1px border is removed and the split and padding information stored in the pack file. TextureAtlas allows an instance of NinePatch to created for the region using the split information.   Image indexes   If an image file name ends with underscore and then a number (eg animation_23.png), the number is considered the “index” and is stored separately. The image name is stored without the underscore and index. TextureAtlas allows a list of all images with the same name to be retrieved, ordered by index. This makes it easy to pack animations without losing the order of the frames.   Packing   The TexturePacker class is in gdx-tools.jar, which is in the extensions directory of the nightlies/releases zip files. You only need TexturePacker as a tool to process your image files for your application, you don’t need it as a dependency to run your application. To run the packer you need both gdx.jar and gdx-tools.jar. Note: gdx.jar must be in the same directory as gdx-tools.jar, for it to run without exceptions   //*NIX (OS X/Linux) java -cp gdx.jar:gdx-tools.jar com.badlogic.gdx.tools.texturepacker.TexturePacker inputDir outputDir packFileName  //WINDOWS java -cp gdx.jar;gdx-tools.jar com.badlogic.gdx.tools.texturepacker.TexturePacker inputDir outputDir packFileName   TexturePacker can also be run from the standalone nightly without gdx.jar (i.e. without the rest of libGDX at all), just substitute runnable-texturepacker.jar for gdx.jar;gdx-tools.jar in the above.   inputDir is the root directory containing the images. outputDir is the output directory where the packed images will be placed. packFileName is the name of the pack file and the prefix used for the output packed image files.   If outputDir is omitted, files will be placed in a new directory that is a sibling to inputDir with the suffix “-packed”. If packFileName is omitted, “pack” is used.   While texture packing is intended to be a fully automated process, there has also been a nice UI contributed by Obli (though slightly out of date): TexturePacker GUI (check out its up to date successor). There is also a commercial product at texturepacker.com which is completely unrelated to libgdx’s texture packer and has a UI, many features and nice documentation.   Automatic packing   During development it can be convenient to have the desktop application run TexturePacker before starting the game:   public class DesktopGame { \tpublic static void main (String[] args) throws IOException { \t\tSettings settings = new Settings(); \t\tsettings.maxWidth = 512; \t\tsettings.maxHeight = 512; \t\tTexturePacker.process(settings, \"../images\", \"../game-android/assets\", \"game\");  \t\tnew LwjglApplication(new Game(), \"Game\", 320, 480, false); \t} }   Each time the game is run, all the images are packed. This can be especially convenient when giving a build to an artist, who can then try out new images without even knowing the game is using packed images. If many images are packed, the fast setting can be useful to avoid waiting.   Note: When loading files from the classpath, Eclipse usually will not reflect changes to files that are updated externally. The project with the changed files must be manually refreshed in Eclipse. During development files can be loaded through the filesystem instead, where this is not an issue.   TextureAtlas   The TexturePacker output is a directory of page images and a text file that describes all the images packed on the pages. This shows how to use the images in an application:   TextureAtlas atlas; atlas = new TextureAtlas(Gdx.files.internal(\"packedimages/pack.atlas\")); AtlasRegion region = atlas.findRegion(\"imagename\"); Sprite sprite = atlas.createSprite(\"otherimagename\"); NinePatch patch = atlas.createPatch(\"patchimagename\");   TextureAtlas reads the pack file and loads all the page images. TextureAtlas.AtlasRegions can be retrieved, which are TextureRegions that provides extra information about the packed image, such as the frame index or any whitespace that was stripped. Sprites and NinePatches can also be created. If whitespace was stripped, the created Sprite will actually be a TextureAtlas.AtlasSprite, which allows the sprite to be used (mostly) as if whitespace was never stripped.   Note that findRegion is not very fast, so the value returned should be stored rather than calling this method each frame. Also note that createSprite and createNinePatch allocate a new instance.   TextureAtlas holds on to all the page textures, disposing the TextureAtlas will dispose all the page textures.  ",
        
        "url": "/wiki/tools/texture-packer" },{
        "title": "The application framework",
        "excerpt":
        
        "Modules  At its core, libGDX consists of six modules in the form of interfaces that provide means to interact with the operating system. Each backend implements these interfaces.      Application: runs the application and informs an API client about application level events, such as window resizing. Provides logging facilities and querying methods, e.g., memory usage.   Files: exposes the underlying file system(s) of the platform. Provides an abstraction over different types of file locations on top of a custom file handle system (which does not inter-operate with Java’s File class).   Input: informs the API client of user input such as mouse, keyboard, touch or accelerometer events. Both polling and event driven processing are supported.   Net: provides means to access resources via HTTP/HTTPS in a cross-platform way, as well as create TCP server and client sockets.   Audio: provides means to playback sound effects and streaming music as well as directly accessing audio devices for PCM audio input/output.   Graphics: exposes OpenGL ES 2.0 (where available) and allows querying/setting video modes and similar things.   Starter Classes  The only platform specific code that needs to be written, are so called starter classes. For each platform that is targeted, a piece of code will instantiate a concrete implementation of the Application interface, provided by the back-end for the platform. For the desktop, this might look something like this, using the LWJGL 3 backend:   public class DesktopLauncher {    public static void main(String[] args) {       Lwjgl3ApplicationConfiguration config = new Lwjgl3ApplicationConfiguration();       new Lwjgl3Application(new MyGdxGame(), config);    } }   For Android, the corresponding starter class might look like this:   public class AndroidStarter extends AndroidApplication {    public void onCreate(Bundle bundle) {       super.onCreate(bundle);       AndroidApplicationConfiguration config = new AndroidApplicationConfiguration();       initialize(new MyGame(), config);    } }   These two classes usually live in separate projects, e.g., a desktop and an Android project. The Project Generation page describes the layout of these projects.   The actual code of the application is located in a class that implements the ApplicationListener interface (MyGame in the above example). An instance of this class is passed to the respective initialization methods of each back-end’s Application implementation (see above). The application will then call into the methods of the ApplicationListener at appropriate times (see The Life-Cycle).   See Starter Classes &amp; Configuration for details on starter classes.   Accessing Modules  The modules described earlier can be accessed via static fields of the Gdx class. This is essentially a set of global variables that allows easy access to any module of libGDX. While generally viewed as bad coding practice, we decided on using this mechanism to ease the pain usually associated with passing around references to things that are used often in all kinds of places within the code base.   To access, for example, the audio module one can simply write the following:   // creates a new AudioDevice to which 16-bit PCM samples can be written AudioDevice audioDevice = Gdx.audio.newAudioDevice(44100, false);   Gdx.audio is a reference to the backend implementation that has been instantiated on application startup by the Application instance. Other modules are accessed in the same fashion, e.g., Gdx.app to get the Application, Gdx.files to access the Files implementation and so on.  ",
        
        "url": "/wiki/app/the-application-framework" },{
        "title": "The life cycle",
        "excerpt":
        
        "A libGDX application has a well defined life-cycle, governing the states of an application, like creating, pausing and resuming, rendering and disposing the application.   ApplicationListener  An application developer hooks into these life-cycle events by implementing the ApplicationListener interface and passing an instance of that implementation to the Application implementation of a specific back-end (see The Application Framework). From there on, the Application will call the ApplicationListener every time an application level event occurs. A bare-bones ApplicationListener implementation may look like this:   public class MyGame implements ApplicationListener {    public void create () {    }     public void render () {            }     public void resize (int width, int height) {    }     public void pause () {    }     public void resume () {    }     public void dispose () {    } }   One can also derive from the ApplicationAdapter class, which provides empty default implementations for those methods.   Once passed to the Application, the ApplicationListener methods will be called as follows:                  Method signature       Description                       create ()       Method called once when the application is created.                 resize (int width, int height)       This method is called every time the game screen is re-sized and the game is not in the paused state. It is also called once just after the create() method.  The parameters are the new width and height the screen has been resized to in pixels.                 render ()       Method called by the game loop from the application every time rendering should be performed. Game logic updates are usually also performed in this method.                 pause ()       On Android this method is called when the Home button is pressed or an incoming call is received. On desktop this is called when the window is minimized and just before dispose() when exiting the application.  A good place to save the game state.                 resume ()       This method is called on Android, when the application resumes from a paused state, and on desktop when unminimized.                 dispose ()       Called when the application is destroyed. It is preceded by a call to pause().           The following diagram illustrates the life-cycle visually:      Where is the main loop?  libGDX is event driven by nature, mostly due to the way Android and Javascript work. An explicit main loop does not exist, however, the ApplicationListener.render() method can be regarded as the body of such a main loop.   See also  libGDX and Android lifecycle if you are aiming for Android. The article also explains why you should not use static variables.  ",
        
        "url": "/wiki/app/the-life-cycle" },{
        "title": "Third Party Extension Support",
        "excerpt":
        
        "Third Party Extension Support in the libGDX setup    The libGDX setup includes a section for 3rd party extensions, these are extensions made by members of the community that aren’t managed by the libGDX development team. This gives users an easy way to generate projects that depend on these 3rd party extensions without having to edit build scripts themselves. (Not that that is terribly difficult).   How to get your extension in the libGDX setup  Requirements     A living breathing extension   Approval by the libGDX core development team   Extension definition in the libGDX setup repository     Am I an extension?  As much as you may try… no you are not (Unless you are an advanced AI extension, in which case, citation needed), but you may be developing one!   Does your project aim to extend libGDX with a specific goal in mind?  Is your project useful to libGDX users?  Is your project well established?  Is your project being pushed to Maven central?   Congratulations! It’s an extension!   Approval  To get your beautiful extension in the setup, you must sneak past/bribe/bewitch the libGDX developers into thinking that your extension belongs in the setup. To do this, make sure:      Your project is open source, for security issues   Your project is well established   Your project is well maintained, (we will remove it if it becomes unsupported/not maintained!)   Extension definition  We use a simple xml file in the libGDX core repository to define external extensions.   The file can be found here   An example of this file:  &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt; &lt;extensions&gt;     &lt;extension&gt;        &lt;name&gt;My Extension&lt;/name&gt; &lt;!-- Extension name --&gt;        &lt;description&gt;What my extension does&lt;/description&gt; &lt;!-- Short description of your extension--&gt;        &lt;package&gt;my.package.cheeky&lt;/package&gt; &lt;!-- Package name--&gt;        &lt;version&gt;0.0.1&lt;/version&gt;             &lt;!-- Current release version of your extension--&gt;        &lt;compatibility&gt;1.5.3&lt;/compatibility&gt; &lt;!-- Latest version of libGDX your extension is compatible with--&gt;        &lt;website&gt;http://mywebsite.com&lt;/website&gt;  &lt;!-- Url of your extension, either your extension website/github--&gt;        &lt;gwtInherits&gt;               &lt;inherit&gt;cheeky&lt;/inherit&gt;     &lt;!-- GWT module of your extension, for the HTML project --&gt;        &lt;/gwtInherits&gt;        &lt;projects&gt;            &lt;core&gt;                                 &lt;!-- All dependencies for the core project--&gt;                &lt;dependency&gt;groupId:artifactId&lt;/dependency&gt; &lt;!--A single dependency--&gt;            &lt;/core&gt;            &lt;desktop&gt;                &lt;dependency&gt;groupId:artifactId:classifier&lt;/dependency&gt; &lt;!--Multiple dependencies --&gt;                &lt;dependency&gt;groupId:artifactIdTwo:classifierTwo&lt;/dependency&gt; &lt;!--Multiple dependencies --&gt;            &lt;/desktop&gt;                                          &lt;!--All dependencies for the desktop project--&gt;            &lt;android&gt;&lt;/android&gt;                                &lt;!--All dependencies for the android project--&gt;            &lt;ios&gt;null&lt;/ios&gt;                                    &lt;!--All dependencies for the ios project--&gt;            &lt;ios-moe&gt;null&lt;/ios-moe&gt;                            &lt;!--All dependencies for the ios-moe project--&gt;            &lt;html&gt;groupId:artifactIdTwo:classifierTwo&lt;/html&gt;   &lt;!--All dependencies for the html project--&gt;        &lt;/projects&gt;     &lt;/extension&gt; &lt;/extensions&gt;   How dependencies are declared in the extensions.xml file  Under the  tag are all the libGDX supported platforms. Core/Desktop/ios/Android/HTML. In each of these project tags, you can include the dependency deceleration/s for each platform.    In the above example, there is a dependency for the core project on the artifact: groupId:artifactId This means that when the project is generated with the extension ticked, we end up with:  compile \"groupId:artifactId:0.0.1\"  In our core project dependency section.   We also have two dependencies for the desktop platform, groupId:artifactId:classifier and groupId:artifactIdTwo:classifierTwo. This would result in:  compile \"groupId:artifactId:0.0.1:classifier\" compile \"groupId:artifactIdTwo:0.0.1:classifierTwo\"  In our desktop project dependency section.     This is the same for all platorms. A few notes:      If you don’t support a platform, you must put null like in the ios project above.   If you don’t have any extras dependencies for platform, (as they are inherited from core for example) leave the section clear, like android in the example above.   Html projects require the source of dependencies! Make sure you push this source artifact and declare it in the extensions.xml!   This provides all the information required to display your extension and add it to user’s projects in the setup.   You must provide all the data shown above, appended to the extensions.xml file and submit your addition as a PR.   ",
        
        "url": "/wiki/extensions/third-party-extension-support" },{
        "title": "Threading",
        "excerpt":
        
        "All of the ApplicationListener methods are called on the same thread. This thread is the rendering thread on which OpenGL calls can be made. For most games it is sufficient to implement both logic updates and rendering in the ApplicationListener.render() method, and on the rendering thread.   Any graphics operations directly involving OpenGL need to be executed on the rendering thread. Doing so on a different thread results in undefined behaviour. This is due to the OpenGL context only being active on the rendering thread. Making the context current on another thread has its problems on a lot of Android devices, hence it is unsupported.   To pass data to the rendering thread from another thread we recommend using Application.postRunnable(). This will run the code in the Runnable in the rendering thread in the next frame, before ApplicationListener.render() is called.   new Thread(new Runnable() {    @Override    public void run() {       // do something important here, asynchronously to the rendering thread       final Result result = createResult();       // post a Runnable to the rendering thread that processes the result       Gdx.app.postRunnable(new Runnable() {          @Override          public void run() {             // process the result, e.g. add it to an Array&lt;Result&gt; field of the ApplicationListener.             results.add(result);          }       });    } }).start();   Which libGDX classes are Thread-safe?  No class in libGDX is thread-safe unless explicitly marked as thread-safe in the class documentation!   Particularly, you should never perform multi-threaded operations on anything that is graphics or audio related, e.g. use scene2D components from multiple threads.   HTML5  JavaScript is inherently single-threaded. As such, one of the limitations of the HTML 5 backend is that threading is not possible. Web Workers might be an option in the future, however, data is passed via message passing between thread. Java uses different threading primitives and mechanisms, porting threading code to Web Workers will not be straight forward.  ",
        
        "url": "/wiki/app/threading" },{
        "title": "Tile maps",
        "excerpt":
        
        "Maps   libGDX features a generic maps API. All map related classes can be found in the com.badlogic.gdx.maps (code) package. The root package contains the base classes, sub-packages contain specialized implementations for tile maps and other forms of maps.   Base Classes  The set of base classes is meant to be generic so we can support not only tiled maps, but any 2D map format.   A map is a set of layers. A layer contains a set of objects. Maps, layers and objects have properties, that depend on the format they’ve been loaded from. For some formats there are specialized map, layer and object implementations, more on that later. The class hierarchy of the base classes looks as follows:      Properties   Properties of maps, layers or objects are represented by  MapProperties (source). This class is essentially a hash map, with string keys and arbitrary values.   Which key/value pairs are available for a map, layer or object depends on the format from which it was loaded. To access properties, you can simply do the following:   map.getProperties().get(\"custom-property\", String.class); layer.getProperties().get(\"another-property\", Float.class); object.getProperties().get(\"foo\", Boolean.class);   Many of the supported editors allow you to specify such properties on maps, layers and objects. What specific type these properties have is format specific. When in doubt, load up your map in your libGDX application, with one of the map loaders, and inspect the properties of the objects you are interested in.   Map Layers   Layers within a map are ordered and indexed, starting by index 0. You can access the layers of a map like this:   MapLayer layer = map.getLayers().get(0);   You can also search a layer by name   MapLayer layer = map.getLayers().get(\"my-layer\");   These getter methods will always return a MapLayer. Some layers may be specialized and offer more functionality, in which case you can simply cast:   TiledMapTileLayer tiledLayer = (TiledMapTileLayer)map.getLayers().get(0);   A layer has a few attribute that we try to normalize for every supported map format:   String name = layer.getName(); float opacity = layer.getOpacity(); boolean isVisible = layer.isVisible();   You can also modify these which may have an effect on how the layer is rendered.   In addition to these normalized attributes, you can also access the more generic properties, as described above.   To get the objects within the layer, simply call the following:   MapObjects objects = layer.getObjects();   The MapObjects (code) instance allows you to retrieve objects by name, index or type. You can also insert and remove objects on the fly.   Map Objects   The API already provides a handful of specialized map objects, such as  CircleMapObject (code), PolygonMapObject (code)   and so on.   The loader of a map format will parse these objects and put them in their respective  MapLayer (code).   For all supported formats, we try to extract the following normalized attributes for every object:   String name = object.getName(); float opacity = object.getOpacity(); boolean isVisible = object.isVisible(); Color color = object.getColor();   The specialized map objects, like PolygonMapObject may also have aditional attributes, e.g.   Polygon poly = polyObject.getPolygon();   Changing any of these attributes may have an effect on how the object is rendered.   As in the case of maps and layers, you can also access the more generic properties, as described above.   Note: tiles of a tiled map are not stored as map objects. There are specialized layer implementations that store these kind of objects more efficiently, see below. Objects as described above are generally used to define trigger areas, spawn points, collision shapes and so on.   Map Renderer   The MapRenderer (code) interface defines methods that allow you to render the layers and objects of a map.   Before you can start rendering, you have to set the view on your map. Think of the view as window through which you look. The easiest way to achieve this, is to tell the map renderer about an OrthographicCamera it should use:   mapRenderer.setView(camera);   Alternatively you can also specify a projection matrix and the view boundaries manually:   mapRenderer.setView(projectionMatrix, startX, startY, endx, endY);   The view boundaries are given in the x/y plane, with the y-axis pointing upwards. The units used are specific to the map and format it was loaded from.   To render all layers of the map you can then simply call:   mapRenderer.render();   If you need more control over which layers should be rendered, you can specify the indices of layers you want to render. Assuming you have 3 layers, two background layers and a foreground layer, and you want to render your custom sprites between the foreground and background layer, you can do this:   int[] backgroundLayers = { 0, 1 }; // don't allocate every frame! int[] foregroundLayers = { 2 };    // don't allocate every frame! mapRenderer.render(backgroundLayers); renderMyCustomSprites(); mapRenderer.render(foregroundLayers);   By rendering each layer separately and modifying the view for every layer, you can also achieve a parallax effect.   Tiled Maps  Maps that contain layers with tiles are handled by the classes in the com.badlogic.gdx.maps.tiled package. The package contains loaders for different formats.   Tile maps are loaded into  TiledMap (code) instances. TiledMap is a subclass of the generic Map class, with additional methods and attributes.   Tiled Map Layers  Layers with tiles in them are stored in  TiledMapTileLayer (code) instances. In order to get access to the tiles, you will have to cast:   TiledMap tiledMap = loadMap(); // see below for this TiledMapTileLayer layer = (TiledMapTileLayer)tiledMap.getLayers().get(0); // assuming the layer at index on contains tiles   A TiledMapTileLayer has all the same attributes as the generic MapLayer, e.g. properties, objects and so on.   In addition to those, the TiledMapTileLayer also has a two dimensional array of TiledMapTileLayer.Cell (code) instances.   To access a cell, you can ask the tile layer to hand it out like this:   Cell cell = tileLayer.getCell(column, row);   Where column and row specify the location of the cell. These are integer indices. The tiles are supposed to be in a y-up coordinate system. The bottom left tile of a map would thus be located at (0,0), the top right tile at (tileLayer.getWidth()-1, tileLayer.getHeight()-1).   If no tile exists at that position, or if the column/row arguments are out of bounds, null will be returned.   You can query the number of horizontal and vertical tiles in a layer by:   int columns = tileLayer.getWidth(); int rows = tileLayer.getHeight();   Cells  A cell is a container for a  TiledMapTile (code). The cell itself stores a reference to a tile in addition to attributes that specify if the tile should be rotated or flipped when rendering it.   Tiles are usually shared by multiple cells.   Tilesets &amp; Tiles  A TiledMap contains one or more  TiledMapTileSet (code) instances. A tile set contains a number of TiledMapTile (code) instances. There are multiple implementations of tiles, e.g. static tiles, animated tiles etc. You can also create your own implementation for special purposes.   Cells in a tile layer reference these tiles. Cells within a layer can reference tiles of multiple tile sets. It is however recommended to stick to a single tile set per layer to reduce texture switches.   Rendering Tiled Maps  To render a TiledMap and its layers, you will need one of the specialized MapRenderer implementations. For orthogonal or top down maps, use  OrthogonalTiledMapRenderer (code), for isometric maps use  IsometricTiledMapRenderer (code). Other renderers in this package are experimental, we do not advise to use them at this point.   Creating such a renderer works like this:   float unitScale = 1 / 16f; OrthogonalTiledMapRenderer renderer = new OrthogonalTiledMapRenderer(map, unitScale);   The renderer will only ever be able to render the map you pass to it in the constructor. This coupling allows the renderer to perform optimizations for this specific map, and cache them.   The unit scale tells the renderer how many pixels map to a single world unit. In the above case 16 pixels would equal one unit. If you want a pixel to map to a unit, unitScale would have to be one, and so on.   The unit scale is a way to couple your rendering coordinate system with your logical or world coordinate system.   A small example: assume you have a tile map, where tiles are 32x32 pixels wide. In your logical representation of the world, you want these to map to 1x1 unit squares. You’d specify 1/32f as your unit scale. You can now setup your camera to also operate on that unit scale. Say you want to view 30x20 tiles of your map on screen, then you can create your camera like this:   OrthographicCamera camera = new OrthographicCamera(); camera.setToOrtho(false, 30, 20);   Working with isometric maps is analogous, just create an IsometricTiledMapRenderer:   renderer = new IsometricTiledMapRenderer(isoMap, 1 / 32f);   Again, you have to specify the map (which should be an isometric tiled map) and a unit scale.   Note: the isometric renderer is experimental, use at your own risk and please report any issues you find. From a performance perspective, rendering isometric maps on mobile devices is very costly, as every tile must have blending on.   Loading TMX/Tiled maps     Tiled is a generic tile map editor for Windows/Linux/Mac OS X that allows you to create tile layers as well as object layers, containing arbitrary shapes for trigger areas and other purposes. libGDX provides a loader to read files generated by Tiled.   To load a Tiled map you have two options: either load it directly or via the AssetManager. The first option works like this:   TiledMap map = new TmxMapLoader().load(\"level1.tmx\");   This will load the file called level1.tmx from the internal file storage (the assets directory). If you want to load a file using a different file type, you have to supply a FileHandleResolver in the constructor of the TmxMapLoader.   TiledMap map = new TmxMapLoader(new ExternalFileHandleResolver()).load(\"level1.tmx\");   We chose this mechanism as the TmxMapLoader can also be used with the AssetManager class, where FileHandleResolvers rule the earth. To load a TMX map via the AssetManager, you can do the following:   // only needed once assetManager.setLoader(TiledMap.class, new TmxMapLoader(new InternalFileHandleResolver())); assetManager.load(\"level1.tmx\", TiledMap.class);  // once the asset manager is done loading TiledMap map = assetManager.get(\"level1.tmx\");   Once loaded you can treat the map just like an other TiledMap.   Note if you load your TMX map directly, you are responsible for calling TiledMap#dispose() once you no longer need it. This call will dispose of any textures loaded for the map.   Note: if you want to use TMX maps with the GWT backend, you need to make sure the map is saved with pure base64 encoding. The compressed TMX formats will not work due to limitations in GWT.   Loading Tide maps     Tide is another general purpose tile map editor, available for Windows only. libGDX provides a loader for the format output by Tide.   As with TMX files, you can either load a Tide map directly or through the asset manager:   // direct loading map = new TideMapLoader().load(\"level1.tide\");  // asset manager loading assetManager.setLoader(TiledMap.class, new TideMapLoader(new InternalFileHandleResolver())); assetManager.load(\"level1.tide\", TiledMap.class);   Note if you load your Tide map directly, you are responsible for calling TiledMap#dispose() once you no longer need it. This call will dispose of any textures loaded for the map.   Performance considerations  While we try to make the renderers as fast as possible, there are a few things you can consider to boost rendering performance.      Only use tiles from a single tile set in a layer. This will reduce texture binding.   Mark tiles that do not need blending as opaque. At the moment you can only do this programmatically, we will provide ways to do it in the editor or automatically.   Do not go overboard with the number of layers.   Examples     Simple platformer using a TMX map   Programmatic creation of a TiledMap   Tile map asset manager loading/rendering   Tile map direct loading/rendering   Tide map asset manager loading/rendering   Tide map direct loading/rendering  ",
        
        "url": "/wiki/graphics/2d/tile-maps" },{
        "title": "Updating libGDX",
        "excerpt":
        
        "   Switching libGDX Versions   Updating Gradle Itself   Switching libGDX Versions  libGDX’s Gradle based projects make it very easy to switch between libGDX versions. In general you’ll be interested in two types of libGDX builds:      Release builds: these are considered stable. You can see the available release versions on Maven Central.   Nightly builds: also known as SNAPSHOT builds in Maven lingo. These are cutting edge versions of libGDX that are built on every change to the source repository. Snapshot builds also have a version number of the form x.y.z-SNAPSHOT, e.g. 1.9.10-SNAPSHOT. You can find the latest SNAPSHOT version string here.   Your Gradle based project makes it very easy to switch between releases and nightly builds. Open up the build.gradle file in the root of your project, and locate the following line:    gdxVersion = \"1.5.2\"   The version you see will most certainly be higher than 1.5.2. Once you’ve located that string, you can simply change it to the latest release (or an older release) or to the current SNAPSHOT version. You may also have to update other modules and dependencies in that same section of the build.gradle file, based on the versions listing. Once edited, save the build.gradle file.   The next step is dependent on your IDE:      Eclipse: Select all your projects in the package explorer, right click, then click Gradle -&gt; Refresh All. This will download the libGDX version you specified in build.gradle and wire up the JAR files with your projects correctly.   Intellij IDEA / Android Studio: will usually detect that your build.gradle has been updated and show a refresh button. Just click it and IDEA will update libGDX to the version you specified in build.gradle. Go into the Gradle tasks panel/tool view and click the refresh button. Running a task like ‘builddependents’ also tends to do this.   Netbeans: in the “Projects” view, right-click the top-most project node and select “Reload Project”. All sub-projects will also be reloaded with the new files.   Command Line: invoking any of the tasks will usually check for changes in dependency versions and redownload anything that changed.   Replacing additional files   You may need to replace additional files for some releases. They are listed here:   Update to release 1.9.13+  Since version 1.9.13, breaking changes and corresponding migration steps are explicitly mentioned in our changelogs. Take a look at them here.   Update to release 1.9.12      HTML: You can delete the soundmanager files for HTML Project and the references in index.html   HTML: You should update code in HTMLLauncher according to setup template   Update to release 1.9.6     Replace soundmanager files for HTML project, otherwise Web Application might not start. See #2246.   Gradle Versions Plugin   In the spirit of the Maven Versions Plugin, the https://github.com/ben-manes/gradle-versions-plugin provides a simple dependencyUpdates task to determine which dependencies have updates.   Updating Gradle Itself  You may also want to update your Gradle version.   Gradle Wrapper  Essentially, the Gradle Wrapper (./gradlew) is a script that invokes a declared version of Gradle, downloading it beforehand if necessary. It is the recommended way to execute any Gradle build, because it does away with complex setups and allows any developer to get a project up and running in no time. Alternatively, you can use a system Gradle installation.   To update the Gradle Wrapper version that is embedded (and stored in your repository), you can run the following command,   ./gradlew wrapper --gradle-version #{GRADLE_VERSION}   where #{GRADLE_VERSION} is your preferred Gradle version. We advise you to use the one specified here for our setup tool.   As an alternative, you can specify a Gradle distribution by URL (take a look here for official distributions):   ./gradlew wrapper --gradle-distribution-url #{GRADLE_DISTRIBUTION_URL}   Additional Steps  Since Gradle updates often introduce breaking changes, you might need to take additional steps to get your project running again after an update. Usually, we recommend just recreating your project structure with the setup tool and then copying over your dependencies and code. Alternatively, you can take a look at the changes we made to the setup tool’s example project here.  ",
        
        "url": "/wiki/articles/updating-libgdx" },{
        "title": "Using libGDX with Clojure",
        "excerpt":
        
        "Clojure is a dialect of Lisp, written for the JVM and with functional programming in mind. Clojure comes with native Java interoperability, making it able to leverage the powerful existing libraries in the Java ecosystem. ClojureTV on YouTube has a lot of good videos, specifically Clojure for Java Programmers (Part 2).   Project setup   Your project’s directory structure should look something like:  demo - android - desktop   - resources   - src     - demo       - core         - desktop_launcher.clj   - src-common     - demo       - core.clj   - project.clj   project.clj  (defproject demo \"0.0.1-SNAPSHOT\"   :description \"FIXME: write description\"   :dependencies [[com.badlogicgames.gdx/gdx \"1.9.3\"]                  [com.badlogicgames.gdx/gdx-backend-lwjgl \"1.9.3\"]                  [com.badlogicgames.gdx/gdx-box2d \"1.9.3\"]                  [com.badlogicgames.gdx/gdx-box2d-platform \"1.9.3\"                   :classifier \"natives-desktop\"]                  [com.badlogicgames.gdx/gdx-bullet \"1.9.3\"]                  [com.badlogicgames.gdx/gdx-bullet-platform \"1.9.3\"                   :classifier \"natives-desktop\"]                  [com.badlogicgames.gdx/gdx-platform \"1.9.3\"                   :classifier \"natives-desktop\"]                  [org.clojure/clojure \"1.7.0\"]]   :source-paths [\"src\" \"src-common\"]   :javac-options [\"-target\" \"1.6\" \"-source\" \"1.6\" \"-Xlint:-options\"]   :aot [demo.core.desktop-launcher]   :main demo.core.desktop-launcher)   desktop_launcher.clj  (ns demo.core.desktop-launcher   (:require [demo.core :refer :all])   (:import [com.badlogic.gdx.backends.lwjgl LwjglApplication]            [org.lwjgl.input Keyboard])   (:gen-class))  (defn -main []   (LwjglApplication. (demo.core.Game.) \"demo\" 800 600)   (Keyboard/enableRepeatEvents true))   core.clj  (ns demo.core   (:import [com.badlogic.gdx Game Gdx Graphics Screen]            [com.badlogic.gdx.graphics Color GL20]            [com.badlogic.gdx.graphics.g2d BitmapFont]            [com.badlogic.gdx.scenes.scene2d Stage]            [com.badlogic.gdx.scenes.scene2d.ui Label Label$LabelStyle]))  (gen-class   :name demo.core.Game   :extends com.badlogic.gdx.Game)  (def main-screen   (let [stage (atom nil)]     (proxy [Screen] []       (show []         (reset! stage (Stage.))         (let [style (Label$LabelStyle. (BitmapFont.) (Color. 1 1 1 1))               label (Label. \"Hello world!\" style)]           (.addActor @stage label)))       (render [delta]         (.glClearColor (Gdx/gl) 0 0 0 0)         (.glClear (Gdx/gl) GL20/GL_COLOR_BUFFER_BIT)         (doto @stage           (.act delta)           (.draw)))       (dispose[])       (hide [])       (pause [])       (resize [w h])       (resume []))))  (defn -create [^Game this]   (.setScreen this main-screen))   You can launch the window with lein run in the project.clj directory, or the repl of your choice by calling (demo.core.desktop-launcher/-main).   For repl based dev have your main-screen call fns for each lifecycle method you wish to re-evaluate.   play-clj   The play-clj library provides a Clojure wrapper for libGDX. To get started, install Leiningen and run the following command:   lein new play-clj hello-world   A directory called hello-world should appear, and inside you’ll find directories for android and desktop. Inside the desktop directory, you’ll find a src-common directory, which contains the game code that both projects will read from. Navigate inside of it to find core.clj, which looks like this:   (ns hello-world.core   (:require [play-clj.core :refer :all]             [play-clj.ui :refer :all]))  (defscreen main-screen   :on-show   (fn [screen entities]     (update! screen :renderer (stage))     (label \"Hello world!\" (color :white)))    :on-render   (fn [screen entities]     (clear!)     (render! screen entities)))  (defgame hello-world   :on-create   (fn [this]     (set-screen! this main-screen)))   This will display a label on the bottom left corner, which you can see by running lein run inside the desktop directory. To generate a JAR file that you can distribute to other people, run lein uberjar and grab the file in the target directory that contains the word “standalone”.   Links      The play-clj tutorial provides a more in-depth walk-through on how to use the library.   The Nightmod game tool provides an easier way to use play-clj by integrating the game and the text editor together so you can see instant results when you save your code.  ",
        
        "url": "/wiki/jvm-langs/using-libgdx-with-clojure" },{
        "title": "Using libGDX with Kotlin",
        "excerpt":
        
        "Kotlin is a modern statically typed JVM language from JetBrains, the creators of IntelliJ IDEA (Kotlin supports Eclipse too). If you’re a C# user or appreciate its features, you will feel more at home as Kotlin has many features C# has.   Due to how GWT works, you will not be able to use the HTML5 target with Kotlin. This could be fixed in the future by using Kotlin’s JavaScript back-end. It might be possible to utilize TeaVM as a replacement for GWT, though.   About the Kotlin language   Notable features:      Null-safe types (more compile-time errors instead of always runtime ones)   Higher-order functions   Lambdas that work well (closures, which Java doesn’t really have)   Cleaner syntax than Java (semi-colons are optional; new keyword isn’t there, because it’s unnecessary)   Extension functions (like C# has), so you can extend with static methods and create e.g. \"a string\".myCustomFunction()   String interpolation: println(\"size is ${list.size} out of $maxElements\")   Operator overloading   Target Java 6 transparently, without the same loss of features like you get in Java. This makes it especially attractive for Android.   100% interoperable with your Java libraries, and even other Java source files in your project. Seamlessly. Has a button to convert existing Java code to Kotlin too.   Properties - no need to write boilerplate getters and setters   Ranges and range operator: if (x in 0..10) println(\"in range!\")   Inlined methods, which make it possible to reduce method counts, as well as optimize methods using lambdas   And more - see language reference docs.   It also does not force much of anything upon you like some other languages. That is, you can create Kotlin code that is much like the same Java code (without lambdas, no higher order functions, same class/OOP design, etc). It’s a more pragmatic language, rather than academic/forceful.   See the Kotlin Language Reference Docs for deciding on and learning the language. You can also read the Kotlin comparison to Java.   Migrating an existing project to Kotlin   This guide describes how to migrate an existing libGDX project to Kotlin. You can also start with a fresh application.      Configure Gradle            Setup the Kotlin Gradle plugin       Apply the Kotlin Gradle Plugin       Configuring Dependencies           Convert Your Code From Java to Kotlin   Build and Run   Examples of libGDX projects using Kotlin   Configure Gradle   This step basically includes following the instructions from the official Kotlin manual.   Set up the kotlin-gradle plugin   Add the following to your parent project’s build.gradle:   buildscript {     ext.kotlinVersion = '&lt;version to use&gt;'      dependencies {         classpath \"org.jetbrains.kotlin:kotlin-gradle-plugin:$kotlinVersion\"     } }   Since most likely you already got the buildscript block in your Gradle config, make sure to only add the sub-items accordingly.   Apply the kotlin-gradle plugin   Replace all occurrences of apply plugin: \"java\" with apply plugin: \"kotlin\". Check your parent project’s build.gradle as well as your sub-projects (core, desktop, ios, android).   In the android sub-project, add apply plugin: \"kotlin-android\" after the apply plugin: \"android\" line.   Configuring Dependencies   Add Kotlin’s stdlib to your core project’s dependencies list:   dependencies {     compile \"org.jetbrains.kotlin:kotlin-stdlib:$kotlinVersion\" }   If you intend to use Kotlin’s reflection capabilities as well, add the respective library too:   dependencies {     compile \"org.jetbrains.kotlin:kotlin-reflect:$kotlinVersion\" }   Note for Intellij users   If you made Intellij automatically configure build.gradle for you, and chose Kotlin 1.1 or higher version, it might add this dependency:   dependencies {     compile \"org.jetbrains.kotlin:kotlin-stdlib-jre8:$kotlinVersion\" }   If you’re targeting platforms that don’t support Java 8 library, such as most Android phones, it won’t compile. You may need to replace it with kotlin-stdlib library.   Convert your code from Java to Kotlin   You do not need to migrate all or any of your Java code right away. Both languages are fully interoperable with each other.   However, if you decide to migrate your Java code to Kotlin, IntelliJ IDEA has a handy function for that.   Open any Java file, e.g. your DesktopLauncher and select Code → Convert Java File to Kotlin File from the menu. Repeat this process for every file you want to migrate. While it is still in its infancy so it won’t be error proof, but it will help you come up with more idiomatic ways of coding your project. There will be some errors, specifically with some Class&lt;&gt; usage, and Java code that it could not deduce null safety (because that information is lacking from Java).   Build and run   That’s it. You successfully enabled Kotlin in your libGDX application. Build and run your project to verify that everything works.   Kotlin libGDX extensions      KTX is a set of libraries that aim to make most aspects of libGDX more Kotlin-friendly thanks to extension functions, utility classes and so on. It includes utilities for assets management, LibGDX custom collections, Box2D, coroutines, math-related classes, actors, i18n, dependency injection, GUI type-safe building and more.   Examples of libGDX projects using Kotlin   These are some examples of projects that are using Kotlin, to help give you ideas on how to structure, take advantage of language features, as well as simple stuff such as build system.      Ore Infinium (desktop, moderate size, uses artemis-odb, kryonet, ktx, protobuf)   HitKlack (desktop &amp; android, small size, code examples to explain Kotlin’s features)   SplinterSweets (desktop, android, iOS (Multi-OS Engine), Google Play Games and Admob integration, small sized and simple)   Herring.io, Neighbourhood Watch (desktop, made by a single team in under 30 hours using KTX on EGU Jam 2016)   Horde! (desktop, made in under 40 hours, won BialJam 2017)   BlockBunny (desktop (with controller support), android, iOS (Multi-OS Engine) / with optimization, changes and improvement from original ForeignGuyMike’s tutorial video)   OMO (PC, Android, and iOS (Multi-OS Engine) / with changes and improvements on top of ForeignGuyMike’s original project)   Asteroids (PC, Android, and iOS (Multi-OS Engine) / gamepad support, with changes and improvements on top of ForeignGuyMike’s original project)   Unciv (PC, Android / Open-source Android/Desktop remake of Civ V)  ",
        
        "url": "/wiki/jvm-langs/using-libgdx-with-kotlin" },{
        "title": "Using libGDX With Other JVM languages",
        "excerpt":
        
        "libGDX is mainly a Java-based framework. However, because Java produces Java bytecodes, and the virtual machine runs these bytecodes, it is possible to run libGDX in any JVM language with proper Java interoperability.   Some target platforms can’t run Java bytecodes directly, and so have more specific compilation requirements. Using a language other than Java may affect support for these platforms.   Language interoperability guides      Clojure            Using libGDX with Clojure           Kotlin            Using libGDX with Kotlin           Scala            Using libGDX with Scala           Python (Jython)            Using libGDX with Python           Ruby (JRuby)   Target platform compatibility   Desktop   This works out of the box, as the desktop libGDX back-end uses the JVM that you have installed on your computer, which is most likely either OpenJDK or Oracle JDK. Both of these JVMs support polyglot code, as they run on .class files, not Java source code.   Android   This works for many languages, but it is sometimes unavailable. For best results, search on your favorite search engine “[JVM language of choice] on Android”.   Some examples:      Lein-droid for Clojure   SBT-Android for Scala   Kotlin on Android using the Kotlin plugin of IntelliJ IDEA Kotlin has full support for Android and Java 6, with the same codebase/featureset.   iOS-ROBOVM   The ROBOVM backend is a JVM on iOS which executes Java bytecode. This should work, but has not been tested!   HTML/GWT   Because libGDX uses GWT, JVM languages other than Java cannot use the HTML5 target of libGDX. GWT transpiles Java to JavaScript, as opposed to Java bytecode (.class files) to JavaScript code. There are a few reasons for this, quickly outlined by a Google employee here.   This could theoretically be fixed for JVM languages that have their own JavaScript back-ends, such as Scala and Kotlin. Your libGDX project’s build system would have to be changed to integrate with those back-ends instead of with GWT.   Examples   Many people have used libGDX in their JVM language of choice. Here are some examples.      Scala   Ruby   Ruby on Android with Ruboto   Kotlin   TODO (find some recent examples, would love contributions!)   Reference   https://web.archive.org/web/20201031162718/https://www.badlogicgames.com/wordpress/?p=2750  ",
        
        "url": "/wiki/jvm-langs/using-libgdx-with-other-jvm-languages" },{
        "title": "Using libGDX with Python",
        "excerpt":
        
        "Python is a dynamic and strongly typed language that supports many programming paradigms, such as procedural, object oriented, and functional programming.   Python has been implemented in several different ways; the standard interpreter in C (CPython), in python itself (PyPy), in the .Net Dynamic Language Runtime (C#) (IronPython), and in Java on the Java Virtual Machine (Jython). Jython comes with java interoperability; allowing it to leverage powerful java libraries, such as Libgdx, while keeping the succinctness and readability of Python.   This article uses the latest Jython beta (Jython 2.7b1 available here), this release aims to bring compatibility with CPython 2.7, so we will be programming with Python 2.7 syntax in this article.   Note: at the time of writting, you can only use Jython with libGDX on the desktop.   Setup   Jython can be worked on with any text editor, including Vim or Emacs. PyDev is an option for eclipse users. Once the environment is setup, create a new Jython project, and all of the libGDX dependencies to the PYTHONPATH. for using the desktop LWJGL backend, this includes gdx.jar, gdx-backend-lwjgl.jar, gdx-backend-lwjgl-natives.jar, gdx-natives.jar, and gdx-sources.jar.   Coding With Python   The entirety of the Drop Tutorial can be contained into a single python file.   from com.badlogic.gdx.backends.lwjgl import LwjglApplication, LwjglApplicationConfiguration from com.badlogic.gdx.utils import TimeUtils, Array from com.badlogic.gdx.math import MathUtils, Rectangle, Vector3 from com.badlogic.gdx import ApplicationListener, Gdx, Input from com.badlogic.gdx.graphics.g2d import SpriteBatch from com.badlogic.gdx.graphics import Texture, OrthographicCamera, GL20  class PyGdx(ApplicationListener):     def __init__(self):         self.camera = None         self.batch = None         self.texture = None         self.bucketimg = None         self.dropsound = None         self.rainmusic = None         self.bucket = None         self.raindrops = None                  self.lastdrop = 0         self.width = 800         self.height = 480          def spawndrop(self):         raindrop = Rectangle()         raindrop.x = MathUtils.random(0, self.width - 64)         raindrop.y = self.height         raindrop.width = 64         raindrop.height = 64         self.raindrops.add(raindrop)         self.lastdrop = TimeUtils.nanoTime()              def create(self):                 self.camera = OrthographicCamera()         self.camera.setToOrtho(False, self.width, self.height)         self.batch = SpriteBatch()                  self.dropimg = Texture(\"assets/droplet.png\")         self.bucketimg = Texture(\"assets/bucket.png\")         self.dropsound = Gdx.audio.newSound(Gdx.files.internal(\"assets/drop.wav\"))         self.rainmusic = Gdx.audio.newSound(Gdx.files.internal(\"assets/rain.mp3\"))                  self.bucket = Rectangle()         self.bucket.x = (self.width / 2) - (64 / 2)         self.bucket.y = 20         self.bucket.width = 64         self.bucket.height = 64                  self.raindrops = Array()         self.spawndrop()                  self.rainmusic.setLooping(True, True)         self.rainmusic.play()          def render(self):         Gdx.gl.glClearColor(0,0,0.2,0)         Gdx.gl.glClear(GL20.GL_COLOR_BUFFER_BIT)                  self.camera.update()                  self.batch.setProjectionMatrix(self.camera.combined)         self.batch.begin()         self.batch.draw(self.bucketimg, self.bucket.x, self.bucket.y)         for drop in self.raindrops:             self.batch.draw(self.dropimg, drop.x, drop.y)         self.batch.end()                  if Gdx.input.isTouched():             touchpos = Vector3()             touchpos.set(Gdx.input.getX(), Gdx.input.getY(), 0)             self.camera.unproject(touchpos)             self.bucket.x = touchpos.x - (64 / 2)         if Gdx.input.isKeyPressed(Input.Keys.LEFT): self.bucket.x -= 200 * Gdx.graphics.getDeltaTime()         if Gdx.input.isKeyPressed(Input.Keys.RIGHT): self.bucket.x += 200 * Gdx.graphics.getDeltaTime()                  if self.bucket.x &lt; 0: self.bucket.x = 0         if self.bucket.x &gt; (self.width - 64): self.bucket.x = self.width - 64                  if (TimeUtils.nanoTime() - self.lastdrop) &gt; 1000000000: self.spawndrop()                                  iterator = self.raindrops.iterator()         while iterator.hasNext():             raindrop = iterator.next()             raindrop.y -= 200 * Gdx.graphics.getDeltaTime();             if (raindrop.y + 64) &lt; 0: iterator.remove()             if raindrop.overlaps(self.bucket):                 self.dropsound.play()                 iterator.remove()              def resize(self, width, height):         pass      def pause(self):         pass      def resume(self):         pass          def dispose(self):         self.batch.dispose()         self.dropimg.dispose()         self.bucketimg.dispose()         self.dropsound.dispose()         self.rainmusic.dispose()   def main():      cfg = LwjglApplicationConfiguration()     cfg.title = \"PyGdx\";     cfg.width = 800     cfg.height = 480          LwjglApplication(PyGdx(), cfg)          if __name__ == '__main__':     main()   note that during asset creation we need to specifiy the assets/ folder. When not using android, we must specify the folder structure that we use, whereas on android all internal assets are assumed to be in the assets/ directory.   Games written in Python using libGDX      none (yet! if you have made a game using python in libgdx, please let me know and edit this article!)  ",
        
        "url": "/wiki/jvm-langs/using-libgdx-with-python" },{
        "title": "Using libGDX with Scala",
        "excerpt":
        
        "Scala is a functional, object-oriented programming language for the JVM that works seamlessly with Java libraries, frameworks, and tools. It has a concise syntax and a REPL, which makes it feel like a scripting language, but it is being used in mission critical server software at companies like Twitter and LinkedIn.   Scala developers usually choose either Gradle or SBT as their build tool. This tutorial shows how to set up either of them to start using LibGDX. You may choose which one to use according to your own preferences.   Due to how GWT works you will not be able to use the HTML5 target with Scala   Using libGDX and Scala with Gradle   The default build created by the gdx-setup.jar tool is the best place to start when going the Gradle route. A repo with the required modifications to the default “blank” gdx project can be found here: gdx-scala-demo. These changes have been outlined below.   In order to support Scala compilation you need to update the build with a couple of additions:           /gradle.properties             Increase the heap used by gradle (otherwise you might have trouble compiling for iOS).                /build.gradle             Add the Scala plugin to the project(\":core\") section: apply plugin: \"scala\"       In the dependencies include the scala library: compile \"org.scala-lang:scala-library:2.11.12\" (Scala 2.12.* requires java 8, but the majority of Android devices don’t support it)                /core/build.gradle             Apply the scala plugin at the top of this file.       optional Set the src directory for scala files: sourceSets.main.scala.srcDirs = [ \"src/\" ]                /android/build.gradle             In the android section (top of the file) you need to add the following:           lintOptions {       abortOnError false // make sure you're paying attention to the linter output!   }    // FIXME: How can we apply this simply for all builds? Copy-pasta makes me sad.   buildTypes {       release {           minifyEnabled true           proguardFile getDefaultProguardFile('proguard-android-optimize.txt')           proguardFile 'proguard-project.txt'       }       debug {           minifyEnabled true           proguardFile getDefaultProguardFile('proguard-android-optimize.txt')           proguardFile 'proguard-project.txt'       }   }                                /android/proguard-project.txt                      In order for Proguard to work you need to add the following lines:             -dontwarn sun.misc.*   -dontwarn java.lang.management.**   -dontwarn java.beans.**                                It might also be required to then change the line -dontwarn com.badlogic.gdx.jnigen.BuildTarget* to -dontwarn com.badlogic.gdx.jnigen.*                   With all of these changes in-place you should be able to use Gradle exactly as you would otherwise from the shell or your favorite IDE.   Using libGDX and Scala with SBT   The standard tooling for working with Scala is quite different than what Java developers will be used to. There is a project, libgdx-sbt-project, that provides a simple path for getting started with libGDX and Scala using standard build tools and best practices.   This tutorial assumes you have installed sbt 0.13, which are used in the Scala community for generating and interacting with projects.   Setting up a new project   In your favourite shell type:   $ sbt new ajhager/libgdx-sbt-project.g8   After filling in some information about your project, you can start placing your game’s source files and assets in common/src/main/scala and common/src/main/resources, respectively.   NOTICE The setup above might not be working with iOS build. If you want to use MobiDevelop’s fork of RoboVM, then one should use      this fork of sbt-robovm, you need sbt publish-local this plugin yourself for now.   this fork of project template and use sbt-robovm and RoboVM version 2.3.0. Then it will resolve to the plugin that get publish-localed   Managing your project   Update to the latest libraries:   $ sbt &gt; update    Run the desktop project:   &gt; desktop/run   Package the desktop project into single jar:   &gt; assembly   Run the android project on a device:   &gt; android/start   Visit android-plugin for a more in-depth guide to android configuration and usage.   Run the ios project on a device:   &gt; ios/device   Visit sbt-robovm for a more in-depth guide to ios configuration and usage.   Using unit tests   Run all unit tests from desktop, android and common (subdirectories src/test/scala):   &gt; test   Run specific set of unit tests:   &gt; common/test   Using with popular IDEs   In most cases you will be able to open and edit each sub-project (like common, android or desktop), but you still need to use SBT to build the project.   See here for details about sbt plugins for each editor.   Other resources  Develop Games in Scala with libgdx  ",
        
        "url": "/wiki/jvm-langs/using-libgdx-with-scala" },{
        "title": "Using textureatlases",
        "excerpt":
        
        "See the TexturePacker2 documentation for more information.  ",
        
        "url": "/wiki/graphics/2d/using-textureatlases" },{
        "title": "Vectors, matrices, quaternions",
        "excerpt":
        
        "Introduction   libGDX has several linear algebra classes for dealing with common tasks in physics and applied math. These include:      Vector   Matrix   Quaternion   A full explanation of these concepts is beyond the current scope, but the above links may provide a starting point for further understanding. What follows is an overview of their use and implementation in Libgdx.     Vectors   A vector is an array of numbers used to describe a concept with a direction and magnitude such as position, velocity, or acceleration. As such vectors are typically used to describe the physical properties of motion of a body through space. libGDX has vector classes for two (Vector2) (code) and 3-dimensional (Vector3) (code) spaces. Each contains common operations for working with vector quantities such as addition, subtraction, normalization, and the cross and dot products. There are also methods for linear and spherical linear interpolation between two vectors.   Method Chaining   A pattern also found elsewhere in libGDX is the use of method chaining for convenience and reduced typing. Each operation which modifies a vector returns a reference to that vector to continue chaining operations in the same line call. The following example creates a unit vector pointing along the direction from a point (x1, y1, z1) to another point (x2, y2, z2):   Vector3 vec = new Vector3( x2, y2, z2 ).sub( x1, y1, z1 ).nor();   A new Vector3 is instantiated with the second point coordinates, the first point is subtracted from this, and the result is normalized. This is of course equivalent to:   Vector3 vec = new Vector3( x2, y2, z2 );  // new vector at (x2, y2, z2) vec.sub( x1, y1, z1 );                    // subtract point (x1, y1, z1) vec.nor();                                // normalize result     Matrices   A matrix is a two-dimensional array of numbers. Matrices are used in graphics to perform transformations on and projections of vertices in 3D space for display on 2D screens. As with OpenGL, libGDX stores matrices in column-major order. Matrices come in  3x3 (Matrix3)  (code) and 4x4 (Matrix4) (code) varieties with convenient methods for moving values between the two. libGDX includes many common operations for working with matrices such as building translation and scaling transformations, building rotations from Euler angles, axis-angle pairs or quaternions, building projections, and performing multiplication with other matrices and vectors.   Probably the most common use for matrices in libGDX is through the view and projection matrices of the Camera(code) class which are used to control how geometry is rendered to the screen. Cameras, which come in OrthographicCamera (code) and PerspectiveCamera (code) varieties, provide a convenient and intuitive way to control the view in a rendered scene through a position and viewing direction, yet underneath are a simple group of 4x4 matrices which are used to tell OpenGL how to process geometry for render.   Method Chaining   As with vectors, operations on matrices in Libgdx return a reference to the modified matrix to allow chaining of operations in a single line call. The following creates a new 4x4 model-view matrix useful for an object with position (x, y, z) and rotation described by an axis-angle pair:   Matrix4 mat = new Matrix4().setToRotation( axis, angle ).trn( x, y, z );   This is of course equivalent to:   Matrix4 mat = new Matrix4();       // new identity matrix mat.setToRotation( axis, angle );  // set rotation from axis-angle pair mat.trn( x, y, z );                // translate by x, y, z   Native Methods   The matrix classes have a number of their operations available in static methods backed by fast native code. While member syntax is often easier to read and more convenient to write, these static methods should be used in areas where performance is a concern. The following example uses one of these methods to perform a multiplication between two 4x4 matrices:   Matrix4 matA; Matrix4 matB; Matrix4.mul( matA.val, matB.val );     // the result is stored in matA   Notice the use of .val to access the underlying float array which backs each matrix. The native methods work directly on these arrays. The above is functionally equivalent to the member syntax:  matA.mul( matB );     Quaternions   Quaternions are four-dimensional number systems which extend complex numbers. They have many esoteric uses in higher mathematics and number theory. Specifically in the context Libgdx the use of unit-quaternions can be useful for describing rotations in three-dimensional space, as they provide for simpler composition, numerical stability, and the avoidance of gimbal lock making them preferable often to other methods of rotational representation which may variously fall short in these areas.   Quaternions can be constructed by supplying their four components explicitly or by passing an axis-angle pair. Note that while a quaternion is often described as the combination of a vector and a scalar, it is not merely an axis-angle pair. Libgdx also provide methods for converting between quaternions and the various other rotational representations such as Euler angles, rotation matrices, and axis-angle pairs.  ",
        
        "url": "/wiki/math-utils/vectors-matrices-quaternions" },{
        "title": "Vibrator",
        "excerpt":
        
        "While not strictly an input device, it nevertheless is a kind of a peripheral. We felt it belonged in the input model.   The vibrator allows you to vibrate the phone of a user. This can be used similar to more sophisticated force feedback functionality found commonly in game console controllers.   The vibrator is only available on Android and needs a special permission in the manifest file   android.permission.VIBRATE   See the application configuration section if you are unsure how to specify permissions in your Android manifest.   Vibrating the phone works as follows:   Gdx.input.vibrate(2000);   As the parameter is given in milliseconds the example above will vibrate the phone for 2 seconds.   More sophisticated patterns can be specified via the second vibrate() method:   Gdx.input.vibrate(new long[] { 0, 200, 200, 200}, -1);    This will turn the vibrator on for 200 milliseconds, then turn it off for 200 milliseconds then on again for another 200 milliseconds. The second parameter specifies that the pattern should not be repeated. Refer to the Javadocs for more information.                  Prev       Next          ",
        
        "url": "/wiki/input/vibrator" },{
        "title": "Viewports",
        "excerpt":
        
        "When dealing with different screens it is often necessary to decide for a certain strategy how those different screen sizes and aspect ratios should be handled. libGDX’s Viewports (source) are the solution to deal with this. Camera and Stage support different viewport strategies, for example when doing picking via Camera.project(vec, viewportX, viewportY, viewportWidth, viewportHeight).   If you have never worked with viewports before, be sure to check out this comprehensive introduction:   {% include video id=”8N2vw_3h9HU” provider=”youtube” %}   There is also a transcript of this video available here. If you are having trouble picking the right viewport for your situation, the interactive examples here and here will most certainly prove useful.   Usage  A viewport always manages a Camera’s viewportWidth and viewportHeight. Thus a camera needs to be supplied to the constructors.      private Viewport viewport;     private Camera camera;      public void create() {         camera = new PerspectiveCamera();         viewport = new FitViewport(800, 480, camera);     }  Whenever a resize event occurs, the viewport needs to be informed about it and updated. This will automatically recalculate the viewport parameters and update the camera:      public void resize(int width, int height) {         viewport.update(width, height);     }  Furthermore it will change the OpenGL Viewport via glViewport, which may add black bars if necessary, making it impossible to render in the area of the black bars. In case black bars appear with a certain viewport strategy, the OpenGL viewport may be reset to its standard size and the viewport can be queried for the size of the bars via Viewport.getLeftGutterWidth() etc. For an example of how to do so, see this test. This might look like the following (probably with a more appropriate border picture…)      In case picking needs to be done, Viewport offers convenient project/unproject/getPickRay methods, which uses the current viewport to do the correct picking. This is how you convert to and from screen and world coordinates.   When Stage is used, the Stage’s viewport needs to be updated when a resize event happens.      private Stage stage;      public void create() {         stage = new Stage(new StretchViewport(width, height));     }      public void resize(int width, int height) {         // use true here to center the camera         // that's what you probably want in case of a UI         stage.getViewport().update(width, height, true);     }  Multiple viewports   When using multiple viewports that have different screen sizes (or you use other code that sets glViewport), you will need to apply the viewport before drawing so the glViewport is set for that viewport.      viewport1.apply();     // draw     viewport2.apply();     // draw  When using multiple Stages:      stage1.getViewport().apply();     stage1.draw();     stage2.getViewport().apply();     stage2.draw();   Examples   To see the viewports in action, have a look at the interactive examples here and here. There are also some tests concerning viewports: ViewportTest1, ViewportTest2 and ViewportTest3.   StretchViewport  The StretchViewport (source)  supports working with a virtual screen size. That means one can assume that a screen is always of the size virtualWidth x virtualHeight. This virtual viewport will then always be stretched to fit the screen. There are no black bars, but the aspect ratio may not be the same after the scaling took place.      FitViewport  A FitViewport (source) also supports a virtual screen size. The difference to StretchViewport is that it will always maintain the aspect ratio of the virtual screen size (virtual viewport), while scaling it as much as possible to fit the screen. One disadvantage with this strategy is that there may appear black bars.      FillViewport  A FillViewport (source) also keeps the aspect ratio of the virtual screen size, but in contrast to FitViewport, it will always fill the whole screen which might result in parts of the viewport being cut off.   ScreenViewport  The ScreenViewport (source) does not have a constant virtual screen size; it will always match the window size which means that no scaling happens and no black bars appear. As a disadvantage this means that the gameplay might change, because a player with a bigger screen might see more of the game, than a player with a smaller screen size.      ExtendViewport  The ExtendViewport (source) keeps the world aspect ratio without black bars by extending the world in one direction. The world is first scaled to fit within the viewport, then the shorter dimension is lengthened to fill the viewport.      A maximum set of dimensions can be supplied to ExtendViewport, in which case, black bars will be added when the aspect ratio falls out of the supported range.      CustomViewport  Different strategies may be implemented by doing CustomViewport extends Viewport and overriding update(width, height, centerCamera). Another approach is use the generic ScalingViewport and supplying another Scaling which is not yet covered by any other Viewport. One example could be to supply Scaling.none to it, which will result in a completely “StaticViewport”, which always keeps the same size. It might look like this:     ",
        
        "url": "/wiki/graphics/viewports" },{
        "title": "Virtual Reality (VR)",
        "excerpt":
        
        "libGDX can be used to render to virtual reality headsets using LWJGL’s OpenVR (HTC Vive) and OVR (Oculus) modules.   Required Dependencies   The Maven coordinates can be found using the LWJGL customize page.   Example code   Example code can be found at https://github.com/badlogic/gdx-vr.   Additionally, the following demos from LWJGL may be useful for debugging:     HelloOpenVR.java   HelloLibOVR.java   Also, see https://github.com/Zomby2D/gdx-vr-extension   OpenVR with offscreen GLFW window   Using an offscreen GLFW window allows to render to VR without rendering or maintaining a window on the monitor’s screen. This will allow achieving the fastest performance with OpenVR.   This isn’t supported yet by libGDX, because some hacking is required, but the general flow is the following.   Lwjgl3NativesLoader.load(); errorCallback = GLFWErrorCallback.createPrint(System.err); GLFW.glfwSetErrorCallback(errorCallback); if (!GLFW.glfwInit()) {    throw new RuntimeException(\"Unable to initialize GLFW\"); } GLFW.glfwWindowHint(GLFW.GLFW_VISIBLE, GLFW.GLFW_FALSE); windowHandle = GLFW.glfwCreateWindow(640, 480, \"\", MemoryUtil.NULL, MemoryUtil.NULL); GLFW.glfwMakeContextCurrent(windowHandle); GL.createCapabilities();  Gdx.gl30 = new Lwjgl3GL30(); Gdx.gl = Gdx.gl30; Gdx.gl20 = Gdx.gl30; Gdx.files = new Lwjgl3Files(); Gdx.graphics = &lt;minimal implementation of Graphics interface&gt; Gdx.app = &lt;minimal implementation of Application interface&gt;  // setup a scene and model batch  while (running) // See https://github.com/ValveSoftware/openvr/wiki/IVRCompositor_Overview {    // OpenVR waitGetPoses();    // OpenVR poll events    // OpenVR Submit eyes } GLFW.glfwDestroyWindow(windowHandle); errorCallback.free(); errorCallback = null; GLFW.glfwTerminate();   Roadmap   VR support might become easier relatively soon via the use of OpenXR. The LWJGL project plans to support this in the future. This would remove the need to support both OVR and OpenVR and instead just using OpenXR to support all hardware.  ",
        
        "url": "/wiki/graphics/3d/virtual-reality" },{
        "title": "Wiki Style Guide",
        "excerpt":
        
        "This page gives some information on how to edit libGDX wiki pages. Please read this before contributing to the libGDX wiki! If you have any (additional) questions, please do not hesitate to ask! See our Discord for more information.   How to?  Every wiki page has an “Edit on GitHub” button on top which redirects you to the GitHub Web Interface of the wiki repo. Use this for small fixes/typos. If you want to undertake more extensive changes, you should fork the repo. The wiki of our website repo also offers some pointers on this.   First Paragraph  Generally, each wiki page should start with an introductory paragraph. This improves useability for the wiki and allows the first few sentences to be used in the meta descriptions and the search results.   Style  We use Markdown to format this wiki. To learn your way around this, here is GitHub’s very concise Markdown Cheatsheet. As our wiki is hosted via GitHub Pages, you can also use HTML, JS and CSS as well as Jekyll’s Liquid Tags. To find out more about your options, take a look here.   Notable syntax      Wiki links are made like this:  [link text to networking](/wiki/networking) renders this: link text to networking   Linking to code/docs  Links to code/docs should be done as follows: [ClassName](link to docs) [(code)](link to code). For example:  [Texture](https://javadoc.io/doc/com.badlogicgames.gdx/gdx/latest/com/badlogic/gdx/graphics/Texture.html) [(code)](https://github.com/libgdx/libgdx/tree/master/gdx/src/com/badlogic/gdx/graphics/Texture.java)   renders the following:   Texture (code)   Don’t use non-alphabetic characters in Wiki page names, because not all operating systems can handle them when cloning Wiki as Git repository (for example, Windows doesn’t support “:”).   {% capture docs-notice %}     Please note that there should be a space in between ClassName (Code) style formatting, in order to differentiate the two.   Please make the format ClassName (Code) with the word Code, not Source or any derivative of that. Consistency is key!   If a link to documentation ends in a right parenthesis ), it will mess up the markdown. Take this example:      https://javadoc.io/doc/com.badlogicgames.gdx/gdx/latest/com/badlogic/gdx/graphics/Texture.html#getWidth()          When using the markdown formatting of []() the end parenthesis will mess up the link, so please remember to escape the ending parenthesis ()). In the example, it should be:       [Link to Texture#getWidth](https://javadoc.io/doc/com.badlogicgames.gdx/gdx/latest/com/badlogic/gdx/graphics/Texture.html#getWidth(\\))          Without the escaped parenthesis, a 404 is imminent! {% endcapture %}       {{ docs-notice | markdownify }}  The main table of contents   If you create a new page, you will most likely want it to be displayed on the main libGDX wiki Table of contents and the sidebar Table of Contents. Therefore, please include the changes to both ToCs with the appropriate positioning of your article in your PR.   Some pages are not listed in the ToC, in particular the ones located in the /wiki/misc folder. Those pages should contain a comment in the frontmatter, clarifying this: # Not listed in ToC.   Tables of contents per page   Tables of contents have to be manually created on a per-page basis. For an example of how to do so outside of this section, please refer to our Box2d article.   When creating headers in markdown, we specify using a number of octothorpes (#) that define the header level. When we create a header ## Comments and Questions/Concerns in an article entitled Help Me the corresponding link would be help-me#comments-and-questionsconcerns. So when we go to make our table of contents, those page fragment links would be placed in an unordered list.   Adding images   Images are stored in the assets/wiki/ directory of the libGDX wiki. To add an image, you must fork and clone the repo. Then add your images to the images folder using the appropriate naming scheme my-page-name# where # is the order of the picture displayed on the page (this can be ommitted if only one image is used in the page, but recommended). Images are linked to with the following syntax (assuming the image is stored in the /assets/wiki/images/ directory) ![image name](/assets/wiki/images/flamedemo.gif) which will display:      If you want to style the image, use something like this: ![image name](/assets/wiki/images/flamedemo.gif){: style=\"width: 300px;\" }   Videos   Videos can be included like this:   {% raw %}{% include video id=\"3kPK_O6Q4wA\" provider=\"youtube\" %}{% endraw %}   Adding GWT examples   Actual libGDX examples can be embedded via GWT as iframes. To do this, use the embed-gwt element on a wiki page:  {% raw %}{% include embed-gwt.html dir='viewport-example' %}{% endraw %}   dir refers to the directory in the libgdx-wiki-examples repo, where the source code of the examples is located. In particular, it denotes the path to the root folder of the example’s Gradle project (without leading and trailing slashes; in this case, viewport-example refers to the  /viewport-example/ folder which contains /html/build.gradle). The examples are automatically built via GH Actions (by calling ./gradlew html:dist) and then deployed through GH Pages.   To style the embedded content, use the containerstyle and iframestyle attributes.   Renaming pages   If you are moving/renaming pages and want to preserve their old links, use redirect_from in the frontmatter:  redirect_from:   - /dev/setup/ # this page is now available via https://libgdx.com/dev/setup/ as well  ",
        
        "url": "/wiki/misc/wiki-style-guide" }];

// PARSE
var idx = lunr(function () {
  this.field('title')
  this.field('excerpt')
  this.ref('id')

  this.pipeline.remove(lunr.trimmer)

  for (var item in store) {
    this.add({
      title: store[item].title,
      excerpt: store[item].excerpt,
      id: item
    })
  }
});

// ACTUAL SEARCH
$(document).ready(function() {
  $('input#search').on('keyup', function () {
    var resultdiv = $('#results');
    var query = $(this).val().toLowerCase();
    var result = idx.search(query);
    resultdiv.empty();
    resultdiv.prepend('<p class="results__found header-meta" style="border: none">'+result.length+' Result(s) found</p>');
    for (var item in result) {
      var ref = result[item].ref;
    	 var searchitem =
        '<div class="list__item">'+
          '<article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">'+
            '<h2 class="archive__item-title" itemprop="headline" style="padding-bottom: 0px">'+
              '<a href="'+store[ref].url+'" rel="permalink">'+store[ref].title+'</a>'+
            '</h2>'+
            '<p class="archive__item-excerpt" itemprop="description" style="padding-bottom: 5px">'+store[ref].excerpt.split(" ").splice(0,45).join(" ")+'...</p>'+
          '</article>'+
        '</div>';

      resultdiv.append(searchitem);
    }
  });
});
</script>
</pre>

    </div>

    

    
  <script src="/assets/js/main.min.js"></script>








  
    <script src="/assets/js/clipboard.js"></script>
  



  </body>
</html>
